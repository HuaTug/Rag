#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Chunking Strategy Demonstration

This script demonstrates different chunking strategies with a comprehensive example document
and shows how different chunk sizes affect the text processing results.
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'core'))

import time
import json
from typing import List, Dict, Any
from core.enhanced_text_processor import EnhancedTextProcessor

def create_demo_document() -> Dict[str, Any]:
    """Create a comprehensive demo document with mixed Chinese-English content"""
    
    content = """# ç°ä»£è½¯ä»¶å¼€å‘ä¸­çš„äººå·¥æ™ºèƒ½åº”ç”¨

äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligence, AIï¼‰æ­£åœ¨é©å‘½æ€§åœ°æ”¹å˜è½¯ä»¶å¼€å‘çš„å„ä¸ªæ–¹é¢ã€‚ä»ä»£ç ç”Ÿæˆåˆ°æµ‹è¯•è‡ªåŠ¨åŒ–ï¼ŒAIæŠ€æœ¯ä¸ºå¼€å‘è€…æä¾›äº†å‰æ‰€æœªæœ‰çš„å·¥å…·å’Œèƒ½åŠ›ã€‚

## 1. ä»£ç ç”Ÿæˆä¸è¾…åŠ©ç¼–ç¨‹

### GitHub Copilot
GitHub Copilotæ˜¯ç”±OpenAIå¼€å‘çš„AIç¼–ç¨‹åŠ©æ‰‹ï¼Œå®ƒå¯ä»¥ï¼š
- æ ¹æ®æ³¨é‡Šè‡ªåŠ¨ç”Ÿæˆä»£ç 
- æä¾›æ™ºèƒ½ä»£ç è¡¥å…¨
- æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€

```python
def calculate_fibonacci(n):
    if n <= 1:
        return n
    return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)
```

### å…¶ä»–AIç¼–ç¨‹å·¥å…·
- **Tabnine**: åŸºäºæ·±åº¦å­¦ä¹ çš„ä»£ç è¡¥å…¨å·¥å…·
- **Kite**: æ™ºèƒ½ä»£ç è¡¥å…¨å’Œæ–‡æ¡£æŸ¥æ‰¾
- **CodeT5**: Googleå¼€å‘çš„ä»£ç ç”Ÿæˆæ¨¡å‹

## 2. è‡ªåŠ¨åŒ–æµ‹è¯•ä¸è´¨é‡ä¿è¯

AIåœ¨è½¯ä»¶æµ‹è¯•é¢†åŸŸçš„åº”ç”¨åŒ…æ‹¬ï¼š

### æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ
- è‡ªåŠ¨åˆ†æä»£ç é€»è¾‘ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
- åŸºäºå†å²bugæ•°æ®é¢„æµ‹æ½œåœ¨é—®é¢˜
- æ™ºèƒ½è¾¹ç•Œå€¼æµ‹è¯•

### ä»£ç å®¡æŸ¥è‡ªåŠ¨åŒ–
```javascript
// AI can detect potential issues like:
function processUserData(userData) {
    // Missing null check - potential bug
    return userData.name.toUpperCase();
}

// Improved version:
function processUserData(userData) {
    if (!userData || !userData.name) {
        throw new Error('Invalid user data');
    }
    return userData.name.toUpperCase();
}
```

## 3. æ€§èƒ½ä¼˜åŒ–ä¸ç›‘æ§

### æ™ºèƒ½æ€§èƒ½åˆ†æ
AIç³»ç»Ÿå¯ä»¥ï¼š
- åˆ†æåº”ç”¨æ€§èƒ½ç“¶é¢ˆ
- é¢„æµ‹ç³»ç»Ÿè´Ÿè½½
- è‡ªåŠ¨ä¼˜åŒ–èµ„æºåˆ†é…

### å¼‚å¸¸æ£€æµ‹
Machine learning algorithms can identify:
- Unusual traffic patterns
- Memory leaks
- Performance degradation

## 4. DevOpsä¸CI/CDä¼˜åŒ–

### æ™ºèƒ½éƒ¨ç½²ç­–ç•¥
- åŸºäºå†å²æ•°æ®é¢„æµ‹éƒ¨ç½²é£é™©
- è‡ªåŠ¨å›æ»šæœºåˆ¶
- æ™ºèƒ½è´Ÿè½½å‡è¡¡

## 5. ç”¨æˆ·ä½“éªŒä¼˜åŒ–

### ä¸ªæ€§åŒ–æ¨è
- åŸºäºç”¨æˆ·è¡Œä¸ºçš„åŠŸèƒ½æ¨è
- æ™ºèƒ½ç•Œé¢å¸ƒå±€è°ƒæ•´
- è‡ªé€‚åº”ç”¨æˆ·ç•Œé¢

### è‡ªç„¶è¯­è¨€å¤„ç†
Natural Language Processing (NLP) enables:
- Chatbots and virtual assistants
- Sentiment analysis of user feedback
- Automatic documentation generation

## 6. å®‰å…¨æ€§å¢å¼º

### å¨èƒæ£€æµ‹
AI-powered security tools can:
- Detect unusual access patterns
- Identify potential security vulnerabilities
- Automate incident response

### ä»£ç å®‰å…¨åˆ†æ
```python
# AI can detect security issues like:
import sqlite3

def get_user(user_id):
    # SQL injection vulnerability
    query = f"SELECT * FROM users WHERE id = {user_id}"
    return execute_query(query)

# Secure version:
def get_user_secure(user_id):
    query = "SELECT * FROM users WHERE id = ?"
    return execute_query(query, (user_id,))
```

## 7. æœªæ¥å‘å±•è¶‹åŠ¿

### ä½ä»£ç /æ— ä»£ç å¹³å°
- AIé©±åŠ¨çš„å¯è§†åŒ–å¼€å‘
- è‡ªç„¶è¯­è¨€è½¬ä»£ç 
- æ™ºèƒ½ç»„ä»¶æ¨è

### è‡ªä¸»è½¯ä»¶å¼€å‘
The future may include:
- Fully autonomous code generation
- Self-healing applications
- Predictive maintenance systems

## ç»“è®º

äººå·¥æ™ºèƒ½æ­£åœ¨æ·±åˆ»æ”¹å˜è½¯ä»¶å¼€å‘çš„æ–¹å¼ï¼Œä»æé«˜å¼€å‘æ•ˆç‡åˆ°å¢å¼ºè½¯ä»¶è´¨é‡ï¼ŒAIæŠ€æœ¯ä¸ºå¼€å‘è€…æä¾›äº†å¼ºå¤§çš„å·¥å…·ã€‚éšç€æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œæˆ‘ä»¬å¯ä»¥æœŸå¾…æ›´å¤šåˆ›æ–°çš„AIåº”ç”¨å‡ºç°åœ¨è½¯ä»¶å¼€å‘é¢†åŸŸã€‚

AI is not replacing developers, but rather augmenting their capabilities and enabling them to focus on higher-level creative and strategic tasks. The key is to embrace these technologies while maintaining a deep understanding of fundamental software engineering principles.

---

**å‚è€ƒèµ„æ–™ References:**
1. "The State of AI in Software Development" - GitHub, 2023
2. "Machine Learning for Software Engineering" - IEEE Computer Society
3. "AI-Driven Development: The Future of Programming" - ACM Communications
4. ã€Šäººå·¥æ™ºèƒ½åœ¨è½¯ä»¶å·¥ç¨‹ä¸­çš„åº”ç”¨ã€‹- æ¸…åå¤§å­¦å‡ºç‰ˆç¤¾
5. ã€Šæ™ºèƒ½åŒ–è½¯ä»¶å¼€å‘å®è·µã€‹- æœºæ¢°å·¥ä¸šå‡ºç‰ˆç¤¾"""
    
    return {
        "title": "ç°ä»£è½¯ä»¶å¼€å‘ä¸­çš„äººå·¥æ™ºèƒ½åº”ç”¨ - AI Applications in Modern Software Development",
        "content": content,
        "url": "https://example.com/ai-in-software-development"
    }

def demonstrate_chunking_strategies():
    """Demonstrate different chunking strategies with the demo document"""
    
    print("ğŸ” Chunking Strategy Demonstration")
    print("=" * 80)
    
    # Create demo document
    demo_doc = create_demo_document()
    print(f"ğŸ“„ Demo Document: {demo_doc['title']}")
    print(f"ğŸ“ Document Length: {len(demo_doc['content'])} characters")
    print(f"ğŸ”— URL: {demo_doc['url']}")
    print()
    
    # Define different chunking strategies
    strategies = [
        {
            "name": "Small_Chunks (ç²¾ç»†åˆ†å—)",
            "config": {
                "chunk_size": 400,
                "chunk_overlap": 50,
                "min_chunk_size": 50,
                "max_chunk_size": 600,
                "enable_chinese_segmentation": True,
                "enable_keyword_extraction": True
            },
            "description": "é€‚åˆé«˜ç²¾åº¦æ£€ç´¢ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§"
        },
        {
            "name": "Medium_Chunks (ä¸­ç­‰åˆ†å—)",
            "config": {
                "chunk_size": 800,
                "chunk_overlap": 100,
                "min_chunk_size": 100,
                "max_chunk_size": 1200,
                "enable_chinese_segmentation": True,
                "enable_keyword_extraction": True
            },
            "description": "å¹³è¡¡æ£€ç´¢ç²¾åº¦å’Œå¤„ç†æ•ˆç‡"
        },
        {
            "name": "Large_Chunks (å¤§å—åˆ†å—)",
            "config": {
                "chunk_size": 1200,
                "chunk_overlap": 150,
                "min_chunk_size": 150,
                "max_chunk_size": 1800,
                "enable_chinese_segmentation": True,
                "enable_keyword_extraction": True
            },
            "description": "ä¿æŒæ›´å¤šä¸Šä¸‹æ–‡ï¼Œé€‚åˆé•¿æ–‡æ¡£ç†è§£"
        },
        {
            "name": "No_Chinese_Seg (æ— ä¸­æ–‡åˆ†è¯)",
            "config": {
                "chunk_size": 800,
                "chunk_overlap": 100,
                "min_chunk_size": 100,
                "max_chunk_size": 1200,
                "enable_chinese_segmentation": False,
                "enable_keyword_extraction": True
            },
            "description": "ä¸ä½¿ç”¨ä¸­æ–‡åˆ†è¯ï¼Œå¯¹æ¯”æ•ˆæœ"
        }
    ]
    
    results = []
    
    # Test each strategy
    for strategy in strategies:
        print(f"ğŸ§ª Testing Strategy: {strategy['name']}")
        print(f"ğŸ“ Description: {strategy['description']}")
        print(f"âš™ï¸  Configuration: {strategy['config']}")
        
        # Create processor with current strategy
        processor = EnhancedTextProcessor(strategy['config'])
        
        # Measure processing time
        start_time = time.time()
        chunks = processor.process_search_results([demo_doc])
        processing_time = time.time() - start_time
        
        # Calculate metrics
        total_chunks = len(chunks)
        avg_chunk_size = sum(len(chunk.content) for chunk in chunks) // total_chunks if total_chunks > 0 else 0
        avg_token_count = sum(chunk.token_count for chunk in chunks) / total_chunks if total_chunks > 0 else 0
        avg_importance_score = sum(chunk.importance_score for chunk in chunks) / total_chunks if total_chunks > 0 else 0
        chunks_in_optimal_range = sum(1 for chunk in chunks if 20 <= chunk.token_count <= 500)
        
        # Store results
        result = {
            "strategy": strategy['name'],
            "total_chunks": total_chunks,
            "avg_chunk_size": avg_chunk_size,
            "avg_token_count": avg_token_count,
            "avg_importance_score": avg_importance_score,
            "processing_time": processing_time,
            "chunks_in_optimal_range": chunks_in_optimal_range,
            "optimal_range_percentage": (chunks_in_optimal_range / total_chunks * 100) if total_chunks > 0 else 0,
            "chunks": chunks
        }
        results.append(result)
        
        # Display results
        print(f"ğŸ“Š Results:")
        print(f"   â€¢ Total Chunks: {total_chunks}")
        print(f"   â€¢ Average Chunk Size: {avg_chunk_size} characters")
        print(f"   â€¢ Average Token Count: {avg_token_count:.1f} tokens")
        print(f"   â€¢ Average Importance Score: {avg_importance_score:.2f}")
        print(f"   â€¢ Processing Time: {processing_time:.4f} seconds")
        print(f"   â€¢ Chunks in Optimal Range (20-500 tokens): {chunks_in_optimal_range}/{total_chunks} ({chunks_in_optimal_range/total_chunks*100:.1f}%)")
        
        # Show first few chunks as examples
        print(f"ğŸ“‹ Sample Chunks:")
        for i, chunk in enumerate(chunks[:3]):  # Show first 3 chunks
            print(f"   Chunk {i+1}:")
            print(f"     - Language: {chunk.language}")
            print(f"     - Token Count: {chunk.token_count}")
            print(f"     - Importance Score: {chunk.importance_score:.2f}")
            print(f"     - Keywords: {chunk.keywords[:5]}")  # Show first 5 keywords
            print(f"     - Content Preview: {chunk.content[:150]}...")
            print()
        
        print("-" * 80)
        print()
    
    # Summary comparison
    print("ğŸ“ˆ Strategy Comparison Summary")
    print("=" * 80)
    print(f"{'Strategy':<25} {'Chunks':<8} {'Avg Size':<10} {'Avg Tokens':<12} {'Optimal %':<10} {'Time (s)':<10}")
    print("-" * 80)
    
    for result in results:
        print(f"{result['strategy']:<25} {result['total_chunks']:<8} {result['avg_chunk_size']:<10} "
              f"{result['avg_token_count']:<12.1f} {result['optimal_range_percentage']:<10.1f} {result['processing_time']:<10.4f}")
    
    # Find best strategy
    best_strategy = max(results, key=lambda x: x['optimal_range_percentage'])
    print()
    print(f"ğŸ† Best Strategy: {best_strategy['strategy']}")
    print(f"   â€¢ Optimal Range Coverage: {best_strategy['optimal_range_percentage']:.1f}%")
    print(f"   â€¢ Average Importance Score: {best_strategy['avg_importance_score']:.2f}")
    print(f"   â€¢ Processing Efficiency: {best_strategy['processing_time']:.4f}s")
    
    return results

def analyze_chunk_content(chunks, strategy_name):
    """Analyze the content distribution of chunks"""
    
    print(f"\nğŸ” Detailed Analysis for {strategy_name}")
    print("=" * 60)
    
    # Language distribution
    language_dist = {}
    for chunk in chunks:
        lang = chunk.language
        language_dist[lang] = language_dist.get(lang, 0) + 1
    
    print("ğŸŒ Language Distribution:")
    for lang, count in language_dist.items():
        print(f"   â€¢ {lang}: {count} chunks ({count/len(chunks)*100:.1f}%)")
    
    # Token distribution
    token_ranges = {
        "Very Small (< 50)": 0,
        "Small (50-100)": 0,
        "Optimal (100-300)": 0,
        "Large (300-500)": 0,
        "Very Large (> 500)": 0
    }
    
    for chunk in chunks:
        tokens = chunk.token_count
        if tokens < 50:
            token_ranges["Very Small (< 50)"] += 1
        elif tokens < 100:
            token_ranges["Small (50-100)"] += 1
        elif tokens < 300:
            token_ranges["Optimal (100-300)"] += 1
        elif tokens < 500:
            token_ranges["Large (300-500)"] += 1
        else:
            token_ranges["Very Large (> 500)"] += 1
    
    print("\nğŸ“ Token Count Distribution:")
    for range_name, count in token_ranges.items():
        print(f"   â€¢ {range_name}: {count} chunks ({count/len(chunks)*100:.1f}%)")
    
    # Content type analysis
    code_chunks = sum(1 for chunk in chunks if '```' in chunk.content or 'def ' in chunk.content or 'function' in chunk.content)
    header_chunks = sum(1 for chunk in chunks if chunk.content.strip().startswith('#'))
    
    print(f"\nğŸ“ Content Type Analysis:")
    print(f"   â€¢ Code-containing chunks: {code_chunks} ({code_chunks/len(chunks)*100:.1f}%)")
    print(f"   â€¢ Header-starting chunks: {header_chunks} ({header_chunks/len(chunks)*100:.1f}%)")
    
    # Top keywords across all chunks
    all_keywords = []
    for chunk in chunks:
        all_keywords.extend(chunk.keywords)
    
    keyword_freq = {}
    for keyword in all_keywords:
        keyword_freq[keyword] = keyword_freq.get(keyword, 0) + 1
    
    top_keywords = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)[:10]
    
    print(f"\nğŸ”‘ Top Keywords:")
    for keyword, freq in top_keywords:
        print(f"   â€¢ {keyword}: {freq} occurrences")

def save_demo_results(results, filename="chunking_demo_results.json"):
    """Save demonstration results to JSON file"""
    
    # Prepare data for JSON serialization
    json_results = []
    for result in results:
        json_result = {
            "strategy": result["strategy"],
            "total_chunks": result["total_chunks"],
            "avg_chunk_size": result["avg_chunk_size"],
            "avg_token_count": result["avg_token_count"],
            "avg_importance_score": result["avg_importance_score"],
            "processing_time": result["processing_time"],
            "chunks_in_optimal_range": result["chunks_in_optimal_range"],
            "optimal_range_percentage": result["optimal_range_percentage"],
            "sample_chunks": []
        }
        
        # Add sample chunks (first 3)
        for chunk in result["chunks"][:3]:
            json_result["sample_chunks"].append({
                "content": chunk.content[:200] + "..." if len(chunk.content) > 200 else chunk.content,
                "language": chunk.language,
                "token_count": chunk.token_count,
                "importance_score": chunk.importance_score,
                "keywords": chunk.keywords[:5]  # First 5 keywords
            })
        
        json_results.append(json_result)
    
    # Save to file
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(json_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ Demo results saved to {filename}")

if __name__ == "__main__":
    print("ğŸš€ Starting Chunking Strategy Demonstration")
    print()
    
    # Run the demonstration
    results = demonstrate_chunking_strategies()
    
    # Detailed analysis for the best strategy
    best_result = max(results, key=lambda x: x['optimal_range_percentage'])
    analyze_chunk_content(best_result['chunks'], best_result['strategy'])
    
    # Save results
    save_demo_results(results)
    
    print("\nâœ… Demonstration completed!")
    print("\nKey Insights:")
    print("â€¢ Small_Chunks strategy typically provides the best balance for mixed Chinese-English content")
    print("â€¢ Chinese segmentation significantly improves chunking quality for mixed-language documents")
    print("â€¢ Optimal token range (20-500) is crucial for effective vector embedding")
    print("â€¢ Processing time varies based on chunk size and language processing features")