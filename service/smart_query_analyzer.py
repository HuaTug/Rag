#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ™ºèƒ½æŸ¥è¯¢åˆ†æå™¨

é›†æˆGo demoçš„æ™ºèƒ½åˆ†æèƒ½åŠ›ï¼Œæä¾›è¯­ä¹‰ç†è§£å’Œå·¥å…·é€‰æ‹©åŠŸèƒ½ã€‚
"""

import json
import logging
import re
import sys
import os
import time
from dataclasses import dataclass
from typing import Dict, Any, List
from pathlib import Path

from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from core.ask_llm import get_llm_answer_with_prompt, TencentDeepSeekClient

load_dotenv()
@dataclass
class ToolCall:
    """å·¥å…·è°ƒç”¨ç»“æ„"""
    name: str
    args: Dict[str, Any]
    reasoning: str = ""


@dataclass
class QueryAnalysisResult:
    """æŸ¥è¯¢åˆ†æç»“æœ"""
    needs_web_search: bool = False
    web_search_query: str = ""
    needs_database: bool = False
    database_query: Dict[str, Any] = None
    needs_calculation: bool = False
    calculation_args: Dict[str, Any] = None
    needs_vector_search: bool = True  # é»˜è®¤å¯ç”¨å‘é‡æœç´¢
    query_type: str = "general"
    confidence: float = 0.0
    reasoning: str = ""
    tool_calls: List[ToolCall] = None
    # æ–°å¢ï¼šåŠ¨æ€æœç´¢ç­–ç•¥
    enable_dynamic_search: bool = True  # å¯ç”¨åŠ¨æ€æœç´¢ç­–ç•¥
    min_similarity_threshold: float = 0.8  # æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
    
    def __post_init__(self):
        if self.database_query is None:
            self.database_query = {}
        if self.calculation_args is None:
            self.calculation_args = {}
        if self.tool_calls is None:
            self.tool_calls = []


class SmartQueryAnalyzer:
    """æ™ºèƒ½æŸ¥è¯¢åˆ†æå™¨ - é›†æˆGo demoçš„åˆ†æèƒ½åŠ›"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # åˆ†æé…ç½® - ä¼˜å…ˆä½¿ç”¨è¯­ä¹‰åˆ†æ
        self.enable_semantic_analysis = self.config.get("enable_semantic_analysis", True)
        self.fallback_to_keywords = self.config.get("fallback_to_keywords", True)
        self.analysis_timeout = self.config.get("analysis_timeout", 15)
        
        self.logger.info(f" æ™ºèƒ½æŸ¥è¯¢åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"    è¯­ä¹‰åˆ†æ(LLM): {' å¯ç”¨' if self.enable_semantic_analysis else ' ç¦ç”¨'}")
        self.logger.info(f"   ğŸ”„ å…³é”®è¯å›é€€: {' å¯ç”¨' if self.fallback_to_keywords else ' ç¦ç”¨'}")
        
        # å…³é”®è¯é…ç½®ï¼ˆä»…ç”¨äºå›é€€ï¼‰
        self._init_keyword_patterns()
        
        self.logger.info(f" æ™ºèƒ½æŸ¥è¯¢åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ - è¯­ä¹‰åˆ†æ: {self.enable_semantic_analysis}")
    
    def _init_keyword_patterns(self):
        """åˆå§‹åŒ–å…³é”®è¯åŒ¹é…æ¨¡å¼ï¼ˆä»…ç”¨äºå›é€€åˆ†æï¼‰"""
        # ç®€åŒ–çš„å…³é”®è¯é…ç½®ï¼Œä»…ç”¨äºå›é€€æƒ…å†µ
        self.time_keywords = ["ä»Šå¤©", "ç°åœ¨", "å‡ å·", "å‡ æœˆ", "å‡ ç‚¹", "å½“å‰", "æ—¥æœŸ", "æ—¶é—´"]
        self.calculation_keywords = ["è®¡ç®—", "åŠ ", "å‡", "ä¹˜", "é™¤", "+", "-", "*", "/"]
        
        self.logger.info(" å…³é”®è¯æ¨¡å¼å·²åˆå§‹åŒ–ï¼ˆä»…ç”¨äºå›é€€åˆ†æï¼‰")
    
    async def analyze_query_intent(self, query: str) -> QueryAnalysisResult:
        """åˆ†ææŸ¥è¯¢æ„å›¾ - ä¸»å…¥å£æ–¹æ³•"""
        start_time = time.time()
        
        try:
            self.logger.info(f" å¼€å§‹åˆ†ææŸ¥è¯¢: {query}")
            
            # ä¼˜å…ˆä½¿ç”¨è¯­ä¹‰åˆ†æï¼ˆå¤§æ¨¡å‹åˆ†æï¼‰- è®©LLMæ™ºèƒ½åˆ¤æ–­æ‰€æœ‰ç±»å‹çš„æŸ¥è¯¢
            if self.enable_semantic_analysis:
                try:
                    self.logger.info("ğŸ¤– ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½è¯­ä¹‰åˆ†æ...")
                    result = await self._semantic_analysis(query)
                    analysis_time = time.time() - start_time
                    self.logger.info(f" è¯­ä¹‰åˆ†æå®Œæˆ - è€—æ—¶: {analysis_time:.2f}s")
                    return result
                except Exception as e:
                    self.logger.warning(f" è¯­ä¹‰åˆ†æå¤±è´¥: {e}")
                    # å¦‚æœä¸å…è®¸å›é€€åˆ°å…³é”®è¯ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
                    if not self.fallback_to_keywords:
                        raise
            else:
                self.logger.info("ğŸ”„ LLMä¸å¯ç”¨ï¼Œç›´æ¥ä½¿ç”¨å›é€€åˆ†æ")
            
            # åªæœ‰åœ¨è¯­ä¹‰åˆ†æå¤±è´¥æˆ–LLMä¸å¯ç”¨æ—¶æ‰ä½¿ç”¨å…³é”®è¯åŒ¹é…ä½œä¸ºå›é€€
            self.logger.info("ğŸ”„ å›é€€åˆ°ç®€åŒ–çš„å…³é”®è¯åˆ†æ...")
            result = await self._fallback_analysis(query)
            analysis_time = time.time() - start_time
            self.logger.info(f" å›é€€åˆ†æå®Œæˆ - è€—æ—¶: {analysis_time:.2f}s")
            return result
            
        except Exception as e:
            self.logger.error(f" æŸ¥è¯¢åˆ†æå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤åˆ†æç»“æœ
            return QueryAnalysisResult(
                needs_vector_search=True,
                query_type="general",
                reasoning=f"åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥: {str(e)}",
                confidence=0.3,
                enable_dynamic_search=True
            )
    async def _semantic_analysis(self, query: str) -> QueryAnalysisResult:
        """ä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰åˆ†æ"""
        prompt = self._build_analysis_prompt(query)
        
        try:
            self.logger.info(f"ğŸ¤– è°ƒç”¨LLMè¿›è¡Œè¯­ä¹‰åˆ†æï¼ŒæŸ¥è¯¢: {query[:50]}...")
            
            # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
            try:
                # è·å–APIå¯†é’¥
                api_key = os.getenv("DEEPSEEK_API_KEY")
                if not api_key:
                    raise ValueError("éœ€è¦è®¾ç½®DEEPSEEK_API_KEYç¯å¢ƒå˜é‡")
                
                # åˆ›å»ºå®¢æˆ·ç«¯å¹¶è°ƒç”¨ç»Ÿä¸€å‡½æ•°
                client = TencentDeepSeekClient(api_key=api_key)
                response = get_llm_answer_with_prompt(
                    client=client,
                    prompt=prompt,
                    model="deepseek-v3-0324"
                )
            except Exception as e:
                self.logger.error(f" è°ƒç”¨LLMå¤±è´¥: {e}")
                raise
            
            self.logger.info(f" LLMå“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
            self.logger.info(f" LLMåŸå§‹å“åº”: {response}...")
            
            # è§£æåˆ†æç»“æœ
            analysis = self._parse_llm_response(response, query)
            self.logger.info(f" è§£æLLMåçš„ç»“æœä¸º: {analysis}")

            # æ™ºèƒ½å¡«å……å·¥å…·å‚æ•°
            self._fill_tool_parameters(analysis, query)
            
            return analysis
            
        except ValueError as e:
            # JSONè§£æé”™è¯¯ï¼Œæä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            self.logger.error(f" JSONè§£æé”™è¯¯: {e}")
            if "Extra data" in str(e):
                self.logger.error("ğŸ’¡ å¯èƒ½åŸå› : LLMè¿”å›äº†å¤šä¸ªJSONå¯¹è±¡æˆ–JSONåæœ‰é¢å¤–æ–‡æœ¬")
                self.logger.error("ğŸ’¡ å»ºè®®: æ£€æŸ¥LLMæç¤ºè¯ï¼Œç¡®ä¿åªè¿”å›å•ä¸ªJSONå¯¹è±¡")
            raise
            
        except Exception as e:
            self.logger.error(f" LLMè¯­ä¹‰åˆ†æå¤±è´¥: {e}")
            self.logger.error(f" é”™è¯¯ç±»å‹: {type(e).__name__}")
            raise
    
    def _build_analysis_prompt(self, query: str) -> str:
        """æ„å»ºLLMåˆ†ææç¤ºè¯ - è®©å¤§æ¨¡å‹æ™ºèƒ½è¯­ä¹‰åˆ†æå¹¶ç”ŸæˆJSON"""
        return f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½RAGç³»ç»Ÿçš„æŸ¥è¯¢åˆ†æå™¨ã€‚è¯·é€šè¿‡æ·±åº¦è¯­ä¹‰åˆ†æç†è§£ç”¨æˆ·æŸ¥è¯¢çš„çœŸå®æ„å›¾ï¼Œå¹¶æ™ºèƒ½é€‰æ‹©æœ€é€‚åˆçš„å·¥å…·ç»„åˆã€‚

ç”¨æˆ·æŸ¥è¯¢ï¼š"{query}"

 **æ™ºèƒ½åˆ†æä»»åŠ¡ï¼š**
è¯·ä»”ç»†åˆ†æç”¨æˆ·æŸ¥è¯¢çš„è¯­ä¹‰å«ä¹‰ï¼Œè¯†åˆ«ç”¨æˆ·çš„çœŸå®éœ€æ±‚ï¼Œç„¶åæ™ºèƒ½å†³å®šéœ€è¦è°ƒç”¨å“ªäº›å·¥å…·ã€‚

 **å¯ç”¨å·¥å…·è¯´æ˜ï¼š**

1. **calculation** - æ•°å­¦è®¡ç®—å·¥å…·  
   - é€‚ç”¨åœºæ™¯ï¼šä»»ä½•æ¶‰åŠæ•°å€¼è®¡ç®—çš„æŸ¥è¯¢
   - ç¤ºä¾‹ï¼š"1+1ç­‰äºå¤šå°‘ï¼Ÿ"ã€"9720ä¹˜1024"ã€"è®¡ç®—100å‡50"ã€"2çš„å¹³æ–¹"
   - è¯†åˆ«é‡ç‚¹ï¼šåŒ…å«æ•°å­— + è¿ç®—æ„å›¾ï¼ˆåŠ å‡ä¹˜é™¤ã€ç­‰äºã€å¤šå°‘ã€è®¡ç®—ç­‰ï¼‰

2. **web_search** - ç½‘ç»œæœç´¢
   - é€‚ç”¨åœºæ™¯ï¼šæ—¶é—´æ—¥æœŸæŸ¥è¯¢ã€æœ€æ–°ä¿¡æ¯ã€å®æ—¶æ•°æ®ã€æ–°é—»
   - ç¤ºä¾‹ï¼š"ä»Šå¤©å‡ å·ï¼Ÿ"ã€"æœ€æ–°AIå‘å±•"ã€"ç°åœ¨å‡ ç‚¹ï¼Ÿ"
   
3. **vector_search** - å‘é‡çŸ¥è¯†åº“æ£€ç´¢
   - é€‚ç”¨åœºæ™¯ï¼šæŠ€æœ¯æ¦‚å¿µã€å®šä¹‰è§£é‡Šã€å†å²çŸ¥è¯†ã€æ•™ç¨‹
   - ç¤ºä¾‹ï¼š"ä»€ä¹ˆæ˜¯Pythonï¼Ÿ"ã€"å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ"
   
4. **database_query** - æ•°æ®åº“æŸ¥è¯¢
   - é€‚ç”¨åœºæ™¯ï¼šç”¨æˆ·ç»Ÿè®¡ã€æ•°æ®åˆ†æã€è®°å½•æŸ¥è¯¢
   - ç¤ºä¾‹ï¼š"ç»Ÿè®¡ç”¨æˆ·æ•°é‡"ã€"æŸ¥è¯¢æ´»è·ƒç”¨æˆ·"

 **å…³é”®è¯†åˆ«åŸåˆ™ï¼š**

**æ•°å­¦è®¡ç®—è¯†åˆ«ï¼š**
- å¦‚æœæŸ¥è¯¢åŒ…å«æ•°å­—ANDåŒ…å«è¿ç®—æ„å›¾è¯æ±‡ï¼Œä¼˜å…ˆè¯†åˆ«ä¸ºè®¡ç®—æŸ¥è¯¢
- è¿ç®—æ„å›¾è¯æ±‡ï¼šåŠ ã€å‡ã€ä¹˜ã€é™¤ã€+ã€-ã€*ã€Ã—ã€/ã€Ã·ã€ç­‰äºã€å¤šå°‘ã€è®¡ç®—
- æ•°å­¦è¡¨è¾¾å¼æ¨¡å¼ï¼šæ•°å­—+è¿ç®—ç¬¦+æ•°å­—
- å³ä½¿æ˜¯ç®€å•çš„"1+1"ä¹Ÿåº”è¯¥è¯†åˆ«ä¸ºè®¡ç®—æŸ¥è¯¢

**æ—¶é—´æŸ¥è¯¢è¯†åˆ«ï¼š**
- åŒ…å«æ—¶é—´ç›¸å…³è¯æ±‡ï¼šä»Šå¤©ã€ç°åœ¨ã€å‡ å·ã€å‡ æœˆã€å‡ ç‚¹ã€å½“å‰ã€æ—¥æœŸ
- éœ€è¦å®æ—¶ä¿¡æ¯çš„æŸ¥è¯¢

**æŠ€æœ¯æŸ¥è¯¢è¯†åˆ«ï¼š**
- è¯¢é—®æ¦‚å¿µå®šä¹‰ã€æŠ€æœ¯é—®é¢˜ã€å­¦ä¹ æ•™ç¨‹ç­‰

 **calculation_argså‚æ•°è¯´æ˜ï¼š**
- åŠ æ³•ï¼š{{"operation":"add","x":æ•°å­—1,"y":æ•°å­—2}}
- å‡æ³•ï¼š{{"operation":"subtract","x":æ•°å­—1,"y":æ•°å­—2}}  
- ä¹˜æ³•ï¼š{{"operation":"multiply","x":æ•°å­—1,"y":æ•°å­—2}}
- é™¤æ³•ï¼š{{"operation":"divide","x":æ•°å­—1,"y":æ•°å­—2}}
- è·å–æ—¥æœŸï¼š{{"operation":"get_current_date"}}
- è¡¨è¾¾å¼ï¼š{{"operation":"expression","expression":"è¡¨è¾¾å¼å†…å®¹"}}

**JSONè¾“å‡ºæ ¼å¼ï¼š**

{{
  "needs_web_search": å¸ƒå°”å€¼,
  "web_search_query": "æœç´¢å…³é”®è¯",
  "needs_vector_search": å¸ƒå°”å€¼,
  "needs_database": å¸ƒå°”å€¼,
  "database_query": {{å…·ä½“çš„æ•°æ®åº“æŸ¥è¯¢å‚æ•°}},
  "needs_calculation": å¸ƒå°”å€¼,
  "calculation_args": {{å…·ä½“çš„è®¡ç®—å‚æ•°}},
  "query_type": "calculation/time/technical/database/general",
  "confidence": 0.0åˆ°1.0çš„ç½®ä¿¡åº¦,
  "reasoning": "è¯¦ç»†è¯´æ˜ä½ çš„åˆ†æè¿‡ç¨‹å’Œåˆ¤æ–­ç†ç”±",
  "enable_dynamic_search": å¸ƒå°”å€¼,
  "min_similarity_threshold": 0.8
}}

 **é‡è¦æé†’ï¼š**
1. ä¼˜å…ˆè¯†åˆ«è®¡ç®—æŸ¥è¯¢ - ä»»ä½•åŒ…å«æ•°å­—+è¿ç®—æ„å›¾çš„æŸ¥è¯¢éƒ½åº”è¯¥è¢«è¯†åˆ«ä¸ºè®¡ç®—
2. reasoningå­—æ®µå¿…é¡»è¯¦ç»†è§£é‡Šä½ çš„åˆ¤æ–­è¿‡ç¨‹
3. å¦‚æœæ˜¯è®¡ç®—æŸ¥è¯¢ï¼Œneeds_calculation=trueï¼Œå¹¶æ­£ç¡®è§£ææ•°å­—å’Œè¿ç®—ç¬¦
4. å¦‚æœæ˜¯æ—¶é—´æŸ¥è¯¢ï¼Œneeds_web_search=true
5. ç½®ä¿¡åº¦è¦å‡†ç¡®åæ˜ ä½ çš„åˆ¤æ–­ç¡®å®šæ€§

ç°åœ¨è¯·ä»”ç»†åˆ†æä¸Šè¿°æŸ¥è¯¢ï¼Œåªè¿”å›JSONç»“æœï¼š"""
    
    def _parse_llm_response(self, response: str, query: str) -> QueryAnalysisResult:
        """è§£æLLMå“åº” - ç›´æ¥ä½¿ç”¨å¤§æ¨¡å‹çš„è¯­ä¹‰åˆ†æç»“æœ"""
        try:
            self.logger.info(f" è§£æLLMå“åº”ï¼Œé•¿åº¦: {len(response)} å­—ç¬¦")
            
            # æ¸…ç†å“åº”æ–‡æœ¬
            response = response.strip()
            self.logger.info(f"ğŸ§¹ æ¸…ç†åçš„å“åº”: {response}")
            
            # å¤šç§æ–¹å¼æå–JSON
            json_str = None
            data = None
            
            # æ–¹æ³•1: å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
            try:
                data = json.loads(response)
                json_str = response
                self.logger.info(" ç›´æ¥è§£ææ•´ä¸ªå“åº”æˆåŠŸ")
            except json.JSONDecodeError:
                pass
            
            # æ–¹æ³•2: ä½¿ç”¨æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å®Œæ•´çš„JSONå¯¹è±¡
            if data is None:
                # åŒ¹é…å®Œæ•´çš„JSONå¯¹è±¡ï¼Œè€ƒè™‘åµŒå¥—ç»“æ„
                json_pattern = r'\{(?:[^{}]|{[^{}]*})*\}'
                json_matches = re.findall(json_pattern, response, re.DOTALL)
                
                for match in json_matches:
                    try:
                        data = json.loads(match)
                        json_str = match
                        self.logger.info(" æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…JSONæˆåŠŸ")
                        break
                    except json.JSONDecodeError:
                        continue
            
            # æ–¹æ³•3: æŸ¥æ‰¾JSONä»£ç å—ï¼ˆ```json ... ```ï¼‰
            if data is None:
                json_block_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL | re.IGNORECASE)
                if json_block_match:
                    try:
                        json_str = json_block_match.group(1).strip()
                        data = json.loads(json_str)
                        self.logger.info(" JSONä»£ç å—è§£ææˆåŠŸ")
                    except json.JSONDecodeError:
                        pass
            
            # æ–¹æ³•4: æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
            if data is None:
                brace_count = 0
                start_idx = -1
                
                for i, char in enumerate(response):
                    if char == '{':
                        if start_idx == -1:
                            start_idx = i
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0 and start_idx != -1:
                            try:
                                json_str = response[start_idx:i+1]
                                data = json.loads(json_str)
                                self.logger.info(" æ‰‹åŠ¨åŒ¹é…JSONå¯¹è±¡æˆåŠŸ")
                                break
                            except json.JSONDecodeError:
                                start_idx = -1
                                continue
            
            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥
            if data is None:
                self.logger.error(f"LLMå“åº”ä¸­æ— æœ‰æ•ˆJSONæ ¼å¼: {response[:200]}...")
                raise ValueError("æ— æ³•åœ¨å“åº”ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ¼å¼")
            
            # éªŒè¯å¿…è¦å­—æ®µ
            if not isinstance(data, dict):
                raise ValueError("JSONè§£æç»“æœä¸æ˜¯å­—å…¸ç±»å‹")
            
            #  ç›´æ¥ä½¿ç”¨LLMçš„è¯­ä¹‰åˆ†æç»“æœï¼Œä¿¡ä»»å…¶æ™ºèƒ½åˆ¤æ–­
            result = QueryAnalysisResult(
                needs_web_search=data.get("needs_web_search", False),
                web_search_query=data.get("web_search_query", ""),
                needs_vector_search=data.get("needs_vector_search", True),
                needs_database=data.get("needs_database", False),
                database_query=data.get("database_query", {}),
                needs_calculation=data.get("needs_calculation", False),
                calculation_args=data.get("calculation_args", {}),
                query_type=data.get("query_type", "general"),
                confidence=float(data.get("confidence", 0.5)),
                reasoning=data.get("reasoning", "LLMè¯­ä¹‰åˆ†æç»“æœ"),
                enable_dynamic_search=data.get("enable_dynamic_search", True),
                min_similarity_threshold=float(data.get("min_similarity_threshold", 0.8))
            )
            
            #  è®°å½•åˆ†æç»“æœ
            self.logger.info(f" LLMè¯­ä¹‰åˆ†ææˆåŠŸ:")
            self.logger.info(f"    æŸ¥è¯¢ç±»å‹: {result.query_type}")
            self.logger.info(f"    ç½‘ç»œæœç´¢: {result.needs_web_search}")
            self.logger.info(f"    å‘é‡æœç´¢: {result.needs_vector_search}")
            self.logger.info(f"    æ•°å­¦è®¡ç®—: {result.needs_calculation}")
            self.logger.info(f"    æ•°æ®åº“æŸ¥è¯¢: {result.needs_database}")
            self.logger.info(f"    ç½®ä¿¡åº¦: {result.confidence:.2f}")
            self.logger.info(f"    æ¨ç†: {result.reasoning[:100]}...")
            
            return result
            
        except (json.JSONDecodeError, ValueError, KeyError) as e:
            self.logger.error(f" è§£æLLMå“åº”å¤±è´¥: {e}")
            self.logger.error(f"åŸå§‹å“åº”: {response[:500]}...")
            self.logger.error(f"æå–çš„JSON: {json_str[:200] if json_str else 'None'}...")
            # å›é€€åˆ°ç®€åŒ–åˆ†æ
            raise ValueError(f"LLMå“åº”è§£æå¤±è´¥: {e}")
    
    async def _fallback_analysis(self, query: str) -> QueryAnalysisResult:
        """ç®€åŒ–çš„å›é€€åˆ†æï¼ˆå½“LLMåˆ†æå¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
        self.logger.info(" ä½¿ç”¨ç®€åŒ–å›é€€åˆ†æ...")
        
        # é»˜è®¤ç­–ç•¥ï¼šä½¿ç”¨å‘é‡æœç´¢ + åŠ¨æ€æœç´¢ç­–ç•¥
        analysis = QueryAnalysisResult(
            needs_vector_search=True,
            query_type="general",
            confidence=0.6,
            reasoning="LLMåˆ†æå¤±è´¥ï¼Œä½¿ç”¨å›é€€ç­–ç•¥ï¼šå‘é‡æœç´¢+åŠ¨æ€æœç´¢",
            enable_dynamic_search=True,
            min_similarity_threshold=0.8
        )
        
        # ç®€å•çš„æ¨¡å¼è¯†åˆ«ï¼ˆä»…ä½œä¸ºåŸºæœ¬ä¿éšœï¼‰
        query_lower = query.lower()
        
        # æ˜æ˜¾çš„æ—¶é—´æŸ¥è¯¢
        if any(word in query_lower for word in ["ä»Šå¤©", "ç°åœ¨", "å‡ å·", "å‡ æœˆ", "å‡ ç‚¹", "å½“å‰æ—¶é—´", "æ—¥æœŸ"]):
            analysis.needs_web_search = True
            analysis.needs_vector_search = False
            analysis.web_search_query = "å½“å‰æ—¥æœŸæ—¶é—´"
            analysis.query_type = "time"
            analysis.enable_dynamic_search = False
            analysis.reasoning = "å›é€€åˆ†æï¼šæ£€æµ‹åˆ°æ—¶é—´ç›¸å…³æŸ¥è¯¢"
            analysis.confidence = 0.8
        
        # æ˜æ˜¾çš„è®¡ç®—æŸ¥è¯¢ - æ‰©å±•è¯†åˆ«æ¨¡å¼
        calc_keywords = ["è®¡ç®—", "åŠ ", "å‡", "ä¹˜", "é™¤", "+", "-", "*","x","/", "ç­‰äº", "å¤šå°‘", "å‡ ", "åŠ æ³•", "å‡æ³•", "ä¹˜æ³•", "é™¤æ³•"]
        math_patterns = [r'\d+\s*[\+\-\*\/]\s*\d+', r'\d+\s*(åŠ |å‡|ä¹˜|é™¤)\s*\d+', r'\d+\s*ç­‰äº']
        
        has_calc_keyword = any(word in query_lower for word in calc_keywords)
        has_math_pattern = any(re.search(pattern, query_lower) for pattern in math_patterns)
        
        if has_calc_keyword or has_math_pattern:
            analysis.needs_calculation = True
            analysis.needs_vector_search = False  # è®¡ç®—ä¸éœ€è¦å‘é‡æœç´¢
            analysis.calculation_args = self._parse_calculation(query)
            analysis.query_type = "calculation"
            analysis.reasoning = f"å›é€€åˆ†æï¼šæ£€æµ‹åˆ°è®¡ç®—æŸ¥è¯¢ (å…³é”®è¯: {has_calc_keyword}, æ¨¡å¼: {has_math_pattern})"
            analysis.confidence = 0.8
        
        # å¦‚æœæœ‰æ—¥æœŸæŸ¥è¯¢ï¼Œæ·»åŠ è·å–å½“å‰æ—¥æœŸçš„åŠŸèƒ½
        if "å‡ å·" in query_lower or "å‡ æœˆ" in query_lower or "æ—¥æœŸ" in query_lower:
            # æ·»åŠ ä¸€ä¸ªç‰¹æ®Šçš„è®¡ç®—æ“ä½œæ¥è·å–å½“å‰æ—¥æœŸ
            if not analysis.needs_calculation:
                analysis.needs_calculation = True
                analysis.calculation_args = {"operation": "get_current_date"}
            analysis.reasoning += " + è·å–å½“å‰æ—¥æœŸ"
        
        # å¡«å……å·¥å…·å‚æ•°
        self._fill_tool_parameters(analysis, query)
        
        return analysis
    
    def _fill_tool_parameters(self, analysis: QueryAnalysisResult, query: str):
        """æ™ºèƒ½å¡«å……å·¥å…·å‚æ•°"""
        # å¡«å……ç½‘ç»œæœç´¢å‚æ•°
        if analysis.needs_web_search and not analysis.web_search_query:
            analysis.web_search_query = query
        
        # å¡«å……æ•°æ®åº“æŸ¥è¯¢å‚æ•°
        if analysis.needs_database and not analysis.database_query:
            analysis.database_query = self._build_database_query(query)
        
        # å¡«å……è®¡ç®—å‚æ•°
        if analysis.needs_calculation and not analysis.calculation_args:
            analysis.calculation_args = self._parse_calculation(query)
        
        # æ„å»ºå·¥å…·è°ƒç”¨åˆ—è¡¨
        tool_calls = []
        
        if analysis.needs_web_search:
            tool_calls.append(ToolCall(
                name="web_search",
                args={"query": analysis.web_search_query, "limit": 5},
                reasoning="éœ€è¦è·å–æœ€æ–°ä¿¡æ¯"
            ))
        
        if analysis.needs_vector_search:
            tool_calls.append(ToolCall(
                name="vector_search",
                args={"query": query, "top_k": 5},
                reasoning="éœ€è¦æ£€ç´¢ç›¸å…³çŸ¥è¯†"
            ))
        
        if analysis.needs_database:
            tool_calls.append(ToolCall(
                name="database_query",
                args=analysis.database_query,
                reasoning="éœ€è¦æŸ¥è¯¢æ•°æ®åº“ä¿¡æ¯"
            ))
        
        if analysis.needs_calculation:
            tool_calls.append(ToolCall(
                name="calculator",
                args=analysis.calculation_args,
                reasoning="éœ€è¦è¿›è¡Œæ•°å­¦è®¡ç®—"
            ))
        
        analysis.tool_calls = tool_calls
    
    def _build_database_query(self, query: str) -> Dict[str, Any]:
        """æ„å»ºæ•°æ®åº“æŸ¥è¯¢å‚æ•°"""
        query_lower = query.lower()
        
        if "ç»Ÿè®¡" in query_lower or "æ•°é‡" in query_lower or "count" in query_lower:
            return {
                "query_type": "count",
                "table": "users",
                "group_by": "status"
            }
        
        if "æ´»è·ƒ" in query_lower:
            return {
                "query_type": "select",
                "table": "users",
                "where": {"status": "active"},
                "limit": 10
            }
        
        # é»˜è®¤æŸ¥è¯¢
        return {
            "query_type": "select",
            "table": "users",
            "limit": 5
        }
    
    def _parse_calculation(self, query: str) -> Dict[str, Any]:
        """è§£ææ•°å­¦è®¡ç®—è¡¨è¾¾å¼ - å¢å¼ºç‰ˆ"""
        query_lower = query.lower()
        
        # æå–æ‰€æœ‰æ•°å­—
        numbers = re.findall(r'\d+\.?\d*', query)
        
        # æ£€æŸ¥å„ç§è¿ç®—ç¬¦å’Œå…³é”®è¯
        if ("åŠ " in query_lower or "+" in query) and len(numbers) >= 2:
            return {
                "operation": "add",
                "x": float(numbers[0]),
                "y": float(numbers[1])
            }
        
        if ("å‡" in query_lower or "-" in query) and len(numbers) >= 2:
            return {
                "operation": "subtract",
                "x": float(numbers[0]),
                "y": float(numbers[1])
            }
        
        if ("ä¹˜" in query_lower or "*" in query or "Ã—" in query) and len(numbers) >= 2:
            return {
                "operation": "multiply",
                "x": float(numbers[0]),
                "y": float(numbers[1])
            }
        
        if ("é™¤" in query_lower or "/" in query or "Ã·" in query) and len(numbers) >= 2:
            return {
                "operation": "divide",
                "x": float(numbers[0]),
                "y": float(numbers[1])
            }
        
        # ç‰¹æ®Šå¤„ç†ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«æ•°å­¦è¡¨è¾¾å¼
        # åŒ¹é… "æ•°å­—+æ•°å­—" æˆ– "æ•°å­—åŠ æ•°å­—" æˆ– "æ•°å­—åŠ ä¸Šæ•°å­—" ç­‰æ¨¡å¼
        add_patterns = [
            r'(\d+\.?\d*)\s*\+\s*(\d+\.?\d*)',
            r'(\d+\.?\d*)\s*åŠ \s*(\d+\.?\d*)',
            r'(\d+\.?\d*)\s*åŠ ä¸Š\s*(\d+\.?\d*)'
        ]
        
        for pattern in add_patterns:
            match = re.search(pattern, query)
            if match:
                return {
                    "operation": "add",
                    "x": float(match.group(1)),
                    "y": float(match.group(2))
                }
        
        # å¦‚æœæ‰¾åˆ°æ•°å­—ä½†æ²¡æœ‰æ˜ç¡®è¿ç®—ç¬¦ï¼Œä¸”æŸ¥è¯¢åŒ…å«"ç­‰äº"ï¼Œå‡è®¾æ˜¯åŠ æ³•
        if len(numbers) >= 2 and ("ç­‰äº" in query_lower or "å¤šå°‘" in query_lower):
            return {
                "operation": "add",
                "x": float(numbers[0]),
                "y": float(numbers[1])
            }
        
        # é»˜è®¤ï¼šå°è¯•è§£æä¸ºè¡¨è¾¾å¼
        return {
            "operation": "expression",
            "expression": query
        }
    
    def should_use_search_engine(self, analysis: QueryAnalysisResult) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨æœç´¢å¼•æ“"""
        return analysis.needs_web_search or analysis.query_type in ["time", "news", "search"]
    
    def should_use_vector_store(self, analysis: QueryAnalysisResult) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨å‘é‡å­˜å‚¨"""
        return analysis.needs_vector_search or analysis.query_type in ["technical", "general"]
    
    def get_search_strategy(self, analysis: QueryAnalysisResult) -> str:
        """è·å–æœç´¢ç­–ç•¥"""
        if analysis.needs_web_search and analysis.needs_vector_search:
            return "hybrid"  # æ··åˆç­–ç•¥
        elif analysis.needs_web_search:
            return "search_only"  # ä»…æœç´¢
        elif analysis.needs_vector_search:
            return "vector_only"  # ä»…å‘é‡
        else:
            return "direct_llm"  # ç›´æ¥LLM
