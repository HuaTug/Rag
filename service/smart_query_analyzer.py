#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ™ºèƒ½æŸ¥è¯¢åˆ†æå™¨

é›†æˆGo demoçš„æ™ºèƒ½åˆ†æèƒ½åŠ›ï¼Œæä¾›è¯­ä¹‰ç†è§£å’Œå·¥å…·é€‰æ‹©åŠŸèƒ½ã€‚
"""

import asyncio
import json
import logging
import re
import sys
import os
from dataclasses import dataclass
from typing import Dict, Any, Optional, List, Union
import aiohttp
import time

# æ·»åŠ ä¸Šçº§ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å°è¯•å¯¼å…¥åŸå§‹å‡½æ•°å’Œå®¢æˆ·ç«¯
try:
    from ..core.ask_llm import get_llm_answer_deepseek as _original_llm_function, TencentDeepSeekClient
    _llm_available = True
except ImportError:
    _original_llm_function = None
    TencentDeepSeekClient = None
    _llm_available = False

# åˆ›å»ºé€‚é…å‡½æ•°
async def get_llm_answer_deepseek(query: str, search_flag: bool = False, timeout: int = 15) -> str:
    """
    é€‚é…å‡½æ•°ï¼Œå…¼å®¹åŸå§‹LLMå‡½æ•°çš„è°ƒç”¨æ–¹å¼
    """
    if not _llm_available:
        return f"æ¨¡æ‹ŸLLMå“åº”: {query}"
    
    try:
        # å°è¯•ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
        import os
        api_key = os.getenv("DEEPSEEK_API_KEY","sk-qFPEqgpxmS8DJ0nJQ6gvdIkozY1k2oEZER2A4zRhLxBvtIHl") or os.getenv("TENCENT_API_KEY")
        
        if not api_key:
            # å¦‚æœæ²¡æœ‰APIå¯†é’¥ï¼Œè¿”å›æ¨¡æ‹Ÿå“åº”
            return f"LLMåˆ†æ: éœ€è¦è®¾ç½®DEEPSEEK_API_KEYæˆ–TENCENT_API_KEYç¯å¢ƒå˜é‡ã€‚æŸ¥è¯¢: {query[:100]}..."
        
        # åˆ›å»ºå®¢æˆ·ç«¯å¹¶è°ƒç”¨åŸå§‹å‡½æ•°
        client = TencentDeepSeekClient(api_key=api_key)
        
        # è°ƒç”¨åŸå§‹å‡½æ•°ï¼Œä½¿ç”¨é€‚å½“çš„å‚æ•°
        response = _original_llm_function(
            client=client,
            context="",  # ç©ºä¸Šä¸‹æ–‡ï¼Œè®©LLMåŸºäºçŸ¥è¯†åº“å›ç­”
            question=query,
            model="deepseek-v3-0324"
        )
        
        return response
        
    except Exception as e:
        # å¦‚æœè°ƒç”¨å¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯å’Œæ¨¡æ‹Ÿå“åº”
        return f"LLMè°ƒç”¨å¤±è´¥ ({str(e)})ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ: {query[:100]}..."


@dataclass
class ToolCall:
    """å·¥å…·è°ƒç”¨ç»“æ„"""
    name: str
    args: Dict[str, Any]
    reasoning: str = ""


@dataclass
class QueryAnalysisResult:
    """æŸ¥è¯¢åˆ†æç»“æœ"""
    needs_web_search: bool = False
    web_search_query: str = ""
    needs_database: bool = False
    database_query: Dict[str, Any] = None
    needs_calculation: bool = False
    calculation_args: Dict[str, Any] = None
    needs_vector_search: bool = True  # é»˜è®¤å¯ç”¨å‘é‡æœç´¢
    query_type: str = "general"
    confidence: float = 0.0
    reasoning: str = ""
    tool_calls: List[ToolCall] = None
    # æ–°å¢ï¼šåŠ¨æ€æœç´¢ç­–ç•¥
    enable_dynamic_search: bool = True  # å¯ç”¨åŠ¨æ€æœç´¢ç­–ç•¥
    min_similarity_threshold: float = 0.8  # æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
    
    def __post_init__(self):
        if self.database_query is None:
            self.database_query = {}
        if self.calculation_args is None:
            self.calculation_args = {}
        if self.tool_calls is None:
            self.tool_calls = []


class SmartQueryAnalyzer:
    """æ™ºèƒ½æŸ¥è¯¢åˆ†æå™¨ - é›†æˆGo demoçš„åˆ†æèƒ½åŠ›"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # åˆ†æé…ç½® - ä¼˜å…ˆä½¿ç”¨è¯­ä¹‰åˆ†æ
        self.enable_semantic_analysis = self.config.get("enable_semantic_analysis", True)
        self.fallback_to_keywords = self.config.get("fallback_to_keywords", True)
        self.analysis_timeout = self.config.get("analysis_timeout", 15)
        
        self.logger.info(f"ğŸ§  æ™ºèƒ½æŸ¥è¯¢åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"   ğŸ¯ è¯­ä¹‰åˆ†æ(LLM): {'âœ… å¯ç”¨' if self.enable_semantic_analysis else 'âŒ ç¦ç”¨'}")
        self.logger.info(f"   ğŸ”„ å…³é”®è¯å›é€€: {'âœ… å¯ç”¨' if self.fallback_to_keywords else 'âŒ ç¦ç”¨'}")
        
        # å…³é”®è¯é…ç½®ï¼ˆä»…ç”¨äºå›é€€ï¼‰
        self._init_keyword_patterns()
        
        self.logger.info(f"ğŸ§  æ™ºèƒ½æŸ¥è¯¢åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ - è¯­ä¹‰åˆ†æ: {self.enable_semantic_analysis}")
    
    def _init_keyword_patterns(self):
        """åˆå§‹åŒ–å…³é”®è¯åŒ¹é…æ¨¡å¼ï¼ˆä»…ç”¨äºå›é€€åˆ†æï¼‰"""
        # ç®€åŒ–çš„å…³é”®è¯é…ç½®ï¼Œä»…ç”¨äºå›é€€æƒ…å†µ
        self.time_keywords = ["ä»Šå¤©", "ç°åœ¨", "å‡ å·", "å‡ æœˆ", "å‡ ç‚¹", "å½“å‰", "æ—¥æœŸ", "æ—¶é—´"]
        self.calculation_keywords = ["è®¡ç®—", "åŠ ", "å‡", "ä¹˜", "é™¤", "+", "-", "*", "/"]
        
        self.logger.info("ğŸ“‹ å…³é”®è¯æ¨¡å¼å·²åˆå§‹åŒ–ï¼ˆä»…ç”¨äºå›é€€åˆ†æï¼‰")
    
    async def analyze_query_intent(self, query: str) -> QueryAnalysisResult:
        """åˆ†ææŸ¥è¯¢æ„å›¾ - ä¸»å…¥å£æ–¹æ³•"""
        start_time = time.time()
        
        try:
            self.logger.info(f"ğŸ” å¼€å§‹åˆ†ææŸ¥è¯¢: {query}")
            
            # ä¼˜å…ˆä½¿ç”¨è¯­ä¹‰åˆ†æï¼ˆå¤§æ¨¡å‹åˆ†æï¼‰
            if self.enable_semantic_analysis:
                try:
                    result = await self._semantic_analysis(query)
                    analysis_time = time.time() - start_time
                    self.logger.info(f"âœ… è¯­ä¹‰åˆ†æå®Œæˆ - è€—æ—¶: {analysis_time:.2f}s")
                    return result
                except Exception as e:
                    self.logger.warning(f"âš ï¸ è¯­ä¹‰åˆ†æå¤±è´¥: {e}")
                    # å¦‚æœä¸å…è®¸å›é€€åˆ°å…³é”®è¯ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
                    if not self.fallback_to_keywords:
                        raise
            
            # åªæœ‰åœ¨è¯­ä¹‰åˆ†æå¤±è´¥ä¸”å…è®¸å›é€€æ—¶æ‰ä½¿ç”¨å…³é”®è¯åŒ¹é…
            self.logger.info("ğŸ”„ å›é€€åˆ°ç®€åŒ–çš„å…³é”®è¯åˆ†æ...")
            result = await self._fallback_analysis(query)
            analysis_time = time.time() - start_time
            self.logger.info(f"âœ… å›é€€åˆ†æå®Œæˆ - è€—æ—¶: {analysis_time:.2f}s")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ æŸ¥è¯¢åˆ†æå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤åˆ†æç»“æœ
            return QueryAnalysisResult(
                needs_vector_search=True,
                query_type="general",
                reasoning=f"åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥: {str(e)}",
                confidence=0.3,
                enable_dynamic_search=True
            )
    
    async def _semantic_analysis(self, query: str) -> QueryAnalysisResult:
        """ä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰åˆ†æ"""
        prompt = self._build_analysis_prompt(query)
        
        try:
            self.logger.info(f"ğŸ¤– è°ƒç”¨LLMè¿›è¡Œè¯­ä¹‰åˆ†æï¼ŒæŸ¥è¯¢: {query[:50]}...")
            
            # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
            response = await get_llm_answer_deepseek(
                query=prompt,
                search_flag=False,  # ç¦ç”¨æœç´¢é¿å…å¾ªç¯è°ƒç”¨
                timeout=self.analysis_timeout
            )
            
            self.logger.info(f"ğŸ“ LLMå“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
            self.logger.debug(f"ğŸ“ LLMåŸå§‹å“åº”: {response[:300]}...")
            
            # è§£æåˆ†æç»“æœ
            analysis = self._parse_llm_response(response, query)
            
            # æ™ºèƒ½å¡«å……å·¥å…·å‚æ•°
            self._fill_tool_parameters(analysis, query)
            
            return analysis
            
        except ValueError as e:
            # JSONè§£æé”™è¯¯ï¼Œæä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            self.logger.error(f"âŒ JSONè§£æé”™è¯¯: {e}")
            if "Extra data" in str(e):
                self.logger.error("ğŸ’¡ å¯èƒ½åŸå› : LLMè¿”å›äº†å¤šä¸ªJSONå¯¹è±¡æˆ–JSONåæœ‰é¢å¤–æ–‡æœ¬")
                self.logger.error("ğŸ’¡ å»ºè®®: æ£€æŸ¥LLMæç¤ºè¯ï¼Œç¡®ä¿åªè¿”å›å•ä¸ªJSONå¯¹è±¡")
            raise
            
        except Exception as e:
            self.logger.error(f"âŒ LLMè¯­ä¹‰åˆ†æå¤±è´¥: {e}")
            self.logger.error(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
            raise
    
    def _build_analysis_prompt(self, query: str) -> str:
        """æ„å»ºLLMåˆ†ææç¤ºè¯ - è®©å¤§æ¨¡å‹æ™ºèƒ½è¯­ä¹‰åˆ†æå¹¶ç”ŸæˆJSON"""
        return f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½RAGç³»ç»Ÿçš„æŸ¥è¯¢åˆ†æå™¨ã€‚è¯·é€šè¿‡è¯­ä¹‰åˆ†æç†è§£ç”¨æˆ·æŸ¥è¯¢çš„çœŸå®æ„å›¾ï¼Œå¹¶æ™ºèƒ½é€‰æ‹©æœ€é€‚åˆçš„å·¥å…·ç»„åˆã€‚

ç”¨æˆ·æŸ¥è¯¢ï¼š"{query}"

ğŸ”§ **å¯ç”¨å·¥å…·è¯´æ˜ï¼š**

1. **web_search** - ç½‘ç»œæœç´¢
   - é€‚ç”¨åœºæ™¯ï¼šæ—¶é—´æ—¥æœŸæŸ¥è¯¢ã€æœ€æ–°ä¿¡æ¯ã€å®æ—¶æ•°æ®ã€æ–°é—»
   - ç¤ºä¾‹ï¼š"ä»Šå¤©å‡ å·ï¼Ÿ"ã€"æœ€æ–°AIå‘å±•"ã€"ç°åœ¨å‡ ç‚¹ï¼Ÿ"
   
2. **vector_search** - å‘é‡çŸ¥è¯†åº“æ£€ç´¢
   - é€‚ç”¨åœºæ™¯ï¼šæŠ€æœ¯æ¦‚å¿µã€å®šä¹‰è§£é‡Šã€å†å²çŸ¥è¯†ã€æ•™ç¨‹
   - ç¤ºä¾‹ï¼š"ä»€ä¹ˆæ˜¯Pythonï¼Ÿ"ã€"å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ"
   
3. **calculation** - æ•°å­¦è®¡ç®—
   - é€‚ç”¨åœºæ™¯ï¼šæ•°å­¦è¿ç®—ã€æ•°å€¼è®¡ç®—ã€å…¬å¼æ±‚è§£
   - ç¤ºä¾‹ï¼š"100+200ç­‰äºå¤šå°‘ï¼Ÿ"ã€"è®¡ç®—å¹³æ–¹æ ¹"
   
4. **database_query** - æ•°æ®åº“æŸ¥è¯¢
   - é€‚ç”¨åœºæ™¯ï¼šç”¨æˆ·ç»Ÿè®¡ã€æ•°æ®åˆ†æã€è®°å½•æŸ¥è¯¢
   - ç¤ºä¾‹ï¼š"ç»Ÿè®¡ç”¨æˆ·æ•°é‡"ã€"æŸ¥è¯¢æ´»è·ƒç”¨æˆ·"

ğŸ§  **æ™ºèƒ½åˆ†æè¦æ±‚ï¼š**

è¯·ä»”ç»†åˆ†ææŸ¥è¯¢è¯­ä¹‰ï¼Œç†è§£ç”¨æˆ·çš„çœŸå®éœ€æ±‚ï¼Œç„¶åæ™ºèƒ½å¡«å……ä»¥ä¸‹JSONå‚æ•°ï¼š

ğŸ“ **è®¡ç®—æŸ¥è¯¢ç¤ºä¾‹åˆ†æï¼š**
- æŸ¥è¯¢ï¼š"100+100ç­‰äºå¤šå°‘ï¼Ÿ"
- è¯­ä¹‰åˆ†æï¼šç”¨æˆ·æ˜ç¡®è¦æ±‚è¿›è¡Œæ•°å­¦åŠ æ³•è¿ç®—
- åº”è®¾ç½®ï¼šneeds_calculation=true, calculation_args={{"operation":"add","x":100,"y":100}}
- æ¨ç†ï¼šè¿™æ˜¯æ˜ç¡®çš„æ•°å­¦è®¡ç®—é—®é¢˜ï¼Œç›´æ¥ä½¿ç”¨calculationå·¥å…·ï¼Œä¸éœ€è¦æœç´¢

â° **æ—¶é—´æŸ¥è¯¢ç¤ºä¾‹åˆ†æï¼š**
- æŸ¥è¯¢ï¼š"ä»Šå¤©å‡ å·ï¼Ÿ"
- è¯­ä¹‰åˆ†æï¼šç”¨æˆ·è¯¢é—®å½“å‰æ—¥æœŸä¿¡æ¯
- åº”è®¾ç½®ï¼šneeds_web_search=true, web_search_query="ä»Šå¤©æ—¥æœŸ", needs_vector_search=false
- æ¨ç†ï¼šæ—¶é—´ä¿¡æ¯éœ€è¦å®æ—¶è·å–ï¼Œä½¿ç”¨web_searchè·å–æœ€æ–°æ—¥æœŸ

ğŸ” **æŠ€æœ¯æ¦‚å¿µç¤ºä¾‹åˆ†æï¼š**
- æŸ¥è¯¢ï¼š"ä»€ä¹ˆæ˜¯Goè¯­è¨€ï¼Ÿ"
- è¯­ä¹‰åˆ†æï¼šç”¨æˆ·è¯¢é—®æŠ€æœ¯æ¦‚å¿µå®šä¹‰
- åº”è®¾ç½®ï¼šneeds_vector_search=true, enable_dynamic_search=true
- æ¨ç†ï¼šæŠ€æœ¯æ¦‚å¿µä¼˜å…ˆä»çŸ¥è¯†åº“æ£€ç´¢ï¼Œå¦‚æœè´¨é‡ä¸ä½³åˆ™å¯ç”¨ç½‘ç»œæœç´¢

ğŸ“Š **å‚æ•°è¯¦ç»†è¯´æ˜ï¼š**

calculation_argsæ”¯æŒçš„è¿ç®—ç±»å‹ï¼š
- åŠ æ³•ï¼š{{"operation":"add","x":æ•°å­—1,"y":æ•°å­—2}}
- å‡æ³•ï¼š{{"operation":"subtract","x":æ•°å­—1,"y":æ•°å­—2}}  
- ä¹˜æ³•ï¼š{{"operation":"multiply","x":æ•°å­—1,"y":æ•°å­—2}}
- é™¤æ³•ï¼š{{"operation":"divide","x":æ•°å­—1,"y":æ•°å­—2}}
- è·å–æ—¥æœŸï¼š{{"operation":"get_current_date"}}
- è¡¨è¾¾å¼ï¼š{{"operation":"expression","expression":"è¡¨è¾¾å¼å†…å®¹"}}

database_queryæ”¯æŒçš„æŸ¥è¯¢ç±»å‹ï¼š
- ç»Ÿè®¡æŸ¥è¯¢ï¼š{{"query_type":"count","table":"users","group_by":"status"}}
- æ¡ä»¶æŸ¥è¯¢ï¼š{{"query_type":"select","table":"users","where":{{"status":"active"}},"limit":10}}

ğŸ¯ **JSONæ ¼å¼è¦æ±‚ï¼š**

è¯·æ ¹æ®è¯­ä¹‰åˆ†æï¼Œæ™ºèƒ½å¡«å……æ‰€æœ‰å‚æ•°ï¼Œç¡®ä¿é€»è¾‘ä¸€è‡´ï¼š

{{
  "needs_web_search": å¸ƒå°”å€¼,
  "web_search_query": "å¦‚æœéœ€è¦ç½‘ç»œæœç´¢ï¼Œå¡«å…¥æœç´¢å…³é”®è¯",
  "needs_vector_search": å¸ƒå°”å€¼,
  "needs_database": å¸ƒå°”å€¼,
  "database_query": {{å…·ä½“çš„æ•°æ®åº“æŸ¥è¯¢å‚æ•°}},
  "needs_calculation": å¸ƒå°”å€¼,
  "calculation_args": {{å…·ä½“çš„è®¡ç®—å‚æ•°}},
  "query_type": "time/technical/calculation/database/general",
  "confidence": 0.0åˆ°1.0çš„ç½®ä¿¡åº¦,
  "reasoning": "è¯¦ç»†è¯´æ˜ä½ çš„è¯­ä¹‰åˆ†æè¿‡ç¨‹å’Œå‚æ•°è®¾ç½®ç†ç”±",
  "enable_dynamic_search": å¸ƒå°”å€¼,
  "min_similarity_threshold": 0.8
}}

âš ï¸ **é‡è¦è§„åˆ™ï¼š**
1. é€šè¿‡è¯­ä¹‰ç†è§£ï¼Œä¸æ˜¯å…³é”®è¯åŒ¹é…
2. reasoningå¿…é¡»è¯¦ç»†è§£é‡Šä¸ºä»€ä¹ˆè¿™æ ·è®¾ç½®å‚æ•°
3. å¦‚æœæ˜¯è®¡ç®—é—®é¢˜ï¼Œå¿…é¡»æ­£ç¡®è§£ææ•°å­—å’Œè¿ç®—ç¬¦
4. å¦‚æœæ˜¯æ—¶é—´é—®é¢˜ï¼Œweb_search_queryè¦å…·ä½“
5. ç½®ä¿¡åº¦è¦åæ˜ åˆ†æçš„ç¡®å®šç¨‹åº¦
6. ç¡®ä¿å‚æ•°ä¹‹é—´é€»è¾‘ä¸€è‡´

ç°åœ¨è¯·åˆ†æä¸Šè¿°æŸ¥è¯¢ï¼Œåªè¿”å›JSONç»“æœï¼š"""
    
    def _parse_llm_response(self, response: str, query: str) -> QueryAnalysisResult:
        """è§£æLLMå“åº” - ç›´æ¥ä½¿ç”¨å¤§æ¨¡å‹çš„è¯­ä¹‰åˆ†æç»“æœ"""
        try:
            self.logger.info(f"ğŸ” è§£æLLMå“åº”ï¼Œé•¿åº¦: {len(response)} å­—ç¬¦")
            
            # æ¸…ç†å“åº”æ–‡æœ¬
            response = response.strip()
            
            # å¤šç§æ–¹å¼æå–JSON
            json_str = None
            data = None
            
            # æ–¹æ³•1: å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
            try:
                data = json.loads(response)
                json_str = response
                self.logger.info("âœ… ç›´æ¥è§£ææ•´ä¸ªå“åº”æˆåŠŸ")
            except json.JSONDecodeError:
                pass
            
            # æ–¹æ³•2: ä½¿ç”¨æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å®Œæ•´çš„JSONå¯¹è±¡
            if data is None:
                # åŒ¹é…å®Œæ•´çš„JSONå¯¹è±¡ï¼Œè€ƒè™‘åµŒå¥—ç»“æ„
                json_pattern = r'\{(?:[^{}]|{[^{}]*})*\}'
                json_matches = re.findall(json_pattern, response, re.DOTALL)
                
                for match in json_matches:
                    try:
                        data = json.loads(match)
                        json_str = match
                        self.logger.info("âœ… æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…JSONæˆåŠŸ")
                        break
                    except json.JSONDecodeError:
                        continue
            
            # æ–¹æ³•3: æŸ¥æ‰¾JSONä»£ç å—ï¼ˆ```json ... ```ï¼‰
            if data is None:
                json_block_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL | re.IGNORECASE)
                if json_block_match:
                    try:
                        json_str = json_block_match.group(1).strip()
                        data = json.loads(json_str)
                        self.logger.info("âœ… JSONä»£ç å—è§£ææˆåŠŸ")
                    except json.JSONDecodeError:
                        pass
            
            # æ–¹æ³•4: æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
            if data is None:
                brace_count = 0
                start_idx = -1
                
                for i, char in enumerate(response):
                    if char == '{':
                        if start_idx == -1:
                            start_idx = i
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0 and start_idx != -1:
                            try:
                                json_str = response[start_idx:i+1]
                                data = json.loads(json_str)
                                self.logger.info("âœ… æ‰‹åŠ¨åŒ¹é…JSONå¯¹è±¡æˆåŠŸ")
                                break
                            except json.JSONDecodeError:
                                start_idx = -1
                                continue
            
            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥
            if data is None:
                self.logger.error(f"LLMå“åº”ä¸­æ— æœ‰æ•ˆJSONæ ¼å¼: {response[:200]}...")
                raise ValueError("æ— æ³•åœ¨å“åº”ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ¼å¼")
            
            # éªŒè¯å¿…è¦å­—æ®µ
            if not isinstance(data, dict):
                raise ValueError("JSONè§£æç»“æœä¸æ˜¯å­—å…¸ç±»å‹")
            
            # ğŸ§  ç›´æ¥ä½¿ç”¨LLMçš„è¯­ä¹‰åˆ†æç»“æœï¼Œä¿¡ä»»å…¶æ™ºèƒ½åˆ¤æ–­
            result = QueryAnalysisResult(
                needs_web_search=data.get("needs_web_search", False),
                web_search_query=data.get("web_search_query", ""),
                needs_vector_search=data.get("needs_vector_search", True),
                needs_database=data.get("needs_database", False),
                database_query=data.get("database_query", {}),
                needs_calculation=data.get("needs_calculation", False),
                calculation_args=data.get("calculation_args", {}),
                query_type=data.get("query_type", "general"),
                confidence=float(data.get("confidence", 0.5)),
                reasoning=data.get("reasoning", "LLMè¯­ä¹‰åˆ†æç»“æœ"),
                enable_dynamic_search=data.get("enable_dynamic_search", True),
                min_similarity_threshold=float(data.get("min_similarity_threshold", 0.8))
            )
            
            # ğŸ“ è®°å½•åˆ†æç»“æœ
            self.logger.info(f"âœ… LLMè¯­ä¹‰åˆ†ææˆåŠŸ:")
            self.logger.info(f"   ğŸ¯ æŸ¥è¯¢ç±»å‹: {result.query_type}")
            self.logger.info(f"   ğŸŒ ç½‘ç»œæœç´¢: {result.needs_web_search}")
            self.logger.info(f"   ğŸ” å‘é‡æœç´¢: {result.needs_vector_search}")
            self.logger.info(f"   ğŸ§® æ•°å­¦è®¡ç®—: {result.needs_calculation}")
            self.logger.info(f"   ğŸ“Š æ•°æ®åº“æŸ¥è¯¢: {result.needs_database}")
            self.logger.info(f"   ğŸ“ˆ ç½®ä¿¡åº¦: {result.confidence:.2f}")
            self.logger.info(f"   ğŸ’­ æ¨ç†: {result.reasoning[:100]}...")
            
            return result
            
        except (json.JSONDecodeError, ValueError, KeyError) as e:
            self.logger.error(f"âŒ è§£æLLMå“åº”å¤±è´¥: {e}")
            self.logger.error(f"åŸå§‹å“åº”: {response[:500]}...")
            self.logger.error(f"æå–çš„JSON: {json_str[:200] if json_str else 'None'}...")
            # å›é€€åˆ°ç®€åŒ–åˆ†æ
            raise ValueError(f"LLMå“åº”è§£æå¤±è´¥: {e}")
    
    async def _fallback_analysis(self, query: str) -> QueryAnalysisResult:
        """ç®€åŒ–çš„å›é€€åˆ†æï¼ˆå½“LLMåˆ†æå¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
        self.logger.info("ğŸ“‹ ä½¿ç”¨ç®€åŒ–å›é€€åˆ†æ...")
        
        # é»˜è®¤ç­–ç•¥ï¼šä½¿ç”¨å‘é‡æœç´¢ + åŠ¨æ€æœç´¢ç­–ç•¥
        analysis = QueryAnalysisResult(
            needs_vector_search=True,
            query_type="general",
            confidence=0.6,
            reasoning="LLMåˆ†æå¤±è´¥ï¼Œä½¿ç”¨å›é€€ç­–ç•¥ï¼šå‘é‡æœç´¢+åŠ¨æ€æœç´¢",
            enable_dynamic_search=True,
            min_similarity_threshold=0.8
        )
        
        # ç®€å•çš„æ¨¡å¼è¯†åˆ«ï¼ˆä»…ä½œä¸ºåŸºæœ¬ä¿éšœï¼‰
        query_lower = query.lower()
        
        # æ˜æ˜¾çš„æ—¶é—´æŸ¥è¯¢
        if any(word in query_lower for word in ["ä»Šå¤©", "ç°åœ¨", "å‡ å·", "å‡ æœˆ", "å‡ ç‚¹", "å½“å‰æ—¶é—´", "æ—¥æœŸ"]):
            analysis.needs_web_search = True
            analysis.needs_vector_search = False
            analysis.web_search_query = "å½“å‰æ—¥æœŸæ—¶é—´"
            analysis.query_type = "time"
            analysis.enable_dynamic_search = False
            analysis.reasoning = "å›é€€åˆ†æï¼šæ£€æµ‹åˆ°æ—¶é—´ç›¸å…³æŸ¥è¯¢"
            analysis.confidence = 0.8
        
        # æ˜æ˜¾çš„è®¡ç®—æŸ¥è¯¢
        elif any(word in query_lower for word in ["è®¡ç®—", "åŠ ", "å‡", "ä¹˜", "é™¤", "+", "-", "*", "/"]):
            analysis.needs_calculation = True
            analysis.calculation_args = self._parse_calculation(query)
            analysis.query_type = "calculation"
            analysis.reasoning = "å›é€€åˆ†æï¼šæ£€æµ‹åˆ°è®¡ç®—æŸ¥è¯¢"
            analysis.confidence = 0.7
        
        # å¦‚æœæœ‰æ—¥æœŸæŸ¥è¯¢ï¼Œæ·»åŠ è·å–å½“å‰æ—¥æœŸçš„åŠŸèƒ½
        if "å‡ å·" in query_lower or "å‡ æœˆ" in query_lower or "æ—¥æœŸ" in query_lower:
            # æ·»åŠ ä¸€ä¸ªç‰¹æ®Šçš„è®¡ç®—æ“ä½œæ¥è·å–å½“å‰æ—¥æœŸ
            if not analysis.needs_calculation:
                analysis.needs_calculation = True
                analysis.calculation_args = {"operation": "get_current_date"}
            analysis.reasoning += " + è·å–å½“å‰æ—¥æœŸ"
        
        # å¡«å……å·¥å…·å‚æ•°
        self._fill_tool_parameters(analysis, query)
        
        return analysis
    
    def _fill_tool_parameters(self, analysis: QueryAnalysisResult, query: str):
        """æ™ºèƒ½å¡«å……å·¥å…·å‚æ•°"""
        # å¡«å……ç½‘ç»œæœç´¢å‚æ•°
        if analysis.needs_web_search and not analysis.web_search_query:
            analysis.web_search_query = query
        
        # å¡«å……æ•°æ®åº“æŸ¥è¯¢å‚æ•°
        if analysis.needs_database and not analysis.database_query:
            analysis.database_query = self._build_database_query(query)
        
        # å¡«å……è®¡ç®—å‚æ•°
        if analysis.needs_calculation and not analysis.calculation_args:
            analysis.calculation_args = self._parse_calculation(query)
        
        # æ„å»ºå·¥å…·è°ƒç”¨åˆ—è¡¨
        tool_calls = []
        
        if analysis.needs_web_search:
            tool_calls.append(ToolCall(
                name="web_search",
                args={"query": analysis.web_search_query, "limit": 5},
                reasoning="éœ€è¦è·å–æœ€æ–°ä¿¡æ¯"
            ))
        
        if analysis.needs_vector_search:
            tool_calls.append(ToolCall(
                name="vector_search",
                args={"query": query, "top_k": 5},
                reasoning="éœ€è¦æ£€ç´¢ç›¸å…³çŸ¥è¯†"
            ))
        
        if analysis.needs_database:
            tool_calls.append(ToolCall(
                name="database_query",
                args=analysis.database_query,
                reasoning="éœ€è¦æŸ¥è¯¢æ•°æ®åº“ä¿¡æ¯"
            ))
        
        if analysis.needs_calculation:
            tool_calls.append(ToolCall(
                name="calculator",
                args=analysis.calculation_args,
                reasoning="éœ€è¦è¿›è¡Œæ•°å­¦è®¡ç®—"
            ))
        
        analysis.tool_calls = tool_calls
    
    def _build_database_query(self, query: str) -> Dict[str, Any]:
        """æ„å»ºæ•°æ®åº“æŸ¥è¯¢å‚æ•°"""
        query_lower = query.lower()
        
        if "ç»Ÿè®¡" in query_lower or "æ•°é‡" in query_lower or "count" in query_lower:
            return {
                "query_type": "count",
                "table": "users",
                "group_by": "status"
            }
        
        if "æ´»è·ƒ" in query_lower:
            return {
                "query_type": "select",
                "table": "users",
                "where": {"status": "active"},
                "limit": 10
            }
        
        # é»˜è®¤æŸ¥è¯¢
        return {
            "query_type": "select",
            "table": "users",
            "limit": 5
        }
    
    def _parse_calculation(self, query: str) -> Dict[str, Any]:
        """è§£ææ•°å­¦è®¡ç®—è¡¨è¾¾å¼"""
        query_lower = query.lower()
        
        # ç®€å•çš„æ•°å­¦è¡¨è¾¾å¼è§£æ
        # åœ¨å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„è¡¨è¾¾å¼è§£æå™¨
        
        if "åŠ " in query_lower or "+" in query:
            # å°è¯•æå–æ•°å­—
            numbers = re.findall(r'\d+\.?\d*', query)
            if len(numbers) >= 2:
                return {
                    "operation": "add",
                    "x": float(numbers[0]),
                    "y": float(numbers[1])
                }
        
        if "å‡" in query_lower or "-" in query:
            numbers = re.findall(r'\d+\.?\d*', query)
            if len(numbers) >= 2:
                return {
                    "operation": "subtract",
                    "x": float(numbers[0]),
                    "y": float(numbers[1])
                }
        
        if "ä¹˜" in query_lower or "*" in query or "Ã—" in query:
            numbers = re.findall(r'\d+\.?\d*', query)
            if len(numbers) >= 2:
                return {
                    "operation": "multiply",
                    "x": float(numbers[0]),
                    "y": float(numbers[1])
                }
        
        if "é™¤" in query_lower or "/" in query or "Ã·" in query:
            numbers = re.findall(r'\d+\.?\d*', query)
            if len(numbers) >= 2:
                return {
                    "operation": "divide",
                    "x": float(numbers[0]),
                    "y": float(numbers[1])
                }
        
        # é»˜è®¤è®¡ç®—
        return {
            "operation": "expression",
            "expression": query
        }
    
    def should_use_search_engine(self, analysis: QueryAnalysisResult) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨æœç´¢å¼•æ“"""
        return analysis.needs_web_search or analysis.query_type in ["time", "news", "search"]
    
    def should_use_vector_store(self, analysis: QueryAnalysisResult) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨å‘é‡å­˜å‚¨"""
        return analysis.needs_vector_search or analysis.query_type in ["technical", "general"]
    
    def get_search_strategy(self, analysis: QueryAnalysisResult) -> str:
        """è·å–æœç´¢ç­–ç•¥"""
        if analysis.needs_web_search and analysis.needs_vector_search:
            return "hybrid"  # æ··åˆç­–ç•¥
        elif analysis.needs_web_search:
            return "search_only"  # ä»…æœç´¢
        elif analysis.needs_vector_search:
            return "vector_only"  # ä»…å‘é‡
        else:
            return "direct_llm"  # ç›´æ¥LLM


# ç®€å•çš„æ•°å­¦è®¡ç®—å™¨
class SimpleCalculator:
    """ç®€å•è®¡ç®—å™¨ - å¤„ç†åŸºæœ¬æ•°å­¦è¿ç®—"""
    
    @staticmethod
    def calculate(args: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œè®¡ç®—"""
        operation = args.get("operation", "")
        
        try:
            if operation == "get_current_date":
                # è·å–å½“å‰æ—¥æœŸ
                from datetime import datetime
                now = datetime.now()
                date_str = now.strftime("%Yå¹´%mæœˆ%dæ—¥")
                weekday = ["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"][now.weekday()]
                return {
                    "result": f"ä»Šå¤©æ˜¯{date_str}ï¼Œ{weekday}",
                    "expression": f"å½“å‰æ—¥æœŸ: {date_str} ({weekday})"
                }
                
            elif operation == "add":
                result = float(args["x"]) + float(args["y"])
                return {"result": result, "expression": f"{args['x']} + {args['y']} = {result}"}
            
            elif operation == "subtract":
                result = float(args["x"]) - float(args["y"])
                return {"result": result, "expression": f"{args['x']} - {args['y']} = {result}"}
            
            elif operation == "multiply":
                result = float(args["x"]) * float(args["y"])
                return {"result": result, "expression": f"{args['x']} Ã— {args['y']} = {result}"}
            
            elif operation == "divide":
                if float(args["y"]) == 0:
                    return {"error": "é™¤æ•°ä¸èƒ½ä¸ºé›¶"}
                result = float(args["x"]) / float(args["y"])
                return {"result": result, "expression": f"{args['x']} Ã· {args['y']} = {result}"}
            
            elif operation == "expression":
                # ç®€å•çš„è¡¨è¾¾å¼è®¡ç®—ï¼ˆå®‰å…¨èµ·è§ï¼Œåªæ”¯æŒåŸºæœ¬è¿ç®—ï¼‰
                expression = args.get("expression", "")
                # è¿™é‡Œå¯ä»¥é›†æˆæ›´å¤æ‚çš„è¡¨è¾¾å¼è§£æå™¨
                return {"result": "è¡¨è¾¾å¼è®¡ç®—åŠŸèƒ½å¾…å®ç°", "expression": expression}
            
            else:
                return {"error": f"ä¸æ”¯æŒçš„è¿ç®—ç±»å‹: {operation}"}
        
        except Exception as e:
            return {"error": f"è®¡ç®—é”™è¯¯: {str(e)}"}


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    async def test_analyzer():
        analyzer = SmartQueryAnalyzer()
        
        test_queries = [
            "ä»Šå¤©å‡ å·ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯Milvuså‘é‡æ•°æ®åº“ï¼Ÿ",
            "è®¡ç®—15.5åŠ ä¸Š24.3çš„ç»“æœ",
            "æŸ¥è¯¢æ´»è·ƒç”¨æˆ·æ•°é‡",
            "æœ€æ–°çš„AIæŠ€æœ¯å‘å±•å¦‚ä½•ï¼Ÿ"
        ]
        
        for query in test_queries:
            print(f"\nğŸ“ æµ‹è¯•æŸ¥è¯¢: {query}")
            print("-" * 40)
            
            result = await analyzer.analyze_query_intent(query)
            
            print(f"æŸ¥è¯¢ç±»å‹: {result.query_type}")
            print(f"éœ€è¦ç½‘ç»œæœç´¢: {result.needs_web_search}")
            print(f"éœ€è¦å‘é‡æœç´¢: {result.needs_vector_search}")
            print(f"éœ€è¦æ•°æ®åº“æŸ¥è¯¢: {result.needs_database}")
            print(f"éœ€è¦è®¡ç®—: {result.needs_calculation}")
            print(f"ç½®ä¿¡åº¦: {result.confidence:.2f}")
            print(f"æ¨ç†è¿‡ç¨‹: {result.reasoning}")
            print(f"å·¥å…·è°ƒç”¨æ•°: {len(result.tool_calls)}")
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_analyzer())
