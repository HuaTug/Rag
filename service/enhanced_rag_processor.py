#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¢å¼ºçš„RAGå¤„ç†å™¨

é›†æˆå¤šé€šé“æœç´¢ã€å‘é‡å­˜å‚¨å’ŒLLMç”Ÿæˆï¼Œæä¾›å®Œæ•´çš„RAGè§£å†³æ–¹æ¡ˆã€‚
"""

import logging
import time
import sys
import os
import re
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from dotenv import load_dotenv

from channel_framework import MProcessor, QueryContext, QueryAnalyzer, QueryType, SearchResult, ChannelType
from smart_query_analyzer import SmartQueryAnalyzer, QueryAnalysisResult
# å¯¼å…¥coreæ¨¡å—
sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'core'))
from core.search_channels import GoogleSearchChannel
from core.dynamic_vector_store import DynamicVectorStore, VectorStoreManager
from core.ask_llm import get_llm_answer_with_prompt
from core.encoder import emb_text

load_dotenv()

@dataclass
class RAGResponse:
    """RAGå“åº”æ•°æ®ç»“æ„"""
    answer: str
    sources: List[Dict[str, Any]]
    search_results: List[SearchResult]
    processing_time: float
    confidence_score: float
    metadata: Dict[str, Any]
    analysis_result: Optional[QueryAnalysisResult] = None  # æ–°å¢åˆ†æç»“æœ


class EnhancedRAGProcessor:
    """å¢å¼ºçš„RAGå¤„ç†å™¨"""
    
    def __init__(self, vector_store=None, search_channels=None, llm_client=None, config: Dict[str, Any] = None):
        self.config = config or {}
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # å­˜å‚¨å¤–éƒ¨ä¼ å…¥çš„ç»„ä»¶
        self.vector_store = vector_store
        self.search_channels = search_channels or []
        self.llm_client = llm_client
        
        # å¦‚æœæ²¡æœ‰ä¼ å…¥LLMå®¢æˆ·ç«¯ï¼Œè‡ªåŠ¨åˆ›å»ºä¸€ä¸ª
        if not self.llm_client:
            self._init_llm_client()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.mcp_processor = MProcessor()
        self.vector_store_manager = VectorStoreManager()
        self.query_analyzer = QueryAnalyzer()
        
        # æ–°å¢æ™ºèƒ½æŸ¥è¯¢åˆ†æå™¨
        self.smart_analyzer = SmartQueryAnalyzer(self.config)

        # æ–°å¢ï¼šåˆå§‹åŒ–å¢å¼ºæ–‡æœ¬å¤„ç†å™¨
        from core.enhanced_text_processor import create_enhanced_text_processor
        text_processor_config = {
            "chunk_size": self.config.get("chunk_size", 800),
            "chunk_overlap": self.config.get("chunk_overlap", 100),
            "enable_chinese_segmentation": self.config.get("enable_chinese_segmentation", True),
            "enable_keyword_extraction": self.config.get("enable_keyword_extraction", True),
            "preserve_code_blocks": self.config.get("preserve_code_blocks", True)
        }

        self.text_processor = create_enhanced_text_processor(text_processor_config)
        self.logger.info(f"âœ… åˆå§‹åŒ–å¢å¼ºæ–‡æœ¬å¤„ç†å™¨: {self.text_processor.__class__.__name__}")

        # æ–°å¢ï¼šåˆå§‹åŒ–MCPå·¥å…·é›†æˆ
        self.mcp_integration = None
        if self.config.get("enable_mcp_tools", False):
            try:
                from core.mcp_tool_integration import MCPToolIntegration
                self.mcp_integration = MCPToolIntegration(self.config)
                self.logger.info("âœ… MCPå·¥å…·é›†æˆæ¨¡å—å·²åŠ è½½")
            except ImportError as e:
                self.logger.warning(f"âš ï¸ MCPå·¥å…·é›†æˆæ¨¡å—åŠ è½½å¤±è´¥: {e}")
                self.mcp_integration = None

        
        # æ™ºèƒ½æŸ¥è¯¢ç­–ç•¥é…ç½® - æé«˜ç›¸ä¼¼åº¦é˜ˆå€¼ä»¥è¿‡æ»¤ä¸ç›¸å…³å†…å®¹
        self.similarity_threshold = self.config.get("similarity_threshold", 0.7)  # æé«˜ç›¸ä¼¼åº¦é˜ˆå€¼åˆ°0.7
        self.min_similarity_for_answer = self.config.get("min_similarity_for_answer", 0.6)  # ç”Ÿæˆç­”æ¡ˆçš„æœ€ä½ç›¸ä¼¼åº¦
        self.min_vector_results = self.config.get("min_vector_results", 2)  # å‡å°‘æœ€å°‘å‘é‡ç»“æœæ•°é‡
        self.enable_smart_search = self.config.get("enable_smart_search", True)  # å¯ç”¨æ™ºèƒ½æœç´¢
        self.enable_fallback_search = self.config.get("enable_fallback_search", True)  # å¯ç”¨å›é€€æœç´¢
        
        # è¾“å‡ºé…ç½®ä¿¡æ¯ç”¨äºè°ƒè¯•
        self.logger.info(f"ğŸ“Š æ™ºèƒ½æœç´¢é…ç½®: similarity_threshold={self.similarity_threshold}, "
                        f"min_similarity_for_answer={self.min_similarity_for_answer}, "
                        f"min_vector_results={self.min_vector_results}, "
                        f"enable_smart_search={self.enable_smart_search}")
        
        # åˆå§‹åŒ–é…ç½®
        self._init_components()
    
    def _init_llm_client(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        try:
            from core.ask_llm import TencentDeepSeekClient
            import os
            
            # è·å–APIå¯†é’¥
            api_key = os.getenv("DEEPSEEK_API_KEY")
            
            if api_key:
                self.llm_client = TencentDeepSeekClient(api_key=api_key)
                self.logger.info("âœ… è‡ªåŠ¨åˆ›å»ºDeepSeek LLMå®¢æˆ·ç«¯æˆåŠŸ")
            else:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°LLM APIå¯†é’¥ï¼Œå°†ä½¿ç”¨æ™ºèƒ½å›é€€æ¨¡å¼")
                self.llm_client = None
                
        except Exception as e:
            self.logger.error(f"âŒ åˆ›å»ºLLMå®¢æˆ·ç«¯å¤±è´¥: {e}")
            self.llm_client = None
    
    def _init_components(self):
        """åˆå§‹åŒ–å„ä¸ªç»„ä»¶"""
        try:
            # 1. åˆå§‹åŒ–å‘é‡å­˜å‚¨
            self._init_vector_stores()
            
            # 2. åˆå§‹åŒ–æœç´¢é€šé“
            self._init_search_channels()
            
            # 3. åˆå§‹åŒ–MCPå·¥å…·é›†æˆï¼ˆå¼‚æ­¥åˆå§‹åŒ–å°†åœ¨éœ€è¦æ—¶è¿›è¡Œï¼‰
            if self.mcp_integration:
                self.logger.info("ğŸ”§ MCPå·¥å…·é›†æˆæ¨¡å—å·²å‡†å¤‡å°±ç»ªï¼Œå°†åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶åˆå§‹åŒ–")
            
            self.logger.info("å¢å¼ºRAGå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _init_vector_stores(self):
        """åˆå§‹åŒ–å‘é‡å­˜å‚¨"""
        try:
            # è·å–Milvusé…ç½®ï¼Œä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„é…ç½®
            milvus_endpoint = (
                self.config.get("milvus_endpoint") or 
                self.config.get("endpoint") or 
                "./milvus_rag.db"
            )
            milvus_token = (
                self.config.get("milvus_token") or 
                self.config.get("token")
            )
            vector_dim = (
                self.config.get("vector_dim") or 
                self.config.get("dimension") or 
                384
            )
            
            self.logger.info(f"ğŸ”§ åˆå§‹åŒ–å‘é‡å­˜å‚¨: endpoint={milvus_endpoint}, dim={vector_dim}")
            
            # åŠ¨æ€å‘é‡å­˜å‚¨ï¼ˆç”¨äºå®æ—¶æœç´¢ç»“æœï¼‰
            try:
                dynamic_store = DynamicVectorStore(
                    milvus_endpoint=milvus_endpoint,
                    milvus_token=milvus_token,
                    collection_name="dynamic_search_results",
                    vector_dim=vector_dim
                )
                self.vector_store_manager.add_store("dynamic", dynamic_store)
                self.logger.info("âœ… åŠ¨æ€å‘é‡å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                self.logger.error(f"âŒ åŠ¨æ€å‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {e}")
            
            # æœ¬åœ°çŸ¥è¯†åº“å­˜å‚¨
            try:
                local_store = DynamicVectorStore(
                    milvus_endpoint=milvus_endpoint,
                    milvus_token=milvus_token,
                    collection_name="local_knowledge",
                    vector_dim=vector_dim
                )
                self.vector_store_manager.add_store("local", local_store)
                self.logger.info("âœ… æœ¬åœ°çŸ¥è¯†åº“å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                self.logger.error(f"âŒ æœ¬åœ°çŸ¥è¯†åº“å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {e}")
                
        except Exception as e:
            self.logger.error(f"âŒ å‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸ç³»ç»Ÿç»§ç»­è¿è¡Œï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•
    
    def _init_search_channels(self):
        """åˆå§‹åŒ–æœç´¢é€šé“"""
        # æ³¨å†Œå¤–éƒ¨ä¼ å…¥çš„æœç´¢é€šé“
        for channel in self.search_channels:
            self.mcp_processor.register_channel(channel)
        
        # Googleæœç´¢é€šé“ï¼ˆå¦‚æœé…ç½®ä¸­å¯ç”¨ä¸”æ²¡æœ‰å¤–éƒ¨ä¼ å…¥ï¼‰
        if (self.config.get("enable_search_engine", True) and 
            not any(hasattr(ch, 'channel_type') and 
                   str(ch.channel_type).endswith('SEARCH_ENGINE') 
                   for ch in self.search_channels)):
            
            search_config = {
                "api_key": self.config.get("google_api_key"),
                "search_engine_id": self.config.get("google_search_engine_id"),
                "timeout": self.config.get("search_timeout", 10),
                "priority": {"factual": 1, "analytical": 2, "creative": 5, "conversational": 3}
            }
            
            if search_config["api_key"] and search_config["search_engine_id"]:
                search_channel = GoogleSearchChannel(search_config)
                self.mcp_processor.register_channel(search_channel)
        
        # æœ¬åœ°çŸ¥è¯†åº“é€šé“ï¼ˆæš‚æ—¶ç¦ç”¨ï¼Œå› ä¸ºLocalKnowledgeChannelç±»ä¸å­˜åœ¨ï¼‰
        if False:  # self.config.get("enable_local_knowledge", True):
            local_store = self.vector_store_manager.get_store("local")
            if local_store:
                local_config = {
                    "milvus_client": local_store.client,
                    "collection_name": "local_knowledge",
                    "encoder": emb_text,
                    "priority": {"factual": 2, "analytical": 1, "creative": 4, "conversational": 2}
                }
                # local_channel = LocalKnowledgeChannel(local_config)
                # self.mcp_processor.register_channel(local_channel)
        
        # æ–°é—»é€šé“ï¼ˆæš‚æ—¶ç¦ç”¨ï¼Œå› ä¸ºNewsChannelç±»ä¸å­˜åœ¨ï¼‰
        if False:  # self.config.get("enable_news", False):
            news_config = {
                "news_api_key": self.config.get("news_api_key"),
                "priority": {"factual": 3, "analytical": 3, "creative": 6, "conversational": 4}
            }
            # news_channel = NewsChannel(news_config)
            # self.mcp_processor.register_channel(news_channel)
    
    async def _ensure_mcp_initialized(self) -> bool:
        """ç¡®ä¿MCPå·¥å…·å·²åˆå§‹åŒ–"""
        if self.mcp_integration and not hasattr(self.mcp_integration, '_is_initialized'):
            try:
                success = await self.mcp_integration.initialize()
                self.mcp_integration._is_initialized = success
                if success:
                    self.logger.info("âœ… MCPå·¥å…·é›†æˆå»¶è¿Ÿåˆå§‹åŒ–æˆåŠŸ")
                else:
                    self.logger.warning("âš ï¸ MCPå·¥å…·é›†æˆå»¶è¿Ÿåˆå§‹åŒ–å¤±è´¥")
                return success
            except Exception as e:
                self.logger.error(f"âŒ MCPå·¥å…·é›†æˆå»¶è¿Ÿåˆå§‹åŒ–å¼‚å¸¸: {e}")
                self.mcp_integration._is_initialized = False
                return False
        elif self.mcp_integration:
            return getattr(self.mcp_integration, '_is_initialized', False)
        return False
    
    async def _init_mcp_tools(self):
        """åˆå§‹åŒ–MCPå·¥å…·é›†æˆ"""
        return await self._ensure_mcp_initialized()
    
    async def call_mcp_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """è°ƒç”¨MCPå·¥å…·"""
        # ç¡®ä¿MCPå·²åˆå§‹åŒ–
        if not await self._ensure_mcp_initialized():
            return {
                "success": False,
                "error": "MCPå·¥å…·é›†æˆæœªåˆå§‹åŒ–æˆ–åˆå§‹åŒ–å¤±è´¥",
                "result": None
            }
        
        try:
            tool_call = await self.mcp_integration.call_tool_by_name(tool_name, arguments)
            return {
                "success": tool_call.success,
                "error": tool_call.error,
                "result": tool_call.result,
                "execution_time": tool_call.execution_time
            }
        except Exception as e:
            self.logger.error(f"MCPå·¥å…·è°ƒç”¨å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "result": None
            }
    
    def get_available_mcp_tools(self) -> Dict[str, Dict[str, Any]]:
        """è·å–å¯ç”¨çš„MCPå·¥å…·"""
        if self.mcp_integration:
            return self.mcp_integration.get_tool_definitions()
        return {}
    
    def suggest_mcp_tools_for_query(self, query: str) -> List[str]:
        """ä¸ºæŸ¥è¯¢å»ºè®®åˆé€‚çš„MCPå·¥å…·"""
        if self.mcp_integration:
            return self.mcp_integration.suggest_tools_for_query(query)
        return []
    
    async def store_search_results_with_enhanced_processing(self, search_results: List[SearchResult]) -> bool:
        """
        å…¬å…±æ–¹æ³•ï¼šä½¿ç”¨å¢å¼ºæ–‡æœ¬å¤„ç†å™¨å­˜å‚¨æœç´¢ç»“æœåˆ°å‘é‡æ•°æ®åº“
        
        Args:
            search_results: æœç´¢ç»“æœåˆ—è¡¨
            
        Returns:
            bool: å­˜å‚¨æ˜¯å¦æˆåŠŸ
            
        Features:
            - æ™ºèƒ½æ–‡æœ¬åˆ†å—å’Œæ¸…ç†
            - ä¸­è‹±æ–‡æ··åˆå¤„ç†
            - å†…å®¹å»é‡å’Œè´¨é‡è¯„åˆ†
            - å…³é”®è¯æå–
            - è¯­è¨€æ£€æµ‹
        """
        return await self._store_search_results_to_vector(search_results)
    
    async def _store_search_results_to_vector(self, search_results: List[SearchResult]) -> bool:
        """å°†æœç´¢ç»“æœå­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ï¼ˆä½¿ç”¨å¢å¼ºæ–‡æœ¬å¤„ç†ï¼‰"""
        try:
            if not search_results:
                self.logger.warning("âš ï¸ æ²¡æœ‰æœç´¢ç»“æœéœ€è¦å­˜å‚¨")
                return False
            
            # è½¬æ¢æœç´¢ç»“æœæ ¼å¼
            formatted_results = []
            for result in search_results:
                formatted_results.append({
                    'title': result.title,
                    'content': result.content,
                    'url': result.url,
                    'source': result.source,
                    'timestamp': result.timestamp,
                    'relevance_score': result.relevance_score
                })
            
            # ä½¿ç”¨å¢å¼ºæ–‡æœ¬å¤„ç†å™¨å¤„ç†æœç´¢ç»“æœ
            text_chunks = self.text_processor.process_search_results(formatted_results)
            
            # ä¼˜åŒ–chunksç”¨äºembedding
            optimized_chunks = self.text_processor.optimize_for_embedding(text_chunks)
            
            if not optimized_chunks:
                self.logger.warning("âš ï¸ æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„æ–‡æœ¬å—")
                return False
            
            # å‡†å¤‡å‘é‡å­˜å‚¨æ•°æ®
            documents = []
            for chunk in optimized_chunks:
                documents.append({
                    'content': chunk.content,
                    'title': chunk.title,
                    'url': chunk.url,
                    'source': 'google_search',
                    'timestamp': time.time(),
                    'chunk_id': chunk.chunk_id,
                    'token_count': chunk.token_count,
                    'language': chunk.language,
                    'importance_score': chunk.importance_score,
                    'keywords': chunk.keywords,
                    'metadata': chunk.metadata
                })
            
            # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
            # ä¼˜å…ˆä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„å‘é‡å­˜å‚¨ï¼Œå¦åˆ™ä½¿ç”¨åŠ¨æ€å‘é‡å­˜å‚¨
            vector_store = self.vector_store or self.vector_store_manager.get_store("dynamic")
            
            if vector_store:
                # å¯¹äºDynamicVectorStoreï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æœç´¢ç»“æœ
                if hasattr(vector_store, 'store_search_results'):
                    stored_count = await vector_store.store_search_results(search_results)
                    if stored_count > 0:
                        self.logger.info(f"âœ… æˆåŠŸå­˜å‚¨ {stored_count} ä¸ªæœç´¢ç»“æœåˆ°å‘é‡æ•°æ®åº“")
                        return True
                    else:
                        self.logger.error("âŒ å‘é‡å­˜å‚¨å¤±è´¥")
                        return False
                # å¯¹äºå…¶ä»–ç±»å‹çš„å‘é‡å­˜å‚¨ï¼Œå°è¯•ä½¿ç”¨add_documentsæ–¹æ³•
                elif hasattr(vector_store, 'add_documents'):
                    success = await vector_store.add_documents(documents)
                    if success:
                        self.logger.info(f"âœ… æˆåŠŸå­˜å‚¨ {len(optimized_chunks)} ä¸ªä¼˜åŒ–æ–‡æœ¬å—åˆ°å‘é‡æ•°æ®åº“")
                        return True
                    else:
                        self.logger.error("âŒ å‘é‡å­˜å‚¨å¤±è´¥")
                        return False
                else:
                    self.logger.error("âŒ å‘é‡å­˜å‚¨å¯¹è±¡ä¸æ”¯æŒå­˜å‚¨æ“ä½œ")
                    return False
            else:
                self.logger.warning("âš ï¸ å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–")
                self.logger.warning("âš ï¸ å¢å¼ºå­˜å‚¨å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•")
                # å°è¯•ä½¿ç”¨å¤‡ç”¨å­˜å‚¨æ–¹æ³•
            return await self._fallback_store_search_results(search_results)
                
        except Exception as e:
            self.logger.error(f"âŒ å­˜å‚¨æœç´¢ç»“æœåˆ°å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            self.logger.warning("âš ï¸ å¢å¼ºå­˜å‚¨å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•")
            # å°è¯•ä½¿ç”¨å¤‡ç”¨å­˜å‚¨æ–¹æ³•
            return await self._fallback_store_search_results(search_results)
    
    async def _fallback_store_search_results(self, search_results: List[SearchResult]) -> bool:
        """å¤‡ç”¨å­˜å‚¨æ–¹æ³•ï¼šä½¿ç”¨åŸå§‹çš„å­˜å‚¨é€»è¾‘"""
        try:
            self.logger.info("ğŸ”„ ä½¿ç”¨å¤‡ç”¨å­˜å‚¨æ–¹æ³•...")
            await self._store_search_results(search_results)
            self.logger.info("âœ… å¤‡ç”¨å­˜å‚¨æ–¹æ³•æ‰§è¡ŒæˆåŠŸ")
            return True
        except Exception as e:
            self.logger.error(f"âŒ å¤‡ç”¨å­˜å‚¨æ–¹æ³•ä¹Ÿå¤±è´¥äº†: {e}")
            return False

    async def process_query(self, context: QueryContext) -> RAGResponse:
        """
        æ™ºèƒ½å¤„ç†æŸ¥è¯¢è¯·æ±‚ - é›†æˆGo demoçš„åˆ†æèƒ½åŠ›
        
        æµç¨‹ï¼š
        1. æ™ºèƒ½åˆ†ææŸ¥è¯¢æ„å›¾
        2. æ ¹æ®åˆ†æç»“æœé€‰æ‹©æœ€ä¼˜ç­–ç•¥
        3. æ‰§è¡Œç›¸åº”çš„å·¥å…·è°ƒç”¨
        4. ç”Ÿæˆç»¼åˆå›ç­”
        """
        start_time = time.time()
        query = context.query
        
        try:
            self.logger.info(f"ğŸ¤– å¼€å§‹æ™ºèƒ½å¤„ç†æŸ¥è¯¢: {query}")
            
            # 1. æ™ºèƒ½æŸ¥è¯¢åˆ†æ - æ ¸å¿ƒæ”¹è¿›
            analysis_result = await self.smart_analyzer.analyze_query_intent(query)
            self.logger.info(f"ğŸ§  æŸ¥è¯¢åˆ†æå®Œæˆ: {analysis_result.query_type} "
                           f"(ç½®ä¿¡åº¦: {analysis_result.confidence:.2f})")
            
            # 1.5 MCPå·¥å…·å»ºè®®å’Œå¢å¼º
            if self.mcp_integration:
                suggested_tools = self.suggest_mcp_tools_for_query(query)
                if suggested_tools:
                    self.logger.info(f"ğŸ› ï¸ å»ºè®®ä½¿ç”¨MCPå·¥å…·: {', '.join(suggested_tools)}")
                    
                    # æ ¹æ®å»ºè®®çš„å·¥å…·è°ƒæ•´åˆ†æç»“æœ
                    if "calculator" in suggested_tools and not analysis_result.needs_calculation:
                        # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¯ç”¨è®¡ç®—
                        if re.search(r'\d+.*[+\-*/].*\d+', query):
                            analysis_result.needs_calculation = True
                            self.logger.info("ğŸ”§ æ ¹æ®MCPå·¥å…·å»ºè®®å¯ç”¨è®¡ç®—åŠŸèƒ½")
                    
                    if "database_query" in suggested_tools and not analysis_result.needs_database:
                        # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¯ç”¨æ•°æ®åº“æŸ¥è¯¢
                        db_keywords = ["ç”¨æˆ·", "æ•°æ®", "ç»Ÿè®¡", "æŸ¥è¯¢", "è¡¨"]
                        if any(keyword in query.lower() for keyword in db_keywords):
                            analysis_result.needs_database = True
                            # æ„å»ºç®€å•çš„æ•°æ®åº“æŸ¥è¯¢å‚æ•°
                            analysis_result.database_query = {
                                "query": "select",
                                "query_type": "structured", 
                                "table_name": "users",
                                "limit": 10
                            }
                            self.logger.info("ğŸ”§ æ ¹æ®MCPå·¥å…·å»ºè®®å¯ç”¨æ•°æ®åº“æŸ¥è¯¢åŠŸèƒ½")
                    
                    # æ–°å¢ï¼šæ ¹æ®MCPå·¥å…·å»ºè®®å¯ç”¨ç½‘ç»œæœç´¢
                    if "web_search" in suggested_tools and not analysis_result.needs_web_search:
                        search_keywords = ["æœ€æ–°", "æ–°é—»", "å®æ—¶", "å½“å‰", "ä»Šå¤©", "ç°åœ¨", "æœç´¢"]
                        if any(keyword in query.lower() for keyword in search_keywords):
                            analysis_result.needs_web_search = True
                            analysis_result.web_search_query = query
                            self.logger.info("ğŸ”§ æ ¹æ®MCPå·¥å…·å»ºè®®å¯ç”¨ç½‘ç»œæœç´¢åŠŸèƒ½")
            
            # 2. æ ¹æ®åˆ†æç»“æœæ‰§è¡Œç›¸åº”ç­–ç•¥
            search_results = []
            vector_results = []
            calculation_results = []
            database_results = []
            
            # è®¡ç®—å¤„ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if analysis_result.needs_calculation:
                self.logger.info("ğŸ§® æ‰§è¡Œæ•°å­¦è®¡ç®—...")
                
                # ä¼˜å…ˆå°è¯•ä½¿ç”¨MCPè®¡ç®—å™¨å·¥å…·
                if self.mcp_integration:
                    # å°è¯•è§£æè®¡ç®—è¡¨è¾¾å¼
                    calc_args = analysis_result.calculation_args
                    if calc_args and isinstance(calc_args, dict):
                        operation = calc_args.get("operation")
                        
                        # æ”¯æŒåŸºæœ¬çš„äºŒå…ƒè¿ç®—
                        if operation in ["add", "subtract", "multiply", "divide"]:
                            # ç›´æ¥ä½¿ç”¨xå’Œyå‚æ•°
                            x = calc_args.get("x")
                            y = calc_args.get("y")
                            
                            if x is not None and y is not None:
                                try:
                                    mcp_result = await self.call_mcp_tool("calculator", {
                                        "operation": operation,
                                        "x": float(x),
                                        "y": float(y)
                                    })
                                    
                                    if mcp_result["success"]:
                                        operation_symbols = {
                                            "add": "+",
                                            "subtract": "-",
                                            "multiply": "*",
                                            "divide": "/"
                                        }
                                        symbol = operation_symbols.get(operation, operation)
                                        calculation_results.append({
                                            "type": "mcp_calculation",
                                            "expression": f"{x} {symbol} {y}",
                                            "result": mcp_result["result"],
                                            "tool": "MCP Calculator",
                                            "execution_time": mcp_result.get("execution_time", 0)
                                        })
                                        self.logger.info(f"âœ… MCPè®¡ç®—å™¨æ‰§è¡ŒæˆåŠŸ: {x} {symbol} {y} = {mcp_result['result']}")
                                    else:
                                        self.logger.warning(f"âš ï¸ MCPè®¡ç®—å™¨æ‰§è¡Œå¤±è´¥: {mcp_result.get('error')}")
                                        
                                except Exception as e:
                                    self.logger.error(f"âŒ MCPè®¡ç®—å™¨è°ƒç”¨å¼‚å¸¸: {e}")

                        elif operation == "expression":
                            # è¡¨è¾¾å¼è®¡ç®—
                            expression = calc_args.get("expression", "")
                            if expression:
                                try:
                                    mcp_result = await self.call_mcp_tool("calculator", {
                                        "operation": "expression",
                                        "expression": expression
                                    })
                                    
                                    if mcp_result["success"]:
                                        calculation_results.append({
                                            "type": "mcp_calculation",
                                            "expression": expression,
                                            "result": mcp_result["result"],
                                            "tool": "MCP Calculator",
                                            "execution_time": mcp_result.get("execution_time", 0)
                                        })
                                        mcp_calc_success = True
                                        self.logger.info(f"âœ… MCPè¡¨è¾¾å¼è®¡ç®—æˆåŠŸ: {expression} = {mcp_result['result']}")
                                    else:
                                        self.logger.warning(f"âš ï¸ MCPè¡¨è¾¾å¼è®¡ç®—å¤±è´¥: {mcp_result.get('error')}")
                                        
                                except Exception as e:
                                    self.logger.error(f"âŒ MCPè¡¨è¾¾å¼è®¡ç®—å¼‚å¸¸: {e}")
    
            
            # å‘é‡æœç´¢ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if analysis_result.needs_vector_search:
                self.logger.info("ğŸ” æ‰§è¡Œå‘é‡æœç´¢...")
                vector_results = await self._perform_vector_search(query, context.max_results)
                
                # åŠ¨æ€æœç´¢ç­–ç•¥ï¼šæ£€æŸ¥å‘é‡æœç´¢ç»“æœè´¨é‡
                if analysis_result.enable_dynamic_search and vector_results:
                    max_similarity = max((result.get("similarity_score", 0) for result in vector_results), default=0)
                    self.logger.info(f"ğŸ“Š å‘é‡æœç´¢æœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.3f}")
                    
                    if max_similarity < analysis_result.min_similarity_threshold:
                        self.logger.warning(f"âš ï¸ å‘é‡æœç´¢ç›¸ä¼¼åº¦è¿‡ä½ ({max_similarity:.3f} < {analysis_result.min_similarity_threshold})ï¼Œå¯ç”¨ç½‘ç»œæœç´¢")
                        analysis_result.needs_web_search = True
                        analysis_result.web_search_query = query
                        analysis_result.reasoning += f" - å‘é‡æœç´¢ç›¸ä¼¼åº¦è¿‡ä½({max_similarity:.3f})ï¼Œå¯ç”¨ç½‘ç»œæœç´¢"
                elif analysis_result.enable_dynamic_search and not vector_results:
                    self.logger.warning("âš ï¸ å‘é‡æœç´¢æ— ç»“æœï¼Œå¯ç”¨ç½‘ç»œæœç´¢")
                    analysis_result.needs_web_search = True
                    analysis_result.web_search_query = query
                    analysis_result.reasoning += " - å‘é‡æœç´¢æ— ç»“æœï¼Œå¯ç”¨ç½‘ç»œæœç´¢"

            # ç½‘ç»œæœç´¢ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if analysis_result.needs_web_search:
                self.logger.info(f"ğŸŒ æ‰§è¡Œç½‘ç»œæœç´¢: {analysis_result.web_search_query}")
                
                # ä¼˜å…ˆå°è¯•ä½¿ç”¨MCPç½‘ç»œæœç´¢å·¥å…·
                mcp_search_success = False
                if self.mcp_integration:
                    try:
                        search_limit = min(context.max_results, 10)
                        mcp_result = await self.call_mcp_tool("web_search", {
                            "query": analysis_result.web_search_query,
                            "limit": search_limit,
                        })
                        self.logger.info(f"ğŸ” MCPç½‘ç»œæœç´¢åŸå§‹ç»“æœ: {mcp_result}")

                        if mcp_result["success"]:
                            # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
                            debug_info = self.debug_mcp_search_result(mcp_result)
                            self.logger.info(f"ğŸ”§ MCPæœç´¢ç»“æœè°ƒè¯•ä¿¡æ¯: {debug_info}")
                            
                            mcp_search_data = mcp_result["result"]
                            self.logger.info(f"ğŸ“ MCPæœç´¢æ•°æ®ç±»å‹: {type(mcp_search_data)}")
                            
                            # å¤„ç†ä¸åŒæ ¼å¼çš„MCPæœç´¢ç»“æœ
                            mcp_search_results = []
                            
                            if isinstance(mcp_search_data, dict):
                                # æƒ…å†µ1: æ ‡å‡†å­—å…¸æ ¼å¼ {"results": [...]}
                                if "results" in mcp_search_data and isinstance(mcp_search_data["results"], list):
                                    for item in mcp_search_data["results"]:
                                        search_result = self._safe_create_search_result(
                                            title=item.get("title", ""),
                                            content=item.get("snippet", item.get("content", "")),
                                            url=item.get("url", item.get("link", "")),
                                            source="MCP Web Search",
                                            relevance_score=item.get("relevance_score", 0.8),
                                            channel_type="MCP_SEARCH"
                                        )
                                        mcp_search_results.append(search_result)
                                    
                                    mcp_search_success = True
                                    self.logger.info(f"âœ… MCPç½‘ç»œæœç´¢æˆåŠŸ(æ ‡å‡†æ ¼å¼)ï¼Œè·å¾— {len(mcp_search_results)} ä¸ªç»“æœ")
                                
                                # æƒ…å†µ2: ç›´æ¥æ˜¯æœç´¢ç»“æœå­—å…¸ {"title": ..., "content": ...}
                                elif "title" in mcp_search_data or "content" in mcp_search_data:
                                    search_result = self._safe_create_search_result(
                                        title=mcp_search_data.get("title", ""),
                                        content=mcp_search_data.get("snippet", mcp_search_data.get("content", "")),
                                        url=mcp_search_data.get("url", mcp_search_data.get("link", "")),
                                        source="MCP Web Search",
                                        relevance_score=mcp_search_data.get("relevance_score", 0.8),
                                        channel_type="MCP_SEARCH"
                                    )
                                    mcp_search_results.append(search_result)
                                    
                                    mcp_search_success = True
                                    self.logger.info(f"âœ… MCPç½‘ç»œæœç´¢æˆåŠŸ(å•ç»“æœæ ¼å¼)ï¼Œè·å¾— 1 ä¸ªç»“æœ")
                                
                                # æƒ…å†µ3: å…¶ä»–å­—å…¸æ ¼å¼ï¼Œå°è¯•è§£æ
                                else:
                                    self.logger.warning(f"âš ï¸ MCPæœç´¢ç»“æœä¸ºå­—å…¸ä½†æ ¼å¼æœªçŸ¥ï¼Œå°è¯•é€šç”¨è§£æ: {list(mcp_search_data.keys())}")
                                    # å°è¯•å°†æ•´ä¸ªå­—å…¸ä½œä¸ºä¸€ä¸ªæœç´¢ç»“æœ
                                    content = str(mcp_search_data)[:500]  # æˆªå–å‰500å­—ç¬¦
                                    search_result = self._safe_create_search_result(
                                        title="MCPæœç´¢ç»“æœ",
                                        content=content,
                                        url="",
                                        source="MCP Web Search (Raw)",
                                        relevance_score=0.6,
                                        channel_type="MCP_SEARCH"
                                    )
                                    mcp_search_results.append(search_result)
                                    
                                    mcp_search_success = True
                                    self.logger.info(f"âœ… MCPç½‘ç»œæœç´¢æˆåŠŸ(é€šç”¨è§£æ)ï¼Œè·å¾— 1 ä¸ªç»“æœ")
                            
                            elif isinstance(mcp_search_data, list):
                                # æƒ…å†µ4: ç›´æ¥æ˜¯åˆ—è¡¨æ ¼å¼
                                for item in mcp_search_data:
                                    if isinstance(item, dict):
                                        search_result = self._safe_create_search_result(
                                            title=item.get("title", ""),
                                            content=item.get("snippet", item.get("content", "")),
                                            url=item.get("url", item.get("link", "")),
                                            source="MCP Web Search",
                                            relevance_score=item.get("relevance_score", 0.8),
                                            channel_type="MCP_SEARCH"
                                        )
                                        mcp_search_results.append(search_result)
                                    else:
                                        # åˆ—è¡¨é¡¹ä¸æ˜¯å­—å…¸ï¼Œç›´æ¥ä½œä¸ºå†…å®¹
                                        search_result = self._safe_create_search_result(
                                            title=f"MCPæœç´¢ç»“æœ {len(mcp_search_results) + 1}",
                                            content=item,
                                            url="",
                                            source="MCP Web Search (List)",
                                            relevance_score=0.7,
                                            channel_type="MCP_SEARCH"
                                        )
                                        mcp_search_results.append(search_result)
                                
                                mcp_search_success = True
                                self.logger.info(f"âœ… MCPç½‘ç»œæœç´¢æˆåŠŸ(åˆ—è¡¨æ ¼å¼)ï¼Œè·å¾— {len(mcp_search_results)} ä¸ªç»“æœ")
                            
                            elif isinstance(mcp_search_data, str):
                                # æƒ…å†µ5: å­—ç¬¦ä¸²æ ¼å¼ï¼Œå¯èƒ½éœ€è¦è§£æ
                                self.logger.info("ğŸ“ MCPè¿”å›å­—ç¬¦ä¸²æ ¼å¼ï¼Œå°è¯•è§£æ...")
                                parsed_results = self._parse_mcp_search_results(mcp_search_data)
                                
                                for item in parsed_results:
                                    search_result = self._safe_create_search_result(
                                        title=item.get("title", ""),
                                        content=item.get("content", item.get("snippet", "")),
                                        url=item.get("url", item.get("link", "")),
                                        source="MCP Web Search (Parsed)",
                                        relevance_score=item.get("relevance_score", 0.8),
                                        channel_type="MCP_SEARCH"
                                    )
                                    mcp_search_results.append(search_result)
                                
                                mcp_search_success = True
                                self.logger.info(f"âœ… MCPç½‘ç»œæœç´¢æˆåŠŸ(å­—ç¬¦ä¸²è§£æ)ï¼Œè·å¾— {len(mcp_search_results)} ä¸ªç»“æœ")
                            
                            else:
                                self.logger.warning(f"âš ï¸ MCPæœç´¢ç»“æœæ ¼å¼ä¸æ”¯æŒ: {type(mcp_search_data)}")
                            
                            # å°†MCPæœç´¢ç»“æœæ·»åŠ åˆ°æ€»æœç´¢ç»“æœä¸­
                            if mcp_search_success and mcp_search_results:
                                search_results.extend(mcp_search_results)
                                self.logger.info(f"ğŸ“Š æ€»æœç´¢ç»“æœæ•°é‡: {len(search_results)}")
                        else:
                            self.logger.warning(f"âš ï¸ MCPç½‘ç»œæœç´¢å¤±è´¥: {mcp_result.get('error')}")
                    except Exception as e:
                        self.logger.error(f"âŒ MCPç½‘ç»œæœç´¢å¼‚å¸¸: {e}")
                        import traceback
                        self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                     
                if not mcp_search_success:
                    # ä½¿ç”¨å¢å¼ºæ–‡æœ¬å¤„ç†å™¨æ‰§è¡Œç½‘ç»œæœç´¢
                    self.logger.info("ğŸ”„ MCPç½‘ç»œæœç´¢å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæœç´¢æ–¹æ³•...")
                    
                    # åˆ›å»ºæŸ¥è¯¢ä¸Šä¸‹æ–‡     
                    search_context = QueryContext(
                        query=analysis_result.web_search_query,
                        query_type=context.query_type,
                        max_results=context.max_results,
                        timeout=context.timeout
                    )
                    fallback_search_results = await self._perform_search(search_context)
                    search_results.extend(fallback_search_results)  # ä½¿ç”¨extendè€Œä¸æ˜¯èµ‹å€¼
                    self.logger.info(f"ğŸ”„ ä¼ ç»Ÿæœç´¢è·å¾— {len(fallback_search_results)} ä¸ªç»“æœï¼Œæ€»è®¡ {len(search_results)} ä¸ªç»“æœ")

                # ä½¿ç”¨å¢å¼ºæ–‡æœ¬å¤„ç†å™¨å­˜å‚¨æœç´¢ç»“æœåˆ°å‘é‡æ•°æ®åº“
                if search_results:
                    success = await self._store_search_results_to_vector(search_results)
                    if success:
                        self.logger.info(f"ğŸ’¾ ä½¿ç”¨å¢å¼ºå¤„ç†å™¨æˆåŠŸå­˜å‚¨äº† {len(search_results)} ä¸ªæœç´¢ç»“æœ")
                    else:
                        self.logger.warning("âš ï¸ å¢å¼ºå­˜å‚¨å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•")
                        await self._store_search_results(search_results)
            
            # æ•°æ®åº“æŸ¥è¯¢ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if analysis_result.needs_database:
                self.logger.info("ğŸ—„ï¸ æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢...")
                
                # ä¼˜å…ˆå°è¯•ä½¿ç”¨MCPæ•°æ®åº“å·¥å…·
                mcp_db_success = False
                if self.mcp_integration and analysis_result.database_query:
                    try:
                        db_query = analysis_result.database_query
                        mcp_db_args = {
                            "query": db_query.get("query", ""),
                            "query_type": db_query.get("query_type", "structured"),
                            "database": db_query.get("database", "default")
                        }
                        
                        # æ·»åŠ å¯é€‰å‚æ•°
                        optional_params = ["table_name", "fields", "where_conditions", 
                                         "order_by", "limit", "offset", "group_by", "having"]
                        for param in optional_params:
                            if param in db_query:
                                mcp_db_args[param] = db_query[param]
                        
                        mcp_result = await self.call_mcp_tool("database_query", mcp_db_args)
                        
                        if mcp_result["success"]:
                            database_results.append({
                                "type": "mcp_database",
                                "query": mcp_db_args,
                                "result": mcp_result["result"],
                                "source": "MCPæ•°æ®åº“å·¥å…·",
                                "timestamp": time.time(),
                                "execution_time": mcp_result.get("execution_time", 0)
                            })
                            mcp_db_success = True
                            self.logger.info("âœ… MCPæ•°æ®åº“æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸ")
                        else:
                            self.logger.warning(f"âš ï¸ MCPæ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {mcp_result.get('error')}")
                            
                    except Exception as e:
                        self.logger.error(f"âŒ MCPæ•°æ®åº“æŸ¥è¯¢å¼‚å¸¸: {e}")
                
                # å¦‚æœMCPæ•°æ®åº“æŸ¥è¯¢å¤±è´¥ï¼Œä½¿ç”¨å†…ç½®æ–¹æ³•
                if not mcp_db_success:
                    self.logger.info("ğŸ”„ ä½¿ç”¨å†…ç½®æ•°æ®åº“æŸ¥è¯¢æ–¹æ³•...")
                    database_results = await self._perform_database_query(analysis_result.database_query)
            
            # 3. èåˆæ‰€æœ‰ç»“æœ
            all_results = self._merge_all_results(
                search_results, vector_results, calculation_results, database_results
            )
            
            # 4. ç”Ÿæˆæ™ºèƒ½ç­”æ¡ˆ
            answer, confidence = await self._generate_smart_answer(
                query, all_results, analysis_result
            )
            
            # 5. æ„å»ºå¢å¼ºå“åº”
            processing_time = time.time() - start_time
            response = RAGResponse(
                answer=answer,
                sources=self._extract_sources(all_results),
                search_results=search_results,
                processing_time=processing_time,
                confidence_score=confidence,
                analysis_result=analysis_result,  # æ–°å¢åˆ†æç»“æœ
                metadata={
                    "query_type": context.query_type.value,
                    "analysis_type": analysis_result.query_type,
                    "analysis_confidence": analysis_result.confidence,
                    "total_results": len(all_results),
                    "search_results_count": len(search_results),
                    "vector_results_count": len(vector_results),
                    "calculation_results_count": len(calculation_results),
                    "database_results_count": len(database_results),
                    "used_search_engine": analysis_result.needs_web_search,
                    "used_vector_search": analysis_result.needs_vector_search,
                    "used_calculation": analysis_result.needs_calculation,
                    "used_database": analysis_result.needs_database,
                    "analysis_reasoning": analysis_result.reasoning,
                    "tools_used": [tool.name for tool in analysis_result.tool_calls],
                    "strategy": self.smart_analyzer.get_search_strategy(analysis_result)
                }
            )
            
            self.logger.info(f"âœ… æ™ºèƒ½æŸ¥è¯¢å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}s, "
                           f"ç­–ç•¥: {response.metadata['strategy']}")
            return response
            
        except Exception as e:
            self.logger.error(f"æ™ºèƒ½æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            return RAGResponse(
                answer=f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æŸ¥è¯¢æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                sources=[],
                search_results=[],
                processing_time=time.time() - start_time,
                confidence_score=0.0,
                metadata={"error": str(e), "fallback": True}
            )
    
    async def _perform_search(self, context: QueryContext) -> List[SearchResult]:
        """æ‰§è¡Œå®æ—¶æœç´¢"""
        try:
            return await self.mcp_processor.process_query(context)
        except Exception as e:
            self.logger.error(f"å®æ—¶æœç´¢å¤±è´¥: {e}")
            return []
    
    async def _perform_vector_search(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """æ‰§è¡Œå‘é‡æœç´¢"""
        try:
            all_results = await self.vector_store_manager.search_all_stores(query, limit)
            
            # åˆå¹¶æ‰€æœ‰å­˜å‚¨çš„ç»“æœ
            merged_results = []
            for store_name, results in all_results.items():
                for result in results:
                    result["store_name"] = store_name
                    merged_results.append(result)
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            merged_results.sort(key=lambda x: x.get("similarity_score", 0), reverse=True)
            
            return merged_results[:limit]
            
        except Exception as e:
            self.logger.error(f"å‘é‡æœç´¢å¤±è´¥: {e}")
            return []
    
    async def _store_search_results(self, search_results: List[SearchResult]):
        """å­˜å‚¨æœç´¢ç»“æœåˆ°å‘é‡æ•°æ®åº“"""
        try:
            dynamic_store = self.vector_store_manager.get_store("dynamic")
            if dynamic_store:
                stored_count = await dynamic_store.store_search_results(search_results)
                self.logger.info(f"å­˜å‚¨äº† {stored_count} ä¸ªæœç´¢ç»“æœ")
        except Exception as e:
            self.logger.error(f"å­˜å‚¨æœç´¢ç»“æœå¤±è´¥: {e}")
    
    def _merge_results(self, 
                      search_results: List[SearchResult], 
                      vector_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """èåˆæœç´¢ç»“æœå’Œå‘é‡ç»“æœ"""
        merged = []
        
        # æ·»åŠ å®æ—¶æœç´¢ç»“æœï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰
        for result in search_results:
            merged.append({
                "content": result.content,
                "title": result.title,
                "url": result.url,
                "source": result.source,
                "score": result.relevance_score + 0.1,  # ç»™æ–°æœç´¢ç»“æœåŠ æƒ
                "type": "search",
                "timestamp": result.timestamp
            })
        
        # æ·»åŠ å‘é‡æœç´¢ç»“æœ
        for result in vector_results:
            merged.append({
                "content": result.get("content", ""),
                "title": result.get("title", ""),
                "url": result.get("url", ""),
                "source": result.get("source", ""),
                "score": result.get("similarity_score", 0),
                "type": "vector",
                "store_name": result.get("store_name", ""),
                "timestamp": result.get("timestamp", 0)
            })
        
        # å»é‡å’Œæ’åº
        seen_urls = set()
        deduplicated = []
        
        for result in merged:
            url = result.get("url", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                deduplicated.append(result)
            elif not url:  # æ²¡æœ‰URLçš„ç»“æœä¹Ÿä¿ç•™
                deduplicated.append(result)
        
        # æŒ‰åˆ†æ•°æ’åº
        deduplicated.sort(key=lambda x: x.get("score", 0), reverse=True)
        
        return deduplicated

    def _parse_mcp_search_results(self, search_text: str) -> List[Dict[str, Any]]:
        """
        è§£æMCPæœç´¢ç»“æœæ–‡æœ¬æ ¼å¼
        
        Args:
            search_text: MCPè¿”å›çš„æœç´¢ç»“æœæ–‡æœ¬
            
        Returns:
            List[Dict]: è§£æåçš„æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        try:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£ææœç´¢ç»“æœ
            # åŒ¹é…æ ¼å¼: æ•°å­—. "æ ‡é¢˜"\n   URL: url\n   æ‘˜è¦: æ‘˜è¦å†…å®¹
            pattern = r'(\d+)\.\s*"([^"]+)"\s*\n\s*URL:\s*([^\n]+)\s*\n\s*æ‘˜è¦:\s*([^\n]+)'
            
            matches = re.findall(pattern, search_text, re.MULTILINE | re.DOTALL)
            
            for match in matches:
                index, title, url, snippet = match
                
                # æ¸…ç†æ•°æ®
                title = title.strip()
                url = url.strip()
                snippet = snippet.strip()
                
                # ç§»é™¤æ‘˜è¦æœ«å°¾çš„çœç•¥å·å’Œç‰¹æ®Šå­—ç¬¦
                snippet = re.sub(r'[â€¦\.]{2,}$', '', snippet).strip()
                
                result = {
                    "title": title,
                    "content": snippet,
                    "snippet": snippet,
                    "url": url,
                    "link": url,
                    "relevance_score": 0.8,  # é»˜è®¤ç›¸å…³æ€§åˆ†æ•°
                    "index": int(index)
                }
                
                results.append(result)
            
            self.logger.info(f"ğŸ“ æˆåŠŸè§£æ {len(results)} ä¸ªæœç´¢ç»“æœ")
            
            # å¦‚æœæ­£åˆ™åŒ¹é…å¤±è´¥ï¼Œå°è¯•ç®€å•çš„è¡Œåˆ†å‰²è§£æ
            if not results:
                self.logger.warning("âš ï¸ æ­£åˆ™è§£æå¤±è´¥ï¼Œå°è¯•ç®€å•è§£æ")
                results = self._simple_parse_search_results(search_text)
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ è§£æMCPæœç´¢ç»“æœå¤±è´¥: {e}")
            # è¿”å›ä¸€ä¸ªåŒ…å«åŸå§‹æ–‡æœ¬çš„ç»“æœ
            return [{
                "title": "MCPæœç´¢ç»“æœ",
                "content": search_text[:500],  # æˆªå–å‰500å­—ç¬¦
                "snippet": search_text[:200],   # æˆªå–å‰200å­—ç¬¦ä½œä¸ºæ‘˜è¦
                "url": "",
                "relevance_score": 0.5
            }]
    
    def _simple_parse_search_results(self, search_text: str) -> List[Dict[str, Any]]:
        """
        ç®€å•è§£ææœç´¢ç»“æœæ–‡æœ¬ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰
        
        Args:
            search_text: æœç´¢ç»“æœæ–‡æœ¬
            
        Returns:
            List[Dict]: è§£æåçš„ç»“æœåˆ—è¡¨
        """
        results = []
        
        try:
            # æŒ‰è¡Œåˆ†å‰²æ–‡æœ¬
            lines = search_text.split('\n')
            current_result = {}
            
            for line in lines:
                line = line.strip()
                
                # æ£€æµ‹æ ‡é¢˜è¡Œï¼ˆä»¥æ•°å­—å¼€å¤´ï¼‰
                title_match = re.match(r'(\d+)\.\s*"?([^"]+)"?', line)
                if title_match:
                    # ä¿å­˜ä¸Šä¸€ä¸ªç»“æœ
                    if current_result and current_result.get('title'):
                        results.append(current_result)
                    
                    # å¼€å§‹æ–°ç»“æœ
                    current_result = {
                        "title": title_match.group(2).strip(),
                        "content": "",
                        "url": "",
                        "relevance_score": 0.8
                    }
                
                # æ£€æµ‹URLè¡Œ
                elif line.startswith('URL:'):
                    url = line.replace('URL:', '').strip()
                    if current_result:
                        current_result["url"] = url
                        current_result["link"] = url
                
                # æ£€æµ‹æ‘˜è¦è¡Œ
                elif line.startswith('æ‘˜è¦:'):
                    snippet = line.replace('æ‘˜è¦:', '').strip()
                    if current_result:
                        current_result["content"] = snippet
                        current_result["snippet"] = snippet
            
            # æ·»åŠ æœ€åä¸€ä¸ªç»“æœ
            if current_result and current_result.get('title'):
                results.append(current_result)
            
            self.logger.info(f"ğŸ“ ç®€å•è§£æè·å¾— {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ ç®€å•è§£æä¹Ÿå¤±è´¥äº†: {e}")
            return []

    def _extract_sources(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æå–æ¥æºä¿¡æ¯"""
        sources = []
        for result in results[:10]:  # æœ€å¤šè¿”å›10ä¸ªæ¥æº
            source = {
                "title": result.get("title", "æœªçŸ¥æ ‡é¢˜"),
                "url": result.get("url", ""),
                "source": result.get("source", "æœªçŸ¥æ¥æº"),
                "score": result.get("score", 0),
                "type": result.get("type", "unknown")
            }
            sources.append(source)
        
        return sources
    
    def update_smart_search_config(self, 
                                  similarity_threshold: float = None,
                                  min_vector_results: int = None,
                                  enable_smart_search: bool = None):
        """æ›´æ–°æ™ºèƒ½æœç´¢é…ç½®"""
        if similarity_threshold is not None:
            self.similarity_threshold = similarity_threshold
            self.logger.info(f"æ›´æ–°ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}")
        
        if min_vector_results is not None:
            self.min_vector_results = min_vector_results
            self.logger.info(f"æ›´æ–°æœ€å°‘å‘é‡ç»“æœæ•°é‡: {min_vector_results}")
        
        if enable_smart_search is not None:
            self.enable_smart_search = enable_smart_search
            self.logger.info(f"æ™ºèƒ½æœç´¢å¼€å…³: {enable_smart_search}")
    
    async def _perform_database_query(self, query_args: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        try:
            # è¿™é‡Œåº”è¯¥é›†æˆå®é™…çš„æ•°æ®åº“æŸ¥è¯¢åŠŸèƒ½
            # ç›®å‰æä¾›æ¨¡æ‹Ÿæ•°æ®
            query_type = query_args.get("query_type", "select")
            
            if query_type == "count":
                return [{
                    "type": "database_result",
                    "query": f"ç»Ÿè®¡æŸ¥è¯¢: {query_args}",
                    "result": "æ´»è·ƒç”¨æˆ·: 1250, éæ´»è·ƒç”¨æˆ·: 350",
                    "source": "ç”¨æˆ·æ•°æ®åº“",
                    "timestamp": time.time()
                }]
            
            elif query_type == "select":
                return [{
                    "type": "database_result",
                    "query": f"æŸ¥è¯¢: {query_args}",
                    "result": "è¿”å›äº†5æ¡ç”¨æˆ·è®°å½•",
                    "source": "ç”¨æˆ·æ•°æ®åº“",
                    "timestamp": time.time()
                }]
            
            return []
            
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
            return []


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
    def _merge_all_results(self, 
                          search_results: List[SearchResult], 
                          vector_results: List[Dict[str, Any]],
                          calculation_results: List[Dict[str, Any]],
                          database_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """èåˆæ‰€æœ‰ç±»å‹çš„ç»“æœ"""
        all_results = []
        
        # æœç´¢ç»“æœ
        for result in search_results:
            all_results.append({
                "type": "search",
                "title": result.title,
                "content": result.content,
                "url": result.url,
                "source": result.source,
                "timestamp": result.timestamp,
                "relevance_score": result.relevance_score,
                "channel_type": str(result.channel_type)
            })
        
        # å‘é‡æœç´¢ç»“æœ
        for result in vector_results:
            result["type"] = "vector"
            all_results.append(result)
        
        # è®¡ç®—ç»“æœ
        for result in calculation_results:
            result["type"] = "calculation"
            result["source"] = "è®¡ç®—å™¨"
            result["timestamp"] = time.time()
            all_results.append(result)
        
        # æ•°æ®åº“ç»“æœ
        for result in database_results:
            all_results.append(result)
        
        # æŒ‰ç›¸å…³æ€§æ’åº
        all_results.sort(key=lambda x: x.get("relevance_score", x.get("similarity_score", 0.5)), reverse=True)
        
        return all_results


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
    async def _generate_smart_answer(self, 
                                    query: str, 
                                    results: List[Dict[str, Any]],
                                    analysis: QueryAnalysisResult) -> Tuple[str, float]:
        """ç”Ÿæˆæ™ºèƒ½ç­”æ¡ˆ - åŸºäºæŸ¥è¯¢åˆ†æç»“æœ"""
        
        if not results:
            return self._generate_fallback_answer(query, analysis), 0.3
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        confidence_factors = []
        
        for result in results[:8]:  # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
            result_type = result.get("type", "unknown")
            
            if result_type == "calculation":
                if "result" in result:
                    context_parts.append(f"[è®¡ç®—ç»“æœ] {result.get('expression', '')} = {result['result']}")
                    confidence_factors.append(0.9)  # è®¡ç®—ç»“æœç½®ä¿¡åº¦é«˜
                elif "error" in result:
                    context_parts.append(f"[è®¡ç®—é”™è¯¯] {result['error']}")
                    confidence_factors.append(0.2)
            
            elif result_type == "database_result":
                context_parts.append(f"[æ•°æ®åº“] {result.get('result', '')}")
                confidence_factors.append(0.8)
            
            elif result_type == "search":
                content = result.get("content", "")[:300]  # é™åˆ¶é•¿åº¦
                source = result.get("source", "")
                context_parts.append(f"[æœç´¢] {content}\næ¥æº: {source}")
                confidence_factors.append(result.get("relevance_score", 0.5))
            
            elif result_type == "vector":
                content = result.get("content", "")[:300]
                source = result.get("source", "çŸ¥è¯†åº“")
                similarity = result.get("similarity_score", 0.5)
                context_parts.append(f"[çŸ¥è¯†åº“] {content}\næ¥æº: {source} (ç›¸ä¼¼åº¦: {similarity:.2f})")
                confidence_factors.append(similarity)
        
        context = "\n\n".join(context_parts)
        
        # æ ¹æ®åˆ†æç±»å‹æ„å»ºæç¤ºè¯
        prompt = self._build_answer_prompt(query, context, analysis)
        
        try:
            # è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ
            if hasattr(self, 'llm_client') and self.llm_client:
                # ä½¿ç”¨çœŸå®çš„LLMå®¢æˆ·ç«¯ - ç›´æ¥ä½¿ç”¨æ„å»ºå¥½çš„prompt
                answer = get_llm_answer_with_prompt(
                    client=self.llm_client,
                    prompt=prompt  # ä½¿ç”¨æ„å»ºå¥½çš„prompt
                )
            else:
                # LLMä¸å¯ç”¨æ—¶ï¼Œä½¿ç”¨æ™ºèƒ½å›é€€ç­”æ¡ˆç”Ÿæˆ
                self.logger.warning("âš ï¸ LLMå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œä½¿ç”¨æ™ºèƒ½å›é€€æ¨¡å¼")
                answer = self._synthesize_intelligent_answer(query, context, analysis)
            
            # è®¡ç®—ç½®ä¿¡åº¦
            avg_confidence = sum(confidence_factors) / len(confidence_factors) if confidence_factors else 0.5
            final_confidence = min(0.95, avg_confidence * analysis.confidence)
            
            # æ ¼å¼åŒ–ç­”æ¡ˆ
            formatted_answer = self._format_answer(answer)
            
            return formatted_answer, final_confidence
            
        except Exception as e:
            self.logger.error(f"LLMç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            return self._synthesize_answer_fallback(query, results), 0.4
    
    def _build_answer_prompt(self, query: str, context: str, analysis: QueryAnalysisResult) -> str:
        """æ ¹æ®æŸ¥è¯¢åˆ†æç»“æœæ„å»ºç­”æ¡ˆæç¤ºè¯"""
        
        if analysis.query_type == "time":
            return f"""ç”¨æˆ·è¯¢é—®æ—¶é—´ç›¸å…³é—®é¢˜ï¼š{query}

ä»¥ä¸‹æ˜¯è·å–çš„æœ€æ–°ä¿¡æ¯ï¼š
{context}

è¯·åŸºäºè¿™äº›æœ€æ–°ä¿¡æ¯å‡†ç¡®å›ç­”ç”¨æˆ·çš„æ—¶é—´ç›¸å…³é—®é¢˜ã€‚å¦‚æœä¿¡æ¯ä¸­åŒ…å«å…·ä½“çš„æ—¶é—´æ•°æ®ï¼Œè¯·ç›´æ¥æä¾›ã€‚"""
        
        elif analysis.query_type == "calculation":
            return f"""ç”¨æˆ·è¯¢é—®æ•°å­¦è®¡ç®—é—®é¢˜ï¼š{query}

è®¡ç®—ç»“æœï¼š
{context}

è¯·åŸºäºè®¡ç®—ç»“æœä¸ºç”¨æˆ·æä¾›æ¸…æ™°çš„æ•°å­¦ç­”æ¡ˆï¼Œå¹¶ç®€è¦è¯´æ˜è®¡ç®—è¿‡ç¨‹ã€‚"""
        
        elif analysis.query_type == "technical":
            return f"""ç”¨æˆ·è¯¢é—®æŠ€æœ¯é—®é¢˜ï¼š{query}

ç›¸å…³æŠ€æœ¯ä¿¡æ¯ï¼š
{context}

è¯·åŸºäºè¿™äº›æŠ€æœ¯èµ„æ–™æä¾›è¯¦ç»†ã€å‡†ç¡®çš„æŠ€æœ¯è§£ç­”ã€‚å¯ä»¥åŒ…å«æŠ€æœ¯ç»†èŠ‚å’Œå®ç°æ–¹æ³•ã€‚"""
        
        else:
            return f"""ç”¨æˆ·é—®é¢˜ï¼š{query}

ç›¸å…³ä¿¡æ¯ï¼š
{context}

è¯·ç»¼åˆä»¥ä¸Šä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚å¦‚æœä¿¡æ¯æ¥æºäºä¸åŒæ¸ é“ï¼Œè¯·é€‚å½“æ•´åˆã€‚"""
    
    def _synthesize_answer_fallback(self, query: str, results: List[Dict[str, Any]]) -> str:
        """åˆæˆå¤‡ç”¨ç­”æ¡ˆ"""
        if not results:
            return f"å…³äºæ‚¨çš„é—®é¢˜ã€Œ{query}ã€ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
        
        answer_parts = [f"å…³äºæ‚¨çš„é—®é¢˜ã€Œ{query}ã€ï¼Œæˆ‘æ‰¾åˆ°äº†ä»¥ä¸‹ä¿¡æ¯ï¼š\n"]
        
        for i, result in enumerate(results[:3], 1):
            result_type = result.get("type", "unknown")
            if result_type == "calculation" and "result" in result:
                answer_parts.append(f"{i}. è®¡ç®—ç»“æœï¼š{result.get('expression', '')} = {result['result']}")
            elif result_type == "database_result":
                answer_parts.append(f"{i}. æ•°æ®åº“æŸ¥è¯¢ï¼š{result.get('result', '')}")
            else:
                content = result.get("content", "")[:200]
                source = result.get("source", "")
                answer_parts.append(f"{i}. {content} (æ¥æºï¼š{source})")
        
        answer_parts.append("\nä»¥ä¸Šä¿¡æ¯ä¾›æ‚¨å‚è€ƒã€‚")
        return "\n\n".join(answer_parts)
    
    def _format_answer(self, answer: str) -> str:
        """æ ¼å¼åŒ–ç­”æ¡ˆè¾“å‡ºï¼Œç¾åŒ–Markdownå†…å®¹"""
        if not answer:
            return answer
        
        try:
            # ç§»é™¤è½¬ä¹‰å­—ç¬¦ï¼Œæ¢å¤æ­£å¸¸çš„æ¢è¡Œç¬¦
            formatted = answer.replace('\\n', '\n').replace('\\"', '"').replace("\\'", "'")
            
            # å¤„ç†ä»£ç å—æ ¼å¼
            
            # ç¡®ä¿ä»£ç å—æœ‰æ­£ç¡®çš„æ¢è¡Œ
            formatted = re.sub(r'```(\w*)\n?', r'```\1\n', formatted)
            formatted = re.sub(r'\n?```', r'\n```', formatted)
            
            # å¤„ç†æ ‡é¢˜æ ¼å¼ï¼Œç¡®ä¿æ ‡é¢˜å‰åæœ‰é€‚å½“çš„ç©ºè¡Œ
            formatted = re.sub(r'\n(#{1,6}\s+[^\n]+)', r'\n\n\1', formatted)
            formatted = re.sub(r'(#{1,6}\s+[^\n]+)\n([^#\n])', r'\1\n\n\2', formatted)
            
            # å¤„ç†åˆ—è¡¨é¡¹ï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®
            formatted = re.sub(r'\n-\s+', r'\n- ', formatted)
            formatted = re.sub(r'\n\*\s+', r'\n* ', formatted)
            formatted = re.sub(r'\n(\d+\.)\s+', r'\n\1 ', formatted)
            
            # å¤„ç†æ®µè½é—´è·ï¼Œç¡®ä¿æ®µè½ä¹‹é—´æœ‰é€‚å½“çš„ç©ºè¡Œ
            formatted = re.sub(r'\n\n\n+', r'\n\n', formatted)
            
            # å¤„ç†ç²—ä½“å’Œæ–œä½“æ ¼å¼
            formatted = re.sub(r'\*\*([^*]+)\*\*', r'**\1**', formatted)
            formatted = re.sub(r'\*([^*]+)\*', r'*\1*', formatted)
            
            # æ¸…ç†å¼€å¤´å’Œç»“å°¾çš„å¤šä½™ç©ºè¡Œ
            formatted = formatted.strip()
            
            # ç¡®ä¿å†…å®¹æœ‰è‰¯å¥½çš„ç»“æ„
            if formatted:
                # å¦‚æœå†…å®¹å¾ˆé•¿ï¼Œæ·»åŠ ä¸€ä¸ªç®€æ´çš„å¼€å¤´
                if len(formatted) > 1000 and not formatted.startswith('## ') and not formatted.startswith('# '):
                    lines = formatted.split('\n')
                    if len(lines) > 3:
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ æ¦‚è¿°
                        first_line = lines[0].strip()
                        if len(first_line) > 50 and '##' not in first_line:
                            formatted = f"**æ¦‚è¿°ï¼š** {first_line}\n\n{formatted}"
            
            return formatted
            
        except Exception as e:
            self.logger.error(f"æ ¼å¼åŒ–ç­”æ¡ˆæ—¶å‡ºé”™: {e}")
            # å¦‚æœæ ¼å¼åŒ–å¤±è´¥ï¼Œè‡³å°‘å¤„ç†åŸºæœ¬çš„è½¬ä¹‰å­—ç¬¦
            return answer.replace('\\n', '\n').replace('\\"', '"').replace("\\'", "'")
    
    def _generate_fallback_answer(self, query: str, analysis: QueryAnalysisResult) -> str:
        """ç”Ÿæˆå›é€€ç­”æ¡ˆ - å½“æ²¡æœ‰æœç´¢ç»“æœæ—¶ä½¿ç”¨"""
        fallback_messages = {
            "time": f"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•è·å–å½“å‰çš„æ—¶é—´ä¿¡æ¯æ¥å›ç­”ã€Œ{query}ã€ã€‚è¯·æ‚¨æŸ¥çœ‹ç³»ç»Ÿæ—¶é—´æˆ–è”ç½‘è·å–æœ€æ–°æ—¶é—´ã€‚",
            "calculation": f"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•è®¡ç®—ã€Œ{query}ã€ã€‚è¯·æ£€æŸ¥è®¡ç®—è¡¨è¾¾å¼æ˜¯å¦æ­£ç¡®ï¼Œæˆ–ä½¿ç”¨è®¡ç®—å™¨å·¥å…·ã€‚",
            "technical": f"å…³äºã€Œ{query}ã€è¿™ä¸ªæŠ€æœ¯é—®é¢˜ï¼Œæˆ‘æš‚æ—¶æ²¡æœ‰æ‰¾åˆ°ç›¸å…³èµ„æ–™ã€‚å»ºè®®æ‚¨ï¼š\n\n1. æŸ¥é˜…å®˜æ–¹æ–‡æ¡£\n2. æœç´¢æŠ€æœ¯è®ºå›\n3. å’¨è¯¢æŠ€æœ¯ä¸“å®¶",
            "database": f"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•æŸ¥è¯¢åˆ°ã€Œ{query}ã€ç›¸å…³çš„æ•°æ®åº“ä¿¡æ¯ã€‚è¯·æ£€æŸ¥æŸ¥è¯¢æ¡ä»¶æˆ–è”ç³»ç®¡ç†å‘˜ã€‚",
            "general": f"å…³äºæ‚¨çš„é—®é¢˜ã€Œ{query}ã€ï¼Œæˆ‘æš‚æ—¶æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚å»ºè®®æ‚¨ï¼š\n\n1. å°è¯•é‡æ–°æè¿°é—®é¢˜\n2. ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯\n3. æˆ–è€…è”ç½‘æœç´¢è·å–æœ€æ–°ä¿¡æ¯"
        }
        
        query_type = analysis.query_type if analysis else "general"
        message = fallback_messages.get(query_type, fallback_messages["general"])
        
        # æ·»åŠ åˆ†ææ¨ç†ä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if analysis and analysis.reasoning:
            message += f"\n\n**åˆ†æè¯´æ˜ï¼š** {analysis.reasoning}"
        
        return message
    
    def _synthesize_intelligent_answer(self, query: str, context: str, analysis: QueryAnalysisResult) -> str:
        """æ™ºèƒ½åˆæˆç­”æ¡ˆ - å½“LLMä¸å¯ç”¨æ—¶çš„æ™ºèƒ½å›é€€"""
        try:
            # æ ¹æ®æŸ¥è¯¢ç±»å‹å’Œä¸Šä¸‹æ–‡æ™ºèƒ½ç”Ÿæˆç­”æ¡ˆ
            if analysis.query_type == "calculation":
                # æŸ¥æ‰¾è®¡ç®—ç»“æœ
                calc_pattern = r'\[è®¡ç®—ç»“æœ\]\s*([^=]+)=\s*([^\n]+)'
                calc_match = re.search(calc_pattern, context)
                if calc_match:
                    expression = calc_match.group(1).strip()
                    result = calc_match.group(2).strip()
                    return f"æ ¹æ®è®¡ç®—ï¼Œ{expression} = **{result}**"
                else:
                    return f"å…³äºè®¡ç®—é—®é¢˜ã€Œ{query}ã€ï¼Œæˆ‘æ‰¾åˆ°äº†ç›¸å…³ä¿¡æ¯ï¼š\n\n{context[:300]}..."
            
            elif analysis.query_type == "time":
                # æŸ¥æ‰¾æ—¶é—´ä¿¡æ¯
                time_keywords = ["æ—¶é—´", "æ—¥æœŸ", "ä»Šå¤©", "ç°åœ¨", "å½“å‰"]
                if any(keyword in context for keyword in time_keywords):
                    # æå–æ—¶é—´ç›¸å…³ä¿¡æ¯
                    lines = context.split('\n')
                    time_info = []
                    for line in lines:
                        if any(keyword in line for keyword in time_keywords):
                            time_info.append(line.strip())
                    
                    if time_info:
                        return f"æ ¹æ®æœ€æ–°ä¿¡æ¯ï¼š\n\n" + "\n".join(time_info[:3])
                
                return f"å…³äºæ—¶é—´é—®é¢˜ã€Œ{query}ã€ï¼Œæˆ‘æ‰¾åˆ°äº†ä»¥ä¸‹ä¿¡æ¯ï¼š\n\n{context[:300]}..."
            
            elif analysis.query_type == "database":
                # æŸ¥æ‰¾æ•°æ®åº“ç»“æœ
                if "[æ•°æ®åº“]" in context:
                    db_info = []
                    lines = context.split('\n')
                    for line in lines:
                        if "[æ•°æ®åº“]" in line:
                            db_info.append(line.replace("[æ•°æ®åº“]", "").strip())
                    
                    if db_info:
                        return f"æ•°æ®åº“æŸ¥è¯¢ç»“æœï¼š\n\n" + "\n".join(db_info)
                
                return f"å…³äºæ•°æ®æŸ¥è¯¢ã€Œ{query}ã€ï¼Œæˆ‘æ‰¾åˆ°äº†ä»¥ä¸‹ä¿¡æ¯ï¼š\n\n{context[:300]}..."
            
            else:
                # é€šç”¨æ™ºèƒ½åˆæˆ
                # æå–å…³é”®ä¿¡æ¯ç‰‡æ®µ
                segments = context.split('\n\n')
                key_segments = []
                
                for segment in segments[:5]:  # æœ€å¤šå¤„ç†5ä¸ªæ®µè½
                    segment = segment.strip()
                    if len(segment) > 50:  # è¿‡æ»¤å¤ªçŸ­çš„ç‰‡æ®µ
                        # ç§»é™¤æ ‡è®°ç¬¦å·
                        clean_segment = re.sub(r'\[.*?\]\s*', '', segment)
                        key_segments.append(clean_segment[:200])  # é™åˆ¶é•¿åº¦
                
                if key_segments:
                    return f"å…³äºã€Œ{query}ã€ï¼Œæˆ‘æ•´ç†äº†ä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼š\n\n" + "\n\n".join(f"â€¢ {seg}" for seg in key_segments[:3])
                else:
                    return f"å…³äºã€Œ{query}ã€ï¼Œæˆ‘æ‰¾åˆ°äº†ä¸€äº›ç›¸å…³ä¿¡æ¯ï¼Œä½†å¯èƒ½éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯ï¼š\n\n{context[:400]}..."
        
        except Exception as e:
            self.logger.error(f"æ™ºèƒ½ç­”æ¡ˆåˆæˆå¤±è´¥: {e}")
            # æœ€åŸºæœ¬çš„å›é€€
            return f"å…³äºã€Œ{query}ã€ï¼Œæˆ‘æ‰¾åˆ°äº†ä¸€äº›ä¿¡æ¯ï¼š\n\n{context[:300]}...\n\nä»¥ä¸Šä¿¡æ¯ä¾›æ‚¨å‚è€ƒã€‚"
    
    def _beautify_technical_content(self, content: str) -> str:
        """ä¸“é—¨ç¾åŒ–æŠ€æœ¯å†…å®¹"""
        if not content:
            return content
            
        try:
            
            # æ·»åŠ æŠ€æœ¯æ–‡æ¡£çš„æ ‡å‡†æ ¼å¼
            formatted = content
            
            # ä¸ºä»£ç ç¤ºä¾‹æ·»åŠ è¯­æ³•é«˜äº®æç¤º
            formatted = re.sub(r'```\n(public class|import|package)', r'```java\n\1', formatted)
            formatted = re.sub(r'```\n(def |import |from |class )', r'```python\n\1', formatted)
            formatted = re.sub(r'```\n(<\?xml|<html|<div)', r'```xml\n\1', formatted)
            
            # ç¾åŒ–æŠ€æœ¯æœ¯è¯­
            tech_terms = {
                'JDK': '**JDK (Javaå¼€å‘å·¥å…·åŒ…)**',
                'JRE': '**JRE (Javaè¿è¡Œæ—¶ç¯å¢ƒ)**', 
                'JVM': '**JVM (Javaè™šæ‹Ÿæœº)**',
                'API': '**API**',
                'IDE': '**IDE**',
                'SDK': '**SDK**'
            }
            
            for term, formatted_term in tech_terms.items():
                # åªæ›¿æ¢å•ç‹¬å‡ºç°çš„æœ¯è¯­ï¼Œé¿å…æ›¿æ¢URLæˆ–ä»£ç ä¸­çš„å†…å®¹
                formatted = re.sub(rf'\b{term}\b(?![/:])', formatted_term, formatted)
            
            # æ·»åŠ æŠ€æœ¯è¦ç‚¹çš„è§†è§‰åˆ†éš”
            if '## ä¸»è¦ç‰¹æ€§' in formatted or '## åŸºæœ¬ç‰¹æ€§' in formatted:
                formatted = f"ğŸ“‹ **æŠ€æœ¯æ–‡æ¡£**\n\n{formatted}"
            
            return formatted
            
        except Exception as e:
            self.logger.error(f"ç¾åŒ–æŠ€æœ¯å†…å®¹æ—¶å‡ºé”™: {e}")
            return content
    
    def debug_mcp_search_result(self, mcp_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        è°ƒè¯•MCPæœç´¢ç»“æœï¼Œæä¾›è¯¦ç»†çš„æ ¼å¼åˆ†æ
        
        Args:
            mcp_result: MCPå·¥å…·è¿”å›çš„åŸå§‹ç»“æœ
            
        Returns:
            Dict: åŒ…å«è°ƒè¯•ä¿¡æ¯çš„å­—å…¸
        """
        debug_info = {
            "original_type": type(mcp_result),
            "success": mcp_result.get("success", False),
            "has_result": "result" in mcp_result,
            "result_type": type(mcp_result.get("result")) if "result" in mcp_result else None,
            "error": mcp_result.get("error"),
            "analysis": []
        }
        
        if "result" in mcp_result:
            result_data = mcp_result["result"]
            
            if isinstance(result_data, dict):
                debug_info["analysis"].append("ç»“æœæ˜¯å­—å…¸ç±»å‹")
                debug_info["dict_keys"] = list(result_data.keys())
                
                if "results" in result_data:
                    debug_info["analysis"].append(f"åŒ…å«'results'é”®ï¼Œç±»å‹: {type(result_data['results'])}")
                    if isinstance(result_data["results"], list):
                        debug_info["analysis"].append(f"resultsæ˜¯åˆ—è¡¨ï¼Œé•¿åº¦: {len(result_data['results'])}")
                        if result_data["results"]:
                            first_item = result_data["results"][0]
                            debug_info["analysis"].append(f"ç¬¬ä¸€ä¸ªç»“æœé¡¹ç±»å‹: {type(first_item)}")
                            if isinstance(first_item, dict):
                                debug_info["first_item_keys"] = list(first_item.keys())
                
                elif any(key in result_data for key in ["title", "content", "url", "snippet"]):
                    debug_info["analysis"].append("çœ‹èµ·æ¥æ˜¯å•ä¸ªæœç´¢ç»“æœæ ¼å¼")
                    debug_info["has_search_fields"] = [key for key in ["title", "content", "url", "snippet"] if key in result_data]
                
                else:
                    debug_info["analysis"].append("å­—å…¸æ ¼å¼æœªçŸ¥ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†")
            
            elif isinstance(result_data, list):
                debug_info["analysis"].append(f"ç»“æœæ˜¯åˆ—è¡¨ç±»å‹ï¼Œé•¿åº¦: {len(result_data)}")
                if result_data:
                    first_item = result_data[0]
                    debug_info["analysis"].append(f"ç¬¬ä¸€ä¸ªåˆ—è¡¨é¡¹ç±»å‹: {type(first_item)}")
                    if isinstance(first_item, dict):
                        debug_info["first_item_keys"] = list(first_item.keys())
            
            elif isinstance(result_data, str):
                debug_info["analysis"].append(f"ç»“æœæ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œé•¿åº¦: {len(result_data)}")
                debug_info["string_preview"] = result_data[:100]
            
            else:
                debug_info["analysis"].append(f"ç»“æœæ˜¯å…¶ä»–ç±»å‹: {type(result_data)}")
        
        return debug_info
    
    def _safe_create_search_result(self, 
                                  title: Any = "", 
                                  content: Any = "", 
                                  url: Any = "", 
                                  source: str = "Unknown",
                                  relevance_score: float = 0.8,
                                  channel_type: Any = "MCP_SEARCH") -> SearchResult:
        """
        å®‰å…¨åˆ›å»ºSearchResultå¯¹è±¡ï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µç±»å‹æ­£ç¡®
        
        Args:
            title: æ ‡é¢˜ï¼ˆä»»æ„ç±»å‹ï¼Œä¼šè½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼‰
            content: å†…å®¹ï¼ˆä»»æ„ç±»å‹ï¼Œä¼šè½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼‰
            url: URLï¼ˆä»»æ„ç±»å‹ï¼Œä¼šè½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼‰
            source: æ¥æº
            relevance_score: ç›¸å…³æ€§åˆ†æ•°
            channel_type: é€šé“ç±»å‹ï¼ˆå­—ç¬¦ä¸²æˆ–ChannelTypeï¼‰
            
        Returns:
            SearchResult: å®‰å…¨åˆ›å»ºçš„æœç´¢ç»“æœå¯¹è±¡
        """
        try:
            # å®‰å…¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            safe_title = str(title) if title is not None else ""
            safe_content = str(content) if content is not None else ""
            safe_url = str(url) if url is not None else ""
            
            # å¤„ç†åˆ—è¡¨ç±»å‹çš„ç‰¹æ®Šæƒ…å†µ
            if isinstance(title, list):
                safe_title = " ".join(str(item) for item in title)
            if isinstance(content, list):
                safe_content = " ".join(str(item) for item in content)
            if isinstance(url, list):
                safe_url = str(url[0]) if url else ""
            
            # æ¸…ç†å’Œæˆªæ–­é•¿åº¦
            safe_title = safe_title.strip()[:200]  # é™åˆ¶æ ‡é¢˜é•¿åº¦
            safe_content = safe_content.strip()[:2000]  # é™åˆ¶å†…å®¹é•¿åº¦
            safe_url = safe_url.strip()[:500]  # é™åˆ¶URLé•¿åº¦
            
            # ç¡®ä¿ç›¸å…³æ€§åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…
            safe_relevance_score = max(0.0, min(1.0, float(relevance_score) if relevance_score else 0.8))
            
            # å¤„ç†ChannelType
            if isinstance(channel_type, ChannelType):
                safe_channel_type = channel_type
            elif isinstance(channel_type, str):
                # å°è¯•å°†å­—ç¬¦ä¸²æ˜ å°„åˆ°ChannelType
                channel_mapping = {
                    "MCP_SEARCH": ChannelType.REAL_TIME_WEB,
                    "GOOGLE_SEARCH": ChannelType.SEARCH_ENGINE,
                    "LOCAL_KNOWLEDGE": ChannelType.LOCAL_KNOWLEDGE,
                    "NEWS": ChannelType.NEWS_FEED,
                    "SOCIAL": ChannelType.SOCIAL_MEDIA
                }
                safe_channel_type = channel_mapping.get(channel_type, ChannelType.REAL_TIME_WEB)
            else:
                safe_channel_type = ChannelType.REAL_TIME_WEB
            
            return SearchResult(
                title=safe_title,
                content=safe_content,
                url=safe_url,
                source=source,
                timestamp=time.time(),
                relevance_score=safe_relevance_score,
                channel_type=safe_channel_type
            )
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ›å»ºSearchResultæ—¶å‡ºé”™: {e}")
            # è¿”å›ä¸€ä¸ªåŸºæœ¬çš„SearchResultå¯¹è±¡
            return SearchResult(
                title="æœç´¢ç»“æœ",
                content=f"å¤„ç†æœç´¢ç»“æœæ—¶å‡ºé”™: {str(e)}",
                url="",
                source=source,
                timestamp=time.time(),
                relevance_score=0.5,
                channel_type=ChannelType.REAL_TIME_WEB
            )
