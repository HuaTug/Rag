#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¢å¼ºçš„RAGå¤„ç†å™¨

é›†æˆå¤šé€šé“æœç´¢ã€å‘é‡å­˜å‚¨å’ŒLLMç”Ÿæˆï¼Œæä¾›å®Œæ•´çš„RAGè§£å†³æ–¹æ¡ˆã€‚
"""

import asyncio
import logging
import time
import sys
import os
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import json

# æ·»åŠ ä¸Šçº§ç›®å½•åˆ°è·¯å¾„ä»¥ä¾¿å¯¼å…¥æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥MCPæ¡†æ¶
from channel_framework import MCPProcessor, QueryContext, QueryAnalyzer, QueryType, SearchResult

# å¯¼å…¥coreæ¨¡å—
sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'core'))
from search_channels import GoogleSearchChannel
from dynamic_vector_store import DynamicVectorStore, VectorStoreManager

# å¯¼å…¥MCPç›®å½•ä¸‹çš„æ™ºèƒ½æŸ¥è¯¢åˆ†æå™¨
sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'mcp'))
from smart_query_analyzer import SmartQueryAnalyzer, QueryAnalysisResult, SimpleCalculator

try:
    from ask_llm import get_llm_answer_deepseek
    from ..core.encoder import emb_text
    from ..core.milvus_utils import get_milvus_client
except ImportError as e:
    print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥æŸäº›æ¨¡å—: {e}")
    # å®šä¹‰mockå‡½æ•°ï¼ŒåŒ¹é…çœŸå®å‡½æ•°ç­¾å
    def get_llm_answer_deepseek(client, context: str, question: str, model: str = "deepseek-v3-0324", min_distance_threshold: float = 0.5) -> str:
        return f"æ¨¡æ‹ŸLLMå“åº” - é—®é¢˜: {question}"
    
    def emb_text(text: str):
        # è¿”å›æ¨¡æ‹Ÿå‘é‡
        import random
        return [random.random() for _ in range(384)]
    
    def get_milvus_client():
        return None


@dataclass
class RAGResponse:
    """RAGå“åº”æ•°æ®ç»“æ„"""
    answer: str
    sources: List[Dict[str, Any]]
    search_results: List[SearchResult]
    processing_time: float
    confidence_score: float
    metadata: Dict[str, Any]
    analysis_result: Optional[QueryAnalysisResult] = None  # æ–°å¢åˆ†æç»“æœ


class EnhancedRAGProcessor:
    """å¢å¼ºçš„RAGå¤„ç†å™¨"""
    
    def __init__(self, vector_store=None, search_channels=None, llm_client=None, config: Dict[str, Any] = None):
        self.config = config or {}
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # å­˜å‚¨å¤–éƒ¨ä¼ å…¥çš„ç»„ä»¶
        self.vector_store = vector_store
        self.search_channels = search_channels or []
        self.llm_client = llm_client
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.mcp_processor = MCPProcessor()
        self.vector_store_manager = VectorStoreManager()
        self.query_analyzer = QueryAnalyzer()
        
        # æ–°å¢æ™ºèƒ½æŸ¥è¯¢åˆ†æå™¨
        self.smart_analyzer = SmartQueryAnalyzer(self.config)
        self.calculator = SimpleCalculator()
        
        # æ™ºèƒ½æŸ¥è¯¢ç­–ç•¥é…ç½®
        self.similarity_threshold = self.config.get("similarity_threshold", 0.5)  # ç›¸ä¼¼åº¦é˜ˆå€¼
        self.min_vector_results = self.config.get("min_vector_results", 3)  # æœ€å°‘å‘é‡ç»“æœæ•°é‡
        self.enable_smart_search = self.config.get("enable_smart_search", True)  # å¯ç”¨æ™ºèƒ½æœç´¢
        
        # è¾“å‡ºé…ç½®ä¿¡æ¯ç”¨äºè°ƒè¯•
        self.logger.info(f"ğŸ“Š æ™ºèƒ½æœç´¢é…ç½®: similarity_threshold={self.similarity_threshold}, "
                        f"min_vector_results={self.min_vector_results}, "
                        f"enable_smart_search={self.enable_smart_search}")
        
        # åˆå§‹åŒ–é…ç½®
        self._init_components()
    
    def _init_components(self):
        """åˆå§‹åŒ–å„ä¸ªç»„ä»¶"""
        try:
            # 1. åˆå§‹åŒ–å‘é‡å­˜å‚¨
            self._init_vector_stores()
            
            # 2. åˆå§‹åŒ–æœç´¢é€šé“
            self._init_search_channels()
            
            self.logger.info("å¢å¼ºRAGå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _init_vector_stores(self):
        """åˆå§‹åŒ–å‘é‡å­˜å‚¨"""
        # è·å–Milvusé…ç½®ï¼Œä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„é…ç½®
        milvus_endpoint = (
            self.config.get("milvus_endpoint") or 
            self.config.get("endpoint") or 
            "./milvus_rag.db"
        )
        milvus_token = (
            self.config.get("milvus_token") or 
            self.config.get("token")
        )
        vector_dim = (
            self.config.get("vector_dim") or 
            self.config.get("dimension") or 
            384
        )
        
        # åŠ¨æ€å‘é‡å­˜å‚¨ï¼ˆç”¨äºå®æ—¶æœç´¢ç»“æœï¼‰
        dynamic_store = DynamicVectorStore(
            milvus_endpoint=milvus_endpoint,
            milvus_token=milvus_token,
            collection_name="dynamic_search_results",
            vector_dim=vector_dim
        )
        self.vector_store_manager.add_store("dynamic", dynamic_store)
        
        # æœ¬åœ°çŸ¥è¯†åº“å­˜å‚¨
        local_store = DynamicVectorStore(
            milvus_endpoint=milvus_endpoint,
            milvus_token=milvus_token,
            collection_name="local_knowledge",
            vector_dim=vector_dim
        )
        self.vector_store_manager.add_store("local", local_store)
    
    def _init_search_channels(self):
        """åˆå§‹åŒ–æœç´¢é€šé“"""
        # æ³¨å†Œå¤–éƒ¨ä¼ å…¥çš„æœç´¢é€šé“
        for channel in self.search_channels:
            self.mcp_processor.register_channel(channel)
        
        # Googleæœç´¢é€šé“ï¼ˆå¦‚æœé…ç½®ä¸­å¯ç”¨ä¸”æ²¡æœ‰å¤–éƒ¨ä¼ å…¥ï¼‰
        if (self.config.get("enable_search_engine", True) and 
            not any(hasattr(ch, 'channel_type') and 
                   str(ch.channel_type).endswith('SEARCH_ENGINE') 
                   for ch in self.search_channels)):
            
            search_config = {
                "api_key": self.config.get("google_api_key"),
                "search_engine_id": self.config.get("google_search_engine_id"),
                "timeout": self.config.get("search_timeout", 10),
                "priority": {"factual": 1, "analytical": 2, "creative": 5, "conversational": 3}
            }
            
            if search_config["api_key"] and search_config["search_engine_id"]:
                search_channel = GoogleSearchChannel(search_config)
                self.mcp_processor.register_channel(search_channel)
        
        # æœ¬åœ°çŸ¥è¯†åº“é€šé“ï¼ˆæš‚æ—¶ç¦ç”¨ï¼Œå› ä¸ºLocalKnowledgeChannelç±»ä¸å­˜åœ¨ï¼‰
        if False:  # self.config.get("enable_local_knowledge", True):
            local_store = self.vector_store_manager.get_store("local")
            if local_store:
                local_config = {
                    "milvus_client": local_store.client,
                    "collection_name": "local_knowledge",
                    "encoder": emb_text,
                    "priority": {"factual": 2, "analytical": 1, "creative": 4, "conversational": 2}
                }
                # local_channel = LocalKnowledgeChannel(local_config)
                # self.mcp_processor.register_channel(local_channel)
        
        # æ–°é—»é€šé“ï¼ˆæš‚æ—¶ç¦ç”¨ï¼Œå› ä¸ºNewsChannelç±»ä¸å­˜åœ¨ï¼‰
        if False:  # self.config.get("enable_news", False):
            news_config = {
                "news_api_key": self.config.get("news_api_key"),
                "priority": {"factual": 3, "analytical": 3, "creative": 6, "conversational": 4}
            }
            # news_channel = NewsChannel(news_config)
            # self.mcp_processor.register_channel(news_channel)
    
    async def process_query(self, context: QueryContext) -> RAGResponse:
        """
        æ™ºèƒ½å¤„ç†æŸ¥è¯¢è¯·æ±‚ - é›†æˆGo demoçš„åˆ†æèƒ½åŠ›
        
        æµç¨‹ï¼š
        1. æ™ºèƒ½åˆ†ææŸ¥è¯¢æ„å›¾
        2. æ ¹æ®åˆ†æç»“æœé€‰æ‹©æœ€ä¼˜ç­–ç•¥
        3. æ‰§è¡Œç›¸åº”çš„å·¥å…·è°ƒç”¨
        4. ç”Ÿæˆç»¼åˆå›ç­”
        """
        start_time = time.time()
        query = context.query
        
        try:
            self.logger.info(f"ğŸ¤– å¼€å§‹æ™ºèƒ½å¤„ç†æŸ¥è¯¢: {query}")
            
            # 1. æ™ºèƒ½æŸ¥è¯¢åˆ†æ - æ ¸å¿ƒæ”¹è¿›
            analysis_result = await self.smart_analyzer.analyze_query_intent(query)
            self.logger.info(f"ğŸ§  æŸ¥è¯¢åˆ†æå®Œæˆ: {analysis_result.query_type} "
                           f"(ç½®ä¿¡åº¦: {analysis_result.confidence:.2f})")
            
            # 2. æ ¹æ®åˆ†æç»“æœæ‰§è¡Œç›¸åº”ç­–ç•¥
            search_results = []
            vector_results = []
            calculation_results = []
            database_results = []
            
            # è®¡ç®—å¤„ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if analysis_result.needs_calculation:
                self.logger.info("ğŸ§® æ‰§è¡Œæ•°å­¦è®¡ç®—...")
                calc_result = self.calculator.calculate(analysis_result.calculation_args)
                calculation_results.append(calc_result)
            
            # å‘é‡æœç´¢ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if analysis_result.needs_vector_search:
                self.logger.info("ğŸ” æ‰§è¡Œå‘é‡æœç´¢...")
                vector_results = await self._perform_vector_search(query, context.max_results)
                
                # åŠ¨æ€æœç´¢ç­–ç•¥ï¼šæ£€æŸ¥å‘é‡æœç´¢ç»“æœè´¨é‡
                if analysis_result.enable_dynamic_search and vector_results:
                    max_similarity = max((result.get("similarity_score", 0) for result in vector_results), default=0)
                    self.logger.info(f"ğŸ“Š å‘é‡æœç´¢æœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.3f}")
                    
                    if max_similarity < analysis_result.min_similarity_threshold:
                        self.logger.warning(f"âš ï¸ å‘é‡æœç´¢ç›¸ä¼¼åº¦è¿‡ä½ ({max_similarity:.3f} < {analysis_result.min_similarity_threshold})ï¼Œå¯ç”¨ç½‘ç»œæœç´¢")
                        analysis_result.needs_web_search = True
                        analysis_result.web_search_query = query
                        analysis_result.reasoning += f" - å‘é‡æœç´¢ç›¸ä¼¼åº¦è¿‡ä½({max_similarity:.3f})ï¼Œå¯ç”¨ç½‘ç»œæœç´¢"
                elif analysis_result.enable_dynamic_search and not vector_results:
                    self.logger.warning("âš ï¸ å‘é‡æœç´¢æ— ç»“æœï¼Œå¯ç”¨ç½‘ç»œæœç´¢")
                    analysis_result.needs_web_search = True
                    analysis_result.web_search_query = query
                    analysis_result.reasoning += " - å‘é‡æœç´¢æ— ç»“æœï¼Œå¯ç”¨ç½‘ç»œæœç´¢"

            # ç½‘ç»œæœç´¢ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if analysis_result.needs_web_search:
                self.logger.info(f"ğŸŒ æ‰§è¡Œç½‘ç»œæœç´¢: {analysis_result.web_search_query}")
                search_context = QueryContext(
                    query=analysis_result.web_search_query,
                    query_type=context.query_type,
                    max_results=context.max_results,
                    timeout=context.timeout
                )
                search_results = await self._perform_search(search_context)
                
                # å­˜å‚¨æ–°çš„æœç´¢ç»“æœåˆ°å‘é‡æ•°æ®åº“
                if search_results:
                    await self._store_search_results(search_results)
                    self.logger.info(f"ğŸ’¾ å­˜å‚¨äº† {len(search_results)} ä¸ªæ–°çš„æœç´¢ç»“æœ")
            
            # æ•°æ®åº“æŸ¥è¯¢ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if analysis_result.needs_database:
                self.logger.info("ğŸ—„ï¸ æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢...")
                # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„æ•°æ®åº“æŸ¥è¯¢åŠŸèƒ½
                database_results = await self._perform_database_query(analysis_result.database_query)
            
            # 3. èåˆæ‰€æœ‰ç»“æœ
            all_results = self._merge_all_results(
                search_results, vector_results, calculation_results, database_results
            )
            
            # 4. ç”Ÿæˆæ™ºèƒ½ç­”æ¡ˆ
            answer, confidence = await self._generate_smart_answer(
                query, all_results, analysis_result
            )
            
            # 5. æ„å»ºå¢å¼ºå“åº”
            processing_time = time.time() - start_time
            response = RAGResponse(
                answer=answer,
                sources=self._extract_sources(all_results),
                search_results=search_results,
                processing_time=processing_time,
                confidence_score=confidence,
                analysis_result=analysis_result,  # æ–°å¢åˆ†æç»“æœ
                metadata={
                    "query_type": context.query_type.value,
                    "analysis_type": analysis_result.query_type,
                    "analysis_confidence": analysis_result.confidence,
                    "total_results": len(all_results),
                    "search_results_count": len(search_results),
                    "vector_results_count": len(vector_results),
                    "calculation_results_count": len(calculation_results),
                    "database_results_count": len(database_results),
                    "used_search_engine": analysis_result.needs_web_search,
                    "used_vector_search": analysis_result.needs_vector_search,
                    "used_calculation": analysis_result.needs_calculation,
                    "used_database": analysis_result.needs_database,
                    "analysis_reasoning": analysis_result.reasoning,
                    "tools_used": [tool.name for tool in analysis_result.tool_calls],
                    "strategy": self.smart_analyzer.get_search_strategy(analysis_result)
                }
            )
            
            self.logger.info(f"âœ… æ™ºèƒ½æŸ¥è¯¢å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}s, "
                           f"ç­–ç•¥: {response.metadata['strategy']}")
            return response
            
        except Exception as e:
            self.logger.error(f"æ™ºèƒ½æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            return RAGResponse(
                answer=f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æŸ¥è¯¢æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                sources=[],
                search_results=[],
                processing_time=time.time() - start_time,
                confidence_score=0.0,
                metadata={"error": str(e), "fallback": True}
            )
    
    def _should_perform_search(self, vector_results: List[Dict[str, Any]], context: QueryContext) -> Tuple[bool, str]:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦æ‰§è¡Œæœç´¢å¼•æ“æŸ¥è¯¢
        
        Returns:
            Tuple[bool, str]: (æ˜¯å¦éœ€è¦æœç´¢, åŸå› è¯´æ˜)
        """
        if not self.enable_smart_search:
            return True, "æ™ºèƒ½æœç´¢å·²ç¦ç”¨"
        
        if not vector_results:
            return True, "å‘é‡æ•°æ®åº“ä¸­æ²¡æœ‰ç›¸å…³ç»“æœ"
        
        # æ£€æŸ¥ç»“æœæ•°é‡
        if len(vector_results) < self.min_vector_results:
            return True, f"å‘é‡ç»“æœæ•°é‡ä¸è¶³ ({len(vector_results)} < {self.min_vector_results})"
        
        # æ£€æŸ¥æœ€é«˜ç›¸ä¼¼åº¦
        max_similarity = max(result.get("similarity_score", 0) for result in vector_results)
        if max_similarity < self.similarity_threshold:
            return True, f"æœ€é«˜ç›¸ä¼¼åº¦ä¸è¶³ ({max_similarity:.3f} < {self.similarity_threshold})"
        
        # æ£€æŸ¥é«˜è´¨é‡ç»“æœæ•°é‡
        high_quality_results = [
            r for r in vector_results 
            if r.get("similarity_score", 0) >= self.similarity_threshold
        ]
        
        if len(high_quality_results) < 2:
            return True, f"é«˜è´¨é‡ç»“æœæ•°é‡ä¸è¶³ ({len(high_quality_results)} < 2)"
        
        # æ£€æŸ¥å†…å®¹æ–°é²œåº¦ï¼ˆå¯é€‰ï¼‰
        current_time = time.time()
        recent_results = [
            r for r in high_quality_results
            if current_time - r.get("timestamp", 0) < 7 * 24 * 3600  # 7å¤©å†…
        ]
        
        if len(recent_results) == 0:
            return True, "æ²¡æœ‰è¶³å¤Ÿæ–°é²œçš„é«˜è´¨é‡ç»“æœ"
        
        # ç‰¹æ®ŠæŸ¥è¯¢ç±»å‹å¤„ç†
        if context.query_type == QueryType.CREATIVE:
            return True, "åˆ›é€ æ€§æŸ¥è¯¢éœ€è¦å®æ—¶æœç´¢"
        
        # æ£€æŸ¥æŸ¥è¯¢ä¸­æ˜¯å¦åŒ…å«æ—¶é—´ç›¸å…³è¯æ±‡
        time_keywords = ["ä»Šå¤©", "æœ€æ–°", "ç°åœ¨", "å½“å‰", "æœ€è¿‘", "ä»Šå¹´", "2024", "2025"]
        if any(keyword in context.query for keyword in time_keywords):
            return True, "æŸ¥è¯¢åŒ…å«æ—¶é—´ç›¸å…³è¯æ±‡ï¼Œéœ€è¦æœ€æ–°ä¿¡æ¯"
        
        return False, f"å‘é‡æ•°æ®åº“æœ‰è¶³å¤Ÿçš„é«˜è´¨é‡ç»“æœ ({len(high_quality_results)} ä¸ªï¼Œæœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.3f})"
    
    async def _perform_search(self, context: QueryContext) -> List[SearchResult]:
        """æ‰§è¡Œå®æ—¶æœç´¢"""
        try:
            return await self.mcp_processor.process_query(context)
        except Exception as e:
            self.logger.error(f"å®æ—¶æœç´¢å¤±è´¥: {e}")
            return []
    
    async def _perform_vector_search(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """æ‰§è¡Œå‘é‡æœç´¢"""
        try:
            all_results = await self.vector_store_manager.search_all_stores(query, limit)
            
            # åˆå¹¶æ‰€æœ‰å­˜å‚¨çš„ç»“æœ
            merged_results = []
            for store_name, results in all_results.items():
                for result in results:
                    result["store_name"] = store_name
                    merged_results.append(result)
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            merged_results.sort(key=lambda x: x.get("similarity_score", 0), reverse=True)
            
            return merged_results[:limit]
            
        except Exception as e:
            self.logger.error(f"å‘é‡æœç´¢å¤±è´¥: {e}")
            return []
    
    async def _store_search_results(self, search_results: List[SearchResult]):
        """å­˜å‚¨æœç´¢ç»“æœåˆ°å‘é‡æ•°æ®åº“"""
        try:
            dynamic_store = self.vector_store_manager.get_store("dynamic")
            if dynamic_store:
                stored_count = await dynamic_store.store_search_results(search_results)
                self.logger.info(f"å­˜å‚¨äº† {stored_count} ä¸ªæœç´¢ç»“æœ")
        except Exception as e:
            self.logger.error(f"å­˜å‚¨æœç´¢ç»“æœå¤±è´¥: {e}")
    
    def _merge_results(self, 
                      search_results: List[SearchResult], 
                      vector_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """èåˆæœç´¢ç»“æœå’Œå‘é‡ç»“æœ"""
        merged = []
        
        # æ·»åŠ å®æ—¶æœç´¢ç»“æœï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰
        for result in search_results:
            merged.append({
                "content": result.content,
                "title": result.title,
                "url": result.url,
                "source": result.source,
                "score": result.relevance_score + 0.1,  # ç»™æ–°æœç´¢ç»“æœåŠ æƒ
                "type": "search",
                "timestamp": result.timestamp
            })
        
        # æ·»åŠ å‘é‡æœç´¢ç»“æœ
        for result in vector_results:
            merged.append({
                "content": result.get("content", ""),
                "title": result.get("title", ""),
                "url": result.get("url", ""),
                "source": result.get("source", ""),
                "score": result.get("similarity_score", 0),
                "type": "vector",
                "store_name": result.get("store_name", ""),
                "timestamp": result.get("timestamp", 0)
            })
        
        # å»é‡å’Œæ’åº
        seen_urls = set()
        deduplicated = []
        
        for result in merged:
            url = result.get("url", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                deduplicated.append(result)
            elif not url:  # æ²¡æœ‰URLçš„ç»“æœä¹Ÿä¿ç•™
                deduplicated.append(result)
        
        # æŒ‰åˆ†æ•°æ’åº
        deduplicated.sort(key=lambda x: x.get("score", 0), reverse=True)
        
        return deduplicated
    
    async def _generate_answer(self, 
                              query: str, 
                              results: List[Dict[str, Any]]) -> Tuple[str, float]:
        """ç”Ÿæˆç­”æ¡ˆ"""
        try:
            if not results:
                return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚", 0.0
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            for i, result in enumerate(results[:5]):  # ä½¿ç”¨å‰5ä¸ªæœ€ç›¸å…³çš„ç»“æœ
                content = result.get("content", "").strip()
                if content:
                    source_info = f"æ¥æº: {result.get('title', 'æœªçŸ¥')} ({result.get('source', 'æœªçŸ¥')})"
                    context_parts.append(f"å‚è€ƒèµ„æ–™ {i+1}:\n{content}\n{source_info}")
            
            context = "\n\n".join(context_parts)
            
            # è°ƒç”¨DeepSeek LLMç”Ÿæˆç­”æ¡ˆ
            try:
                # ä½¿ç”¨ä¼ å…¥çš„LLMå®¢æˆ·ç«¯
                if hasattr(self, 'llm_client') and self.llm_client:
                    # æ„å»ºæ¶ˆæ¯æ ¼å¼
                    messages = []
                    
                    if context:
                        system_prompt = """
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯å……åˆ†ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯å›ç­”ã€‚
å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸å¤Ÿå……åˆ†ï¼Œå¯ä»¥ç»“åˆä½ çš„çŸ¥è¯†ç»™å‡ºæœ‰å¸®åŠ©çš„å›ç­”ã€‚
è¯·ç¡®ä¿å›ç­”å‡†ç¡®ã€æœ‰æ¡ç†ï¼Œå¹¶å°½å¯èƒ½æä¾›å…·ä½“çš„ä¿¡æ¯ã€‚
è¯·æä¾›å®Œæ•´è¯¦ç»†çš„å›ç­”ï¼Œä¸è¦æˆªæ–­å†…å®¹ã€‚
"""
                        messages.append({"role": "system", "content": system_prompt})
                        
                        user_content = f"""
åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

{context}

é—®é¢˜ï¼š{query}
"""
                    else:
                        user_content = query
                    
                    messages.append({"role": "user", "content": user_content})
                    
                    # è°ƒç”¨DeepSeek API - æ·»åŠ é‡è¦å‚æ•°
                    response = self.llm_client.chat_completions_create(
                        model="deepseek-v3-0324",
                        messages=messages,
                        stream=False,
                        enable_search=True,  # å¯ç”¨DeepSeekçš„æœç´¢åŠŸèƒ½
                        temperature=0.7,     # è®¾ç½®åˆ›é€ æ€§å‚æ•°
                        top_p=0.9,          # è®¾ç½®æ ¸é‡‡æ ·å‚æ•°
                        frequency_penalty=0.0,  # é¢‘ç‡æƒ©ç½š
                        presence_penalty=0.0    # å­˜åœ¨æƒ©ç½š
                    )
                    
                    if response and "choices" in response:
                        answer = response["choices"][0]["message"]["content"]
                    else:
                        answer = "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆå›ç­”"
                        
                else:
                    # é™çº§åˆ°ç®€å•çš„ä¸Šä¸‹æ–‡æ‹¼æ¥
                    if context:
                        answer = f"åŸºäºæœç´¢ç»“æœï¼š\n\n{context}\n\nå›ç­”ï¼šè¯·å‚è€ƒä»¥ä¸Šä¿¡æ¯æ¥å›ç­”å…³äº'{query}'çš„é—®é¢˜ã€‚"
                    else:
                        answer = f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°å…³äº'{query}'çš„ç›¸å…³ä¿¡æ¯ã€‚"
                        
            except Exception as e:
                self.logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
                # é™çº§æ–¹æ¡ˆ
                if context:
                    answer = f"åŸºäºæœç´¢åˆ°çš„ä¿¡æ¯ï¼š\n\n{context}"
                else:
                    answer = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°äº†é”™è¯¯ï¼š{str(e)}"
            
            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºç»“æœæ•°é‡å’Œç›¸å…³æ€§ï¼‰
            confidence = min(1.0, len(results) * 0.1 + sum(r.get("score", 0) for r in results[:3]) / 3)
            
            return answer, confidence
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {e}")
            return f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯: {str(e)}", 0.0
    
    def _extract_sources(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æå–æ¥æºä¿¡æ¯"""
        sources = []
        for result in results[:10]:  # æœ€å¤šè¿”å›10ä¸ªæ¥æº
            source = {
                "title": result.get("title", "æœªçŸ¥æ ‡é¢˜"),
                "url": result.get("url", ""),
                "source": result.get("source", "æœªçŸ¥æ¥æº"),
                "score": result.get("score", 0),
                "type": result.get("type", "unknown")
            }
            sources.append(source)
        
        return sources
    
    async def cleanup_old_data(self, max_age_hours: int = 24):
        """æ¸…ç†è¿‡æœŸæ•°æ®"""
        try:
            dynamic_store = self.vector_store_manager.get_store("dynamic")
            if dynamic_store:
                cleaned_count = await dynamic_store.cleanup_old_documents(max_age_hours)
                self.logger.info(f"æ¸…ç†äº† {cleaned_count} ä¸ªè¿‡æœŸæ–‡æ¡£")
                return cleaned_count
        except Exception as e:
            self.logger.error(f"æ¸…ç†è¿‡æœŸæ•°æ®å¤±è´¥: {e}")
        return 0
    
    def update_smart_search_config(self, 
                                  similarity_threshold: float = None,
                                  min_vector_results: int = None,
                                  enable_smart_search: bool = None):
        """æ›´æ–°æ™ºèƒ½æœç´¢é…ç½®"""
        if similarity_threshold is not None:
            self.similarity_threshold = similarity_threshold
            self.logger.info(f"æ›´æ–°ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}")
        
        if min_vector_results is not None:
            self.min_vector_results = min_vector_results
            self.logger.info(f"æ›´æ–°æœ€å°‘å‘é‡ç»“æœæ•°é‡: {min_vector_results}")
        
        if enable_smart_search is not None:
            self.enable_smart_search = enable_smart_search
            self.logger.info(f"æ™ºèƒ½æœç´¢å¼€å…³: {enable_smart_search}")
    
    def get_smart_search_stats(self) -> Dict[str, Any]:
        """è·å–æ™ºèƒ½æœç´¢ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "similarity_threshold": self.similarity_threshold,
            "min_vector_results": self.min_vector_results,
            "enable_smart_search": self.enable_smart_search,
            "vector_stores": list(self.vector_store_manager.stores.keys())
        }
    
    async def _perform_database_query(self, query_args: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        try:
            # è¿™é‡Œåº”è¯¥é›†æˆå®é™…çš„æ•°æ®åº“æŸ¥è¯¢åŠŸèƒ½
            # ç›®å‰æä¾›æ¨¡æ‹Ÿæ•°æ®
            query_type = query_args.get("query_type", "select")
            
            if query_type == "count":
                return [{
                    "type": "database_result",
                    "query": f"ç»Ÿè®¡æŸ¥è¯¢: {query_args}",
                    "result": "æ´»è·ƒç”¨æˆ·: 1250, éæ´»è·ƒç”¨æˆ·: 350",
                    "source": "ç”¨æˆ·æ•°æ®åº“",
                    "timestamp": time.time()
                }]
            
            elif query_type == "select":
                return [{
                    "type": "database_result",
                    "query": f"æŸ¥è¯¢: {query_args}",
                    "result": "è¿”å›äº†5æ¡ç”¨æˆ·è®°å½•",
                    "source": "ç”¨æˆ·æ•°æ®åº“",
                    "timestamp": time.time()
                }]
            
            return []
            
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
            return []


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
    def _merge_all_results(self, 
                          search_results: List[SearchResult], 
                          vector_results: List[Dict[str, Any]],
                          calculation_results: List[Dict[str, Any]],
                          database_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """èåˆæ‰€æœ‰ç±»å‹çš„ç»“æœ"""
        all_results = []
        
        # æœç´¢ç»“æœ
        for result in search_results:
            all_results.append({
                "type": "search",
                "title": result.title,
                "content": result.content,
                "url": result.url,
                "source": result.source,
                "timestamp": result.timestamp,
                "relevance_score": result.relevance_score,
                "channel_type": str(result.channel_type)
            })
        
        # å‘é‡æœç´¢ç»“æœ
        for result in vector_results:
            result["type"] = "vector"
            all_results.append(result)
        
        # è®¡ç®—ç»“æœ
        for result in calculation_results:
            result["type"] = "calculation"
            result["source"] = "è®¡ç®—å™¨"
            result["timestamp"] = time.time()
            all_results.append(result)
        
        # æ•°æ®åº“ç»“æœ
        for result in database_results:
            all_results.append(result)
        
        # æŒ‰ç›¸å…³æ€§æ’åº
        all_results.sort(key=lambda x: x.get("relevance_score", x.get("similarity_score", 0.5)), reverse=True)
        
        return all_results


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
    async def _generate_smart_answer(self, 
                                    query: str, 
                                    results: List[Dict[str, Any]],
                                    analysis: QueryAnalysisResult) -> Tuple[str, float]:
        """ç”Ÿæˆæ™ºèƒ½ç­”æ¡ˆ - åŸºäºæŸ¥è¯¢åˆ†æç»“æœ"""
        
        if not results:
            return self._generate_fallback_answer(query, analysis), 0.3
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        confidence_factors = []
        
        for result in results[:8]:  # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
            result_type = result.get("type", "unknown")
            
            if result_type == "calculation":
                if "result" in result:
                    context_parts.append(f"[è®¡ç®—ç»“æœ] {result.get('expression', '')} = {result['result']}")
                    confidence_factors.append(0.9)  # è®¡ç®—ç»“æœç½®ä¿¡åº¦é«˜
                elif "error" in result:
                    context_parts.append(f"[è®¡ç®—é”™è¯¯] {result['error']}")
                    confidence_factors.append(0.2)
            
            elif result_type == "database_result":
                context_parts.append(f"[æ•°æ®åº“] {result.get('result', '')}")
                confidence_factors.append(0.8)
            
            elif result_type == "search":
                content = result.get("content", "")[:300]  # é™åˆ¶é•¿åº¦
                source = result.get("source", "")
                context_parts.append(f"[æœç´¢] {content}\næ¥æº: {source}")
                confidence_factors.append(result.get("relevance_score", 0.5))
            
            elif result_type == "vector":
                content = result.get("content", "")[:300]
                source = result.get("source", "çŸ¥è¯†åº“")
                similarity = result.get("similarity_score", 0.5)
                context_parts.append(f"[çŸ¥è¯†åº“] {content}\næ¥æº: {source} (ç›¸ä¼¼åº¦: {similarity:.2f})")
                confidence_factors.append(similarity)
        
        context = "\n\n".join(context_parts)
        
        # æ ¹æ®åˆ†æç±»å‹æ„å»ºæç¤ºè¯
        prompt = self._build_answer_prompt(query, context, analysis)
        
        try:
            # è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ
            if hasattr(self, 'llm_client') and self.llm_client:
                # ä½¿ç”¨çœŸå®çš„LLMå®¢æˆ·ç«¯
                answer = get_llm_answer_deepseek(
                    client=self.llm_client,
                    context=context,
                    question=query
                )
            else:
                # ä½¿ç”¨mockå‡½æ•°
                answer = f"åŸºäºåˆ†æç»“æœå›ç­”ï¼š{prompt[:200]}..."
            
            # è®¡ç®—ç½®ä¿¡åº¦
            avg_confidence = sum(confidence_factors) / len(confidence_factors) if confidence_factors else 0.5
            final_confidence = min(0.95, avg_confidence * analysis.confidence)
            
            return answer, final_confidence
            
        except Exception as e:
            self.logger.error(f"LLMç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            return self._synthesize_answer_fallback(query, results), 0.4
    
    def _build_answer_prompt(self, query: str, context: str, analysis: QueryAnalysisResult) -> str:
        """æ ¹æ®æŸ¥è¯¢åˆ†æç»“æœæ„å»ºç­”æ¡ˆæç¤ºè¯"""
        
        if analysis.query_type == "time":
            return f"""ç”¨æˆ·è¯¢é—®æ—¶é—´ç›¸å…³é—®é¢˜ï¼š{query}

ä»¥ä¸‹æ˜¯è·å–çš„æœ€æ–°ä¿¡æ¯ï¼š
{context}

è¯·åŸºäºè¿™äº›æœ€æ–°ä¿¡æ¯å‡†ç¡®å›ç­”ç”¨æˆ·çš„æ—¶é—´ç›¸å…³é—®é¢˜ã€‚å¦‚æœä¿¡æ¯ä¸­åŒ…å«å…·ä½“çš„æ—¶é—´æ•°æ®ï¼Œè¯·ç›´æ¥æä¾›ã€‚"""
        
        elif analysis.query_type == "calculation":
            return f"""ç”¨æˆ·è¯¢é—®æ•°å­¦è®¡ç®—é—®é¢˜ï¼š{query}

è®¡ç®—ç»“æœï¼š
{context}

è¯·åŸºäºè®¡ç®—ç»“æœä¸ºç”¨æˆ·æä¾›æ¸…æ™°çš„æ•°å­¦ç­”æ¡ˆï¼Œå¹¶ç®€è¦è¯´æ˜è®¡ç®—è¿‡ç¨‹ã€‚"""
        
        elif analysis.query_type == "technical":
            return f"""ç”¨æˆ·è¯¢é—®æŠ€æœ¯é—®é¢˜ï¼š{query}

ç›¸å…³æŠ€æœ¯ä¿¡æ¯ï¼š
{context}

è¯·åŸºäºè¿™äº›æŠ€æœ¯èµ„æ–™æä¾›è¯¦ç»†ã€å‡†ç¡®çš„æŠ€æœ¯è§£ç­”ã€‚å¯ä»¥åŒ…å«æŠ€æœ¯ç»†èŠ‚å’Œå®ç°æ–¹æ³•ã€‚"""
        
        else:
            return f"""ç”¨æˆ·é—®é¢˜ï¼š{query}

ç›¸å…³ä¿¡æ¯ï¼š
{context}

è¯·ç»¼åˆä»¥ä¸Šä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚å¦‚æœä¿¡æ¯æ¥æºäºä¸åŒæ¸ é“ï¼Œè¯·é€‚å½“æ•´åˆã€‚"""
    
    def _generate_fallback_answer(self, query: str, analysis: QueryAnalysisResult) -> str:
        """ç”Ÿæˆå¤‡ç”¨ç­”æ¡ˆ"""
        if analysis.query_type == "time":
            return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•è·å–å½“å‰çš„æ—¶é—´ä¿¡æ¯ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åå†è¯•ã€‚"
        elif analysis.query_type == "calculation":
            return f"æŠ±æ­‰ï¼Œæ— æ³•å®Œæˆè®¡ç®—ï¼š{query}ã€‚è¯·æ£€æŸ¥è¡¨è¾¾å¼æ˜¯å¦æ­£ç¡®ã€‚"
        elif analysis.query_type == "technical":
            return f"å…³äºæ‚¨è¯¢é—®çš„æŠ€æœ¯é—®é¢˜ã€Œ{query}ã€ï¼Œæˆ‘æš‚æ—¶æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚å»ºè®®æ‚¨æŸ¥é˜…å®˜æ–¹æ–‡æ¡£æˆ–æŠ€æœ¯è®ºå›ã€‚"
        else:
            return f"æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€Œ{query}ã€ã€‚è¯·å°è¯•é‡æ–°è¡¨è¿°æˆ–æä¾›æ›´å¤šä¸Šä¸‹æ–‡ã€‚"
    
    def _synthesize_answer_fallback(self, query: str, results: List[Dict[str, Any]]) -> str:
        """åˆæˆå¤‡ç”¨ç­”æ¡ˆ"""
        if not results:
            return f"å…³äºæ‚¨çš„é—®é¢˜ã€Œ{query}ã€ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
        
        answer_parts = [f"å…³äºæ‚¨çš„é—®é¢˜ã€Œ{query}ã€ï¼Œæˆ‘æ‰¾åˆ°äº†ä»¥ä¸‹ä¿¡æ¯ï¼š\n"]
        
        for i, result in enumerate(results[:3], 1):
            result_type = result.get("type", "unknown")
            if result_type == "calculation" and "result" in result:
                answer_parts.append(f"{i}. è®¡ç®—ç»“æœï¼š{result.get('expression', '')} = {result['result']}")
            elif result_type == "database_result":
                answer_parts.append(f"{i}. æ•°æ®åº“æŸ¥è¯¢ï¼š{result.get('result', '')}")
            else:
                content = result.get("content", "")[:200]
                source = result.get("source", "")
                answer_parts.append(f"{i}. {content} (æ¥æºï¼š{source})")
        
        answer_parts.append("\nä»¥ä¸Šä¿¡æ¯ä¾›æ‚¨å‚è€ƒã€‚")
        return "\n\n".join(answer_parts)
