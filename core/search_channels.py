
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Googleæœç´¢å¼•æ“é€šé“å®ç°ï¼ˆä½¿ç”¨å®˜æ–¹APIï¼‰

è¿™ä¸ªæ¨¡å—æä¾›äº†åŸºäºGoogle Custom Search APIçš„é€šé“å®ç°ã€‚
"""

import asyncio
import logging
import time
from typing import List, Dict, Any

import aiohttp
import requests  # æ·»åŠ åŒæ­¥è¯·æ±‚åº“
from bs4 import BeautifulSoup

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from service.channel_framework import BaseChannel, ChannelType, SearchResult, QueryContext
from dotenv import load_dotenv


load_dotenv()

class GoogleSearchChannel(BaseChannel):
    """Googleæœç´¢å¼•æ“é€šé“ï¼ˆä½¿ç”¨å®˜æ–¹APIï¼‰"""
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–Googleæœç´¢é€šé“
        
        Args:
            config: é…ç½®å­—å…¸ï¼Œå¿…é¡»åŒ…å« api_key å’Œ search_engine_id
        """
        if config is None:
            config = {}
        
        # è®¾ç½®é»˜è®¤é…ç½®
        default_config = {
            "priority": {
                "factual": 1,
                "analytical": 2, 
                "creative": 3,
                "conversational": 2
            },
            "timeout": 10,
            "max_content_length": 1000,
        }
        default_config.update(config)
        
        super().__init__(ChannelType.SEARCH_ENGINE, default_config)
        
        # Google API é…ç½®
        self.api_key = self.config.get("api_key")
        self.search_engine_id = self.config.get("search_engine_id")  # Custom Search Engine ID
        self.timeout = self.config.get("timeout", 10)
        self.max_content_length = self.config.get("max_content_length", 1000)
        
        if not self.api_key:
            self.logger.error("Google API Key æœªé…ç½®ï¼")
        if not self.search_engine_id:
            self.logger.error("Google Custom Search Engine ID æœªé…ç½®ï¼")
    
    async def search(self, context: QueryContext) -> List[SearchResult]:
        """
        æ‰§è¡ŒGoogleæœç´¢
        
        Args:
            context: æŸ¥è¯¢ä¸Šä¸‹æ–‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        return await self._google_api_search(context)
    
    def is_available(self) -> bool:
        """
        æ£€æŸ¥Googleæœç´¢æ˜¯å¦å¯ç”¨
        
        Returns:
            Trueè¡¨ç¤ºå¯ç”¨
        """
        return bool(self.api_key and self.search_engine_id)
    
    async def _google_api_search(self, context: QueryContext) -> List[SearchResult]:
        """
        ä½¿ç”¨Google Custom Search APIè¿›è¡Œæœç´¢
        
        Args:
            context: æŸ¥è¯¢ä¸Šä¸‹æ–‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.is_available():
            self.logger.error("Google API é…ç½®ä¸å®Œæ•´")
            return []
        
        try:
            self.logger.info(f"å¼€å§‹Google APIæœç´¢: {context.query}")
            
            # Google Custom Search API URL
            api_url = "https://www.googleapis.com/customsearch/v1"
            
            params = {
                "key": self.api_key,
                "cx": self.search_engine_id,
                "q": context.query,
                "num": min(context.max_results, 10),  # APIæœ€å¤šè¿”å›10ä¸ªç»“æœ
            }
            
            # ä½¿ç”¨åŒæ­¥requestsè¯·æ±‚ï¼Œå’Œtest.pyä¿æŒä¸€è‡´
            response = requests.get(api_url, params=params, timeout=self.timeout)
            
            if response.status_code == 200:
                data = response.json()
                return await self._process_api_results(data)
            else:
                error_text = response.text
                self.logger.error(f"Google API è¯·æ±‚å¤±è´¥: {response.status_code} - {error_text}")
                return []
                        
        except Exception as e:
            self.logger.error(f"Google APIæœç´¢å¤±è´¥: {e}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return []
    
    async def _process_api_results(self, data: Dict[str, Any]) -> List[SearchResult]:
        """
        å¤„ç†APIè¿”å›çš„æœç´¢ç»“æœ
        
        Args:
            data: APIè¿”å›çš„JSONæ•°æ®
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        if "items" not in data:
            self.logger.warning("APIè¿”å›æ•°æ®ä¸­æ²¡æœ‰æœç´¢ç»“æœ")
            return results
        
        for i, item in enumerate(data["items"]):
            try:
                title = item.get("title", "æ— æ ‡é¢˜")
                snippet = item.get("snippet", "")
                url = item.get("link", "")
                
                # è·å–æ›´è¯¦ç»†çš„é¡µé¢å†…å®¹ï¼ˆå¯é€‰ï¼‰
                detailed_content = await self._fetch_page_content(url) if url else ""
                content = detailed_content if detailed_content else snippet
                
                results.append(SearchResult(
                    title=title,
                    content=content,
                    url=url,
                    source="google_api",
                    timestamp=time.time(),
                    relevance_score=1.0 - (i * 0.1),
                    channel_type=self.channel_type,
                    metadata={
                        "search_rank": i + 1,
                        "snippet": snippet,
                        "content_length": len(content)
                    }
                ))
                
            except Exception as e:
                self.logger.warning(f"å¤„ç†æœç´¢ç»“æœæ—¶å‡ºé”™: {e}")
                continue
        
        self.logger.info(f"Google APIæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        return results
    
    async def _fetch_page_content(self, url: str) -> str:
        """
        è·å–é¡µé¢è¯¦ç»†å†…å®¹ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
        
        Args:
            url: é¡µé¢URL
            
        Returns:
            é¡µé¢å†…å®¹
        """
        try:
            headers = {
                'User-Agent': (
                    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                    'AppleWebKit/537.36 (KHTML, like Gecko) '
                    'Chrome/91.0.4472.124 Safari/537.36'
                )
            }
            
            async with aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=5),  # è¾ƒçŸ­è¶…æ—¶
                headers=headers
            ) as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        html_content = await response.text()
                        return self._extract_content(html_content)
                        
        except Exception as e:
            self.logger.debug(f"è·å–é¡µé¢å†…å®¹å¤±è´¥ {url}: {e}")
        
        return ""
    
    def _extract_content(self, html_content: str) -> str:
        """
        æå–é¡µé¢ä¸»è¦å†…å®¹
        
        Args:
            html_content: HTMLå†…å®¹
            
        Returns:
            æå–çš„æ–‡æœ¬å†…å®¹
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
            for tag in soup(["script", "style", "nav", "header", "footer", "aside"]):
                tag.decompose()
            
            # ä¼˜å…ˆæå–ä¸»è¦å†…å®¹åŒºåŸŸ
            main_content = None
            for selector in ["main", "article", ".content", "#content", ".main"]:
                main_content = soup.select_one(selector)
                if main_content:
                    break
            
            if not main_content:
                main_content = soup.find('body') or soup
            
            # æå–æ–‡æœ¬
            text = main_content.get_text()
            
            # æ¸…ç†æ–‡æœ¬
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            
            # é™åˆ¶é•¿åº¦
            if len(text) > self.max_content_length:
                text = text[:self.max_content_length] + "..."
            
            return text
            
        except Exception as e:
            self.logger.debug(f"æå–å†…å®¹å¤±è´¥: {e}")
            return ""


# ä¸ºäº†ä¿æŒå‘åå…¼å®¹æ€§ï¼Œæä¾›ä¸€ä¸ªåˆ«å
SearchEngineChannel = GoogleSearchChannel


def create_google_search_channel(api_key: str, search_engine_id: str, config: Dict[str, Any] = None) -> GoogleSearchChannel:
    """
    åˆ›å»ºGoogleæœç´¢é€šé“çš„å·¥å‚å‡½æ•°
    
    Args:
        api_key: Google API Key
        search_engine_id: Google Custom Search Engine ID
        config: é¢å¤–é…ç½®å­—å…¸
        
    Returns:
        GoogleSearchChannelå®ä¾‹
    """
    if config is None:
        config = {}
    
    config.update({
        "api_key": api_key,
        "search_engine_id": search_engine_id
    })
    
    return GoogleSearchChannel(config)


# æµ‹è¯•å‡½æ•°
async def test_google_search():
    """æµ‹è¯•Googleæœç´¢åŠŸèƒ½"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•Google APIæœç´¢é€šé“")
    
    # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
    import os
    api_key = os.getenv("GOOGLE_API_KEY")
    search_engine_id = os.getenv("GOOGLE_SEARCH_ENGINE_ID")
    
    if not api_key or not search_engine_id:
        print("âŒ è¯·è®¾ç½®ç¯å¢ƒå˜é‡:")
        print("   export GOOGLE_API_KEY='your_api_key'")
        print("   export GOOGLE_SEARCH_ENGINE_ID='your_search_engine_id'")
        return
    
    # åˆ›å»ºæœç´¢é€šé“
    channel = create_google_search_channel(api_key, search_engine_id)
    
    if not channel.is_available():
        print("âŒ Googleæœç´¢é€šé“ä¸å¯ç”¨")
        return
    
    print("âœ… Google APIæœç´¢é€šé“å¯ç”¨")
    
    # åˆ›å»ºæµ‹è¯•æŸ¥è¯¢
    from service.channel_framework import QueryContext, QueryType
    
    test_queries = [
        "ä¹ä¸‰é˜…å…µ",
        "äººå·¥æ™ºèƒ½æœ€æ–°å‘å±•",
        "æœºå™¨å­¦ä¹ ç®—æ³•"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: {query}")
        
        context = QueryContext(
            query=query,
            query_type=QueryType.FACTUAL,
            max_results=10
        )
        
        try:
            results = await channel.search(context)
            
            if results:
                print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªç»“æœ:")
                for i, result in enumerate(results, 1):
                    print(f"  {i}. {result.title}")
                    print(f"     URL: {result.url}")
                    print(f"     ç›¸å…³æ€§: {result.relevance_score:.2f}")
                    print(f"     å†…å®¹é¢„è§ˆ: {result.content}...")
                    print()
            else:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°ç»“æœ")
                
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
    
    print("ğŸ‰ æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_google_search())