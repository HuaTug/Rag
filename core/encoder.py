#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ–‡æœ¬åµŒå…¥æ¨¡å—

æä¾›ç»Ÿä¸€çš„æ–‡æœ¬åµŒå…¥æ¥å£ï¼Œæ”¯æŒå¼€æºæ¨¡å‹å’ŒOpenAIæ¨¡å‹ã€‚
ç§»é™¤äº†Streamlitä¾èµ–ï¼Œä½¿ç”¨æ ‡å‡†çš„ç¼“å­˜æœºåˆ¶ã€‚
"""

import os
from functools import lru_cache
from typing import Dict, List, Optional, Union

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False


class EmbeddingCache:
    """åµŒå…¥å‘é‡ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, max_size: int = 1000):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            max_size: æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
        """
        self.cache: Dict[str, List[float]] = {}
        self.max_size = max_size
        self.access_order: List[str] = []
    
    def get(self, key: str) -> Optional[List[float]]:
        """è·å–ç¼“å­˜çš„åµŒå…¥å‘é‡"""
        if key in self.cache:
            # æ›´æ–°è®¿é—®é¡ºåº
            if key in self.access_order:
                self.access_order.remove(key)
            self.access_order.append(key)
            return self.cache[key]
        return None
    
    def set(self, key: str, value: List[float]):
        """è®¾ç½®ç¼“å­˜çš„åµŒå…¥å‘é‡"""
        # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€ä¹…æœªè®¿é—®çš„æ¡ç›®
        if len(self.cache) >= self.max_size and key not in self.cache:
            oldest_key = self.access_order.pop(0)
            del self.cache[oldest_key]
        
        self.cache[key] = value
        if key in self.access_order:
            self.access_order.remove(key)
        self.access_order.append(key)
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        self.access_order.clear()


# å…¨å±€ç¼“å­˜å®ä¾‹
_embedding_cache = EmbeddingCache()


@lru_cache(maxsize=1)
def get_sentence_transformer_model(model_name: str = "all-MiniLM-L6-v2") -> Optional[SentenceTransformer]:
    """
    è·å–sentence-transformersæ¨¡å‹ï¼ˆå¸¦ç¼“å­˜ï¼‰
    
    Args:
        model_name: æ¨¡å‹åç§°
        
    Returns:
        SentenceTransformeræ¨¡å‹å®ä¾‹æˆ–None
    """
    if not SENTENCE_TRANSFORMERS_AVAILABLE:
        raise ImportError("è¯·å®‰è£… sentence-transformers: pip install sentence-transformers")
    
    try:
        # ä½¿ç”¨å¤šè¯­è¨€æ”¯æŒçš„åµŒå…¥æ¨¡å‹
        model = SentenceTransformer(model_name)
        return model
    except Exception as e:
        raise RuntimeError(f"æ— æ³•åŠ è½½sentence-transformersæ¨¡å‹ {model_name}: {e}")


def emb_text_opensource(text: str, model_name: str = "all-MiniLM-L6-v2") -> List[float]:
    """
    ä½¿ç”¨å¼€æºæ¨¡å‹è¿›è¡Œæ–‡æœ¬åµŒå…¥
    
    Args:
        text: è¦åµŒå…¥çš„æ–‡æœ¬
        model_name: æ¨¡å‹åç§°
        
    Returns:
        åµŒå…¥å‘é‡åˆ—è¡¨
        
    Raises:
        ImportError: å½“sentence-transformersæœªå®‰è£…æ—¶
        RuntimeError: å½“æ¨¡å‹åŠ è½½å¤±è´¥æ—¶
    """
    # æ£€æŸ¥ç¼“å­˜
    cache_key = f"{model_name}:{text}"
    cached_embedding = _embedding_cache.get(cache_key)
    if cached_embedding is not None:
        return cached_embedding
    
    # è·å–æ¨¡å‹
    model = get_sentence_transformer_model(model_name)
    if model is None:
        raise RuntimeError("æ— æ³•åŠ è½½sentence-transformersæ¨¡å‹")
    
    # ç”ŸæˆåµŒå…¥å‘é‡
    embedding = model.encode(text).tolist()
    
    # ç¼“å­˜ç»“æœ
    _embedding_cache.set(cache_key, embedding)
    
    return embedding


def emb_text_openai(
    client: OpenAI, 
    text: str, 
    model: str = "text-embedding-3-small"
) -> List[float]:
    """
    ä½¿ç”¨OpenAIè¿›è¡Œæ–‡æœ¬åµŒå…¥
    
    Args:
        client: OpenAIå®¢æˆ·ç«¯å®ä¾‹
        text: è¦åµŒå…¥çš„æ–‡æœ¬
        model: OpenAIåµŒå…¥æ¨¡å‹åç§°
        
    Returns:
        åµŒå…¥å‘é‡åˆ—è¡¨
        
    Raises:
        ImportError: å½“openaiåº“æœªå®‰è£…æ—¶
        Exception: å½“APIè°ƒç”¨å¤±è´¥æ—¶
    """
    if not OPENAI_AVAILABLE:
        raise ImportError("è¯·å®‰è£… openai: pip install openai")
    
    # æ£€æŸ¥ç¼“å­˜
    cache_key = f"openai:{model}:{text}"
    cached_embedding = _embedding_cache.get(cache_key)
    if cached_embedding is not None:
        return cached_embedding
    
    try:
        # è°ƒç”¨OpenAI API
        response = client.embeddings.create(input=text, model=model)
        embedding = response.data[0].embedding
        
        # ç¼“å­˜ç»“æœ
        _embedding_cache.set(cache_key, embedding)
        
        return embedding
    except Exception as e:
        raise RuntimeError(f"OpenAIåµŒå…¥APIè°ƒç”¨å¤±è´¥: {e}")


def emb_text(
    client_or_text: Union[OpenAI, str], 
    text: Optional[str] = None, 
    model: str = "auto"
) -> List[float]:
    """
    ç»Ÿä¸€çš„åµŒå…¥æ¥å£ï¼Œè‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„åµŒå…¥æ–¹æ³•
    
    Args:
        client_or_text: å¦‚æœæ˜¯å­—ç¬¦ä¸²åˆ™ç›´æ¥ä½¿ç”¨å¼€æºæ¨¡å‹ï¼Œå¦‚æœæ˜¯OpenAIå®¢æˆ·ç«¯åˆ™ä½¿ç”¨OpenAI
        text: è¦åµŒå…¥çš„æ–‡æœ¬ï¼ˆå½“ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯å®¢æˆ·ç«¯æ—¶ä½¿ç”¨ï¼‰
        model: æ¨¡å‹é€‰æ‹©ï¼Œ"auto"ä¸ºè‡ªåŠ¨é€‰æ‹©
        
    Returns:
        åµŒå…¥å‘é‡åˆ—è¡¨
        
    Raises:
        ValueError: å½“å‚æ•°ä¸æ­£ç¡®æ—¶
        ImportError: å½“æ‰€éœ€åº“æœªå®‰è£…æ—¶
        RuntimeError: å½“åµŒå…¥ç”Ÿæˆå¤±è´¥æ—¶
    """
    # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨å¼€æºæ¨¡å‹
    use_opensource = os.getenv("USE_OPENSOURCE_EMBEDDING", "true").lower() == "true"
    
    if use_opensource or isinstance(client_or_text, str):
        # ä½¿ç”¨å¼€æºæ¨¡å‹
        input_text = client_or_text if isinstance(client_or_text, str) else text
        if input_text is None:
            raise ValueError("æ–‡æœ¬å‚æ•°ä¸èƒ½ä¸ºç©º")
        
        # é€‰æ‹©æ¨¡å‹
        if model == "auto":
            model_name = "all-MiniLM-L6-v2"  # é»˜è®¤è½»é‡çº§è‹±æ–‡æ¨¡å‹
        else:
            model_name = model
        
        return emb_text_opensource(input_text, model_name)
    else:
        # ä½¿ç”¨OpenAIæ¨¡å‹
        if not isinstance(client_or_text, OpenAI):
            raise ValueError("ç¬¬ä¸€ä¸ªå‚æ•°å¿…é¡»æ˜¯OpenAIå®¢æˆ·ç«¯å®ä¾‹")
        if text is None:
            raise ValueError("æ–‡æœ¬å‚æ•°ä¸èƒ½ä¸ºç©º")
        
        # é€‰æ‹©æ¨¡å‹
        if model == "auto":
            openai_model = "text-embedding-3-small"  # é»˜è®¤OpenAIæ¨¡å‹
        else:
            openai_model = model
        
        return emb_text_openai(client_or_text, text, openai_model)


def get_embedding_dimension(model_name: str = "all-MiniLM-L6-v2") -> int:
    """
    è·å–åµŒå…¥å‘é‡çš„ç»´åº¦
    
    Args:
        model_name: æ¨¡å‹åç§°
        
    Returns:
        åµŒå…¥å‘é‡ç»´åº¦
    """
    # å¸¸è§æ¨¡å‹çš„ç»´åº¦æ˜ å°„
    dimension_map = {
        "all-MiniLM-L6-v2": 384,
        "all-MiniLM-L12-v2": 384,
        "paraphrase-multilingual-MiniLM-L12-v2": 384,
        "text-embedding-3-small": 1536,
        "text-embedding-3-large": 3072,
        "text-embedding-ada-002": 1536,
    }
    
    return dimension_map.get(model_name, 384)  # é»˜è®¤è¿”å›384


def clear_embedding_cache():
    """æ¸…ç©ºåµŒå…¥å‘é‡ç¼“å­˜"""
    _embedding_cache.clear()


def get_cache_stats() -> Dict[str, int]:
    """
    è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        åŒ…å«ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    return {
        "cache_size": len(_embedding_cache.cache),
        "max_size": _embedding_cache.max_size,
        "hit_rate": len(_embedding_cache.access_order) / max(1, len(_embedding_cache.cache))
    }


# ä¾¿æ·å‡½æ•°
def encode_text(text: str, use_openai: bool = False, **kwargs) -> List[float]:
    """
    ä¾¿æ·çš„æ–‡æœ¬ç¼–ç å‡½æ•°
    
    Args:
        text: è¦ç¼–ç çš„æ–‡æœ¬
        use_openai: æ˜¯å¦ä½¿ç”¨OpenAIï¼ˆéœ€è¦è®¾ç½®ç›¸åº”çš„ç¯å¢ƒå˜é‡ï¼‰
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        åµŒå…¥å‘é‡åˆ—è¡¨
    """
    if use_openai and OPENAI_AVAILABLE:
        # å°è¯•ä½¿ç”¨OpenAI
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            client = OpenAI(api_key=api_key)
            return emb_text(client, text, kwargs.get("model", "auto"))
    
    # ä½¿ç”¨å¼€æºæ¨¡å‹
    return emb_text(text, model=kwargs.get("model", "auto"))


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
    
    print("ğŸ§ª æµ‹è¯•æ–‡æœ¬åµŒå…¥åŠŸèƒ½...")
    
    try:
        # æµ‹è¯•å¼€æºæ¨¡å‹
        print("æµ‹è¯•å¼€æºæ¨¡å‹...")
        embedding = emb_text(test_text)
        print(f"âœ… å¼€æºæ¨¡å‹åµŒå…¥æˆåŠŸï¼Œç»´åº¦: {len(embedding)}")
        
        # æµ‹è¯•ç¼“å­˜
        print("æµ‹è¯•ç¼“å­˜åŠŸèƒ½...")
        embedding2 = emb_text(test_text)
        print(f"âœ… ç¼“å­˜æµ‹è¯•æˆåŠŸï¼Œç»“æœä¸€è‡´: {embedding == embedding2}")
        
        # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
        stats = get_cache_stats()
        print(f"ğŸ“Š ç¼“å­˜ç»Ÿè®¡: {stats}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
