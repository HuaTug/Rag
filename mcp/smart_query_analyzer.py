#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ™ºèƒ½æŸ¥è¯¢åˆ†æå™¨

é›†æˆGo demoçš„æ™ºèƒ½åˆ†æèƒ½åŠ›ï¼Œæä¾›è¯­ä¹‰ç†è§£å’Œå·¥å…·é€‰æ‹©åŠŸèƒ½ã€‚
"""

import asyncio
import json
import logging
import re
import sys
import os
from dataclasses import dataclass
from typing import Dict, Any, Optional, List, Union
import aiohttp
import time

# æ·»åŠ ä¸Šçº§ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å°è¯•å¯¼å…¥åŸå§‹å‡½æ•°å’Œå®¢æˆ·ç«¯
try:
    from ask_llm import get_llm_answer_deepseek as _original_llm_function, TencentDeepSeekClient
    _llm_available = True
except ImportError:
    _original_llm_function = None
    TencentDeepSeekClient = None
    _llm_available = False

# åˆ›å»ºé€‚é…å‡½æ•°
async def get_llm_answer_deepseek(query: str, search_flag: bool = False, timeout: int = 15) -> str:
    """
    é€‚é…å‡½æ•°ï¼Œå…¼å®¹åŸå§‹LLMå‡½æ•°çš„è°ƒç”¨æ–¹å¼
    """
    if not _llm_available:
        # ğŸ§  æ™ºèƒ½æ¨¡æ‹Ÿå“åº” - æ ¹æ®æŸ¥è¯¢å†…å®¹ç”Ÿæˆåˆé€‚çš„JSON
        return _generate_mock_response(query)
    
    try:
        # å°è¯•ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
        import os
        api_key = os.getenv("DEEPSEEK_API_KEY","sk-qFPEqgpxmS8DJ0nJQ6gvdIkozY1k2oEZER2A4zRhLxBvtIHl") or os.getenv("TENCENT_API_KEY")
        
        if not api_key:
            # å¦‚æœæ²¡æœ‰APIå¯†é’¥ï¼Œè¿”å›æ™ºèƒ½æ¨¡æ‹Ÿå“åº”
            return _generate_mock_response(query)
        
        # åˆ›å»ºå®¢æˆ·ç«¯å¹¶è°ƒç”¨åŸå§‹å‡½æ•°
        client = TencentDeepSeekClient(api_key=api_key)
        
        # è°ƒç”¨åŸå§‹å‡½æ•°ï¼Œä½¿ç”¨é€‚å½“çš„å‚æ•°
        response = _original_llm_function(
            client=client,
            context="",  # ç©ºä¸Šä¸‹æ–‡ï¼Œè®©LLMåŸºäºçŸ¥è¯†åº“å›ç­”
            question=query,
            model="deepseek-v3-0324"
        )
        
        return response
        
    except Exception as e:
        # å¦‚æœè°ƒç”¨å¤±è´¥ï¼Œè¿”å›æ™ºèƒ½æ¨¡æ‹Ÿå“åº”
        return _generate_mock_response(query, error=str(e))


def _generate_mock_response(query: str, error: str = None) -> str:
    """ç”Ÿæˆæ™ºèƒ½æ¨¡æ‹Ÿå“åº”ï¼Œç”¨äºæµ‹è¯•å’Œå›é€€"""
    query_lower = query.lower()
    
    # ğŸ§® è®¡ç®—æŸ¥è¯¢æ¨¡æ‹Ÿ
    if any(word in query_lower for word in ["è®¡ç®—", "åŠ ", "å‡", "ä¹˜", "é™¤", "+", "-", "*", "/", "ç­‰äº"]):
        numbers = re.findall(r'\d+\.?\d*', query)  # åªåœ¨æŸ¥è¯¢ä¸­æŸ¥æ‰¾æ•°å­—
        if len(numbers) >= 2:
            calc_args = {"operation": "add", "x": float(numbers[0]), "y": float(numbers[1])}
            if "å‡" in query_lower or "-" in query:
                calc_args["operation"] = "subtract"
            elif "ä¹˜" in query_lower or "*" in query or "Ã—" in query:
                calc_args["operation"] = "multiply"
            elif "é™¤" in query_lower or "/" in query or "Ã·" in query:
                calc_args["operation"] = "divide"
        else:
            calc_args = {"operation": "expression", "expression": query}
        
        return json.dumps({
            "needs_web_search": False,
            "web_search_query": "",
            "needs_vector_search": False,
            "needs_database": False,
            "database_query": {},
            "needs_calculation": True,
            "calculation_args": calc_args,
            "query_type": "calculation",
            "confidence": 0.9,
            "reasoning": f"ç”¨æˆ·æŸ¥è¯¢'{query}'æ˜¯ä¸€ä¸ªæ˜ç¡®çš„æ•°å­¦è®¡ç®—é—®é¢˜ã€‚æ£€æµ‹åˆ°æ•°å­—: {numbers}ï¼Œè¿ç®—ç±»å‹: {calc_args.get('operation', 'unknown')}ã€‚æ ¹æ®æ™ºèƒ½åˆ†æè§„åˆ™ï¼Œè¿™ç±»æ˜ç¡®çš„è®¡ç®—éœ€æ±‚åº”ç›´æ¥ä½¿ç”¨calculationå·¥å…·è¿›è¡Œå¤„ç†ã€‚",
            "enable_dynamic_search": False,
            "min_similarity_threshold": 0.8
        }, ensure_ascii=False)
    
    # â° æ—¶é—´æŸ¥è¯¢æ¨¡æ‹Ÿ
    elif any(word in query_lower for word in ["ä»Šå¤©", "ç°åœ¨", "å‡ å·", "å‡ æœˆ", "å‡ ç‚¹", "å½“å‰", "æ—¥æœŸ", "æ—¶é—´"]):
        return json.dumps({
            "needs_web_search": True,
            "web_search_query": "å½“å‰æ—¶é—´æ—¥æœŸ",
            "needs_vector_search": False,
            "needs_database": False,
            "database_query": {},
            "needs_calculation": True,
            "calculation_args": {"operation": "get_current_date"},
            "query_type": "time",
            "confidence": 0.95,
            "reasoning": f"ç”¨æˆ·æŸ¥è¯¢'{query}'æ¶‰åŠæ—¶é—´ç›¸å…³ä¿¡æ¯ï¼Œéœ€è¦è·å–å½“å‰çš„å®æ—¶æ•°æ®ã€‚æ ¹æ®æ™ºèƒ½åˆ†æè§„åˆ™ï¼Œæ—¶é—´ç›¸å…³æŸ¥è¯¢åº”ç›´æ¥ä½¿ç”¨web_searchè·å–æœ€æ–°ä¿¡æ¯ï¼ŒåŒæ—¶ä½¿ç”¨calculationè·å–å½“å‰æ—¥æœŸã€‚",
            "enable_dynamic_search": False,
            "min_similarity_threshold": 0.8
        }, ensure_ascii=False)
    
    # ğŸ” æŠ€æœ¯æ¦‚å¿µæŸ¥è¯¢æ¨¡æ‹Ÿ
    elif any(word in query_lower for word in ["ä»€ä¹ˆæ˜¯", "å¦‚ä½•", "æ€ä¹ˆ", "åŸç†", "æ¦‚å¿µ", "å®šä¹‰", "æŠ€æœ¯"]):
        return json.dumps({
            "needs_web_search": False,
            "web_search_query": "",
            "needs_vector_search": True,
            "needs_database": False,
            "database_query": {},
            "needs_calculation": False,
            "calculation_args": {},
            "query_type": "technical",
            "confidence": 0.85,
            "reasoning": f"ç”¨æˆ·æŸ¥è¯¢'{query}'æ˜¯æŠ€æœ¯æ¦‚å¿µç›¸å…³é—®é¢˜ï¼Œè¯¢é—®æŸä¸ªæŠ€æœ¯æˆ–æ¦‚å¿µçš„å®šä¹‰ã€åŸç†æˆ–ä½¿ç”¨æ–¹æ³•ã€‚æ ¹æ®æ™ºèƒ½åˆ†æè§„åˆ™ï¼ŒæŠ€æœ¯æ¦‚å¿µæŸ¥è¯¢åº”ä¼˜å…ˆä½¿ç”¨vector_searchæ£€ç´¢çŸ¥è¯†åº“ï¼Œå¯ç”¨åŠ¨æ€æœç´¢ç­–ç•¥ä»¥ç¡®ä¿ç­”æ¡ˆè´¨é‡ã€‚",
            "enable_dynamic_search": True,
            "min_similarity_threshold": 0.8
        }, ensure_ascii=False)
    
    # ğŸ“Š æ•°æ®åº“æŸ¥è¯¢æ¨¡æ‹Ÿ
    elif any(word in query_lower for word in ["ç»Ÿè®¡", "æ•°é‡", "ç”¨æˆ·", "æ•°æ®", "æŸ¥è¯¢", "è®°å½•", "æ´»è·ƒ"]):
        return json.dumps({
            "needs_web_search": False,
            "web_search_query": "",
            "needs_vector_search": True,
            "needs_database": True,
            "database_query": {"query_type": "count", "table": "users", "group_by": "status"},
            "needs_calculation": False,
            "calculation_args": {},
            "query_type": "database",
            "confidence": 0.8,
            "reasoning": f"ç”¨æˆ·æŸ¥è¯¢'{query}'æ¶‰åŠæ•°æ®ç»Ÿè®¡æˆ–æŸ¥è¯¢éœ€æ±‚ã€‚æ ¹æ®æ™ºèƒ½åˆ†æè§„åˆ™ï¼Œè¿™ç±»æŸ¥è¯¢éœ€è¦ä½¿ç”¨database_queryè·å–æ•°æ®åº“ä¿¡æ¯ï¼ŒåŒæ—¶å¯ç”¨vector_searchæä¾›èƒŒæ™¯çŸ¥è¯†ã€‚",
            "enable_dynamic_search": True,
            "min_similarity_threshold": 0.8
        }, ensure_ascii=False)
    
    # ğŸ“Š é»˜è®¤æŸ¥è¯¢æ¨¡æ‹Ÿ
    else:
        return json.dumps({
            "needs_web_search": False,
            "web_search_query": "",
            "needs_vector_search": True,
            "needs_database": False,
            "database_query": {},
            "needs_calculation": False,
            "calculation_args": {},
            "query_type": "general",
            "confidence": 0.7,
            "reasoning": f"'{query}'æ˜¯ä¸€ä¸ªä¸€èˆ¬æ€§æŸ¥è¯¢ï¼Œæ²¡æœ‰æ˜æ˜¾çš„æ—¶é—´ã€è®¡ç®—æˆ–æŠ€æœ¯æ¦‚å¿µç‰¹å¾ã€‚æ ¹æ®æ™ºèƒ½åˆ†æè§„åˆ™ï¼Œé‡‡ç”¨å‘é‡æœç´¢ä½œä¸ºä¸»è¦ç­–ç•¥ï¼Œå¯ç”¨åŠ¨æ€æœç´¢ä»¥æé«˜ç­”æ¡ˆè´¨é‡ã€‚",
            "enable_dynamic_search": True,
            "min_similarity_threshold": 0.8
        }, ensure_ascii=False)


@dataclass
class ToolCall:
    """å·¥å…·è°ƒç”¨ç»“æ„"""
    name: str
    args: Dict[str, Any]
    reasoning: str = ""


@dataclass
class QueryAnalysisResult:
    """æŸ¥è¯¢åˆ†æç»“æœ"""
    needs_web_search: bool = False
    web_search_query: str = ""
    needs_database: bool = False
    database_query: Dict[str, Any] = None
    needs_calculation: bool = False
    calculation_args: Dict[str, Any] = None
    needs_vector_search: bool = True  # é»˜è®¤å¯ç”¨å‘é‡æœç´¢
    query_type: str = "general"
    confidence: float = 0.0
    reasoning: str = ""
    tool_calls: List[ToolCall] = None
    # æ–°å¢ï¼šåŠ¨æ€æœç´¢ç­–ç•¥
    enable_dynamic_search: bool = True  # å¯ç”¨åŠ¨æ€æœç´¢ç­–ç•¥
    min_similarity_threshold: float = 0.8  # æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
    
    def __post_init__(self):
        if self.database_query is None:
            self.database_query = {}
        if self.calculation_args is None:
            self.calculation_args = {}
        if self.tool_calls is None:
            self.tool_calls = []


class SmartQueryAnalyzer:
    """æ™ºèƒ½æŸ¥è¯¢åˆ†æå™¨ - é›†æˆGo demoçš„åˆ†æèƒ½åŠ›"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # åˆ†æé…ç½® - ä¼˜å…ˆä½¿ç”¨è¯­ä¹‰åˆ†æ
        self.enable_semantic_analysis = self.config.get("enable_semantic_analysis", True)
        self.fallback_to_keywords = self.config.get("fallback_to_keywords", True)
        self.analysis_timeout = self.config.get("analysis_timeout", 15)
        
        self.logger.info(f"ğŸ§  æ™ºèƒ½æŸ¥è¯¢åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"   ğŸ¯ è¯­ä¹‰åˆ†æ(LLM): {'âœ… å¯ç”¨' if self.enable_semantic_analysis else 'âŒ ç¦ç”¨'}")
        self.logger.info(f"   ğŸ”„ å…³é”®è¯å›é€€: {'âœ… å¯ç”¨' if self.fallback_to_keywords else 'âŒ ç¦ç”¨'}")
        
        # å…³é”®è¯é…ç½®ï¼ˆä»…ç”¨äºå›é€€ï¼‰
        self._init_keyword_patterns()
        
        self.logger.info(f"ğŸ§  æ™ºèƒ½æŸ¥è¯¢åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ - è¯­ä¹‰åˆ†æ: {self.enable_semantic_analysis}")
    
    def _init_keyword_patterns(self):
        """åˆå§‹åŒ–å…³é”®è¯åŒ¹é…æ¨¡å¼ï¼ˆä»…ç”¨äºå›é€€åˆ†æï¼‰"""
        # ç®€åŒ–çš„å…³é”®è¯é…ç½®ï¼Œä»…ç”¨äºå›é€€æƒ…å†µ
        self.time_keywords = ["ä»Šå¤©", "ç°åœ¨", "å‡ å·", "å‡ æœˆ", "å‡ ç‚¹", "å½“å‰", "æ—¥æœŸ", "æ—¶é—´"]
        self.calculation_keywords = ["è®¡ç®—", "åŠ ", "å‡", "ä¹˜", "é™¤", "+", "-", "*", "/"]
        
        self.logger.info("ğŸ“‹ å…³é”®è¯æ¨¡å¼å·²åˆå§‹åŒ–ï¼ˆä»…ç”¨äºå›é€€åˆ†æï¼‰")
    
    async def analyze_query_intent(self, query: str) -> QueryAnalysisResult:
        """åˆ†ææŸ¥è¯¢æ„å›¾ - ä¸»å…¥å£æ–¹æ³•"""
        start_time = time.time()
        
        try:
            self.logger.info(f"ğŸ” å¼€å§‹åˆ†ææŸ¥è¯¢: {query}")
            
            # ä¼˜å…ˆä½¿ç”¨è¯­ä¹‰åˆ†æï¼ˆå¤§æ¨¡å‹åˆ†æï¼‰
            if self.enable_semantic_analysis:
                try:
                    result = await self._semantic_analysis(query)
                    analysis_time = time.time() - start_time
                    self.logger.info(f"âœ… è¯­ä¹‰åˆ†æå®Œæˆ - è€—æ—¶: {analysis_time:.2f}s")
                    return result
                except Exception as e:
                    self.logger.warning(f"âš ï¸ è¯­ä¹‰åˆ†æå¤±è´¥: {e}")
                    # å¦‚æœä¸å…è®¸å›é€€åˆ°å…³é”®è¯ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
                    if not self.fallback_to_keywords:
                        raise
            
            # åªæœ‰åœ¨è¯­ä¹‰åˆ†æå¤±è´¥ä¸”å…è®¸å›é€€æ—¶æ‰ä½¿ç”¨å…³é”®è¯åŒ¹é…
            self.logger.info("ğŸ”„ å›é€€åˆ°ç®€åŒ–çš„å…³é”®è¯åˆ†æ...")
            result = await self._fallback_analysis(query)
            analysis_time = time.time() - start_time
            self.logger.info(f"âœ… å›é€€åˆ†æå®Œæˆ - è€—æ—¶: {analysis_time:.2f}s")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ æŸ¥è¯¢åˆ†æå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤åˆ†æç»“æœ
            return QueryAnalysisResult(
                needs_vector_search=True,
                query_type="general",
                reasoning=f"åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥: {str(e)}",
                confidence=0.3,
                enable_dynamic_search=True
            )
    
    async def _semantic_analysis(self, query: str) -> QueryAnalysisResult:
        """ä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰åˆ†æ"""
        try:
            # ğŸ§  ç›´æ¥å¯¹ç”¨æˆ·æŸ¥è¯¢è¿›è¡Œæ™ºèƒ½åˆ†æï¼Œè€Œä¸æ˜¯ä¼ é€’æç¤ºè¯
            response = await get_llm_answer_deepseek(
                query=query,  # ç›´æ¥ä¼ é€’ç”¨æˆ·æŸ¥è¯¢
                search_flag=False,  # ç¦ç”¨æœç´¢é¿å…å¾ªç¯è°ƒç”¨
                timeout=self.analysis_timeout
            )
            
            self.logger.info(f"ğŸ” LLMåŸå§‹å“åº”: {response[:200]}...")
            
            # è§£æåˆ†æç»“æœ
            analysis = self._parse_llm_response(response, query)
            
            # æ™ºèƒ½å¡«å……å·¥å…·å‚æ•°
            self._fill_tool_parameters(analysis, query)
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"LLMè¯­ä¹‰åˆ†æå¤±è´¥: {e}")
            raise
    
    def _build_analysis_prompt(self, query: str) -> str:
        """æ„å»ºLLMåˆ†ææç¤ºè¯ - è®©å¤§æ¨¡å‹æ™ºèƒ½è¯­ä¹‰åˆ†æå¹¶ç”ŸæˆJSON"""
        return f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½RAGç³»ç»Ÿçš„æŸ¥è¯¢åˆ†æå™¨ã€‚è¯·é€šè¿‡è¯­ä¹‰åˆ†æç†è§£ç”¨æˆ·æŸ¥è¯¢çš„çœŸå®æ„å›¾ï¼Œå¹¶æ™ºèƒ½é€‰æ‹©æœ€é€‚åˆçš„å·¥å…·ç»„åˆã€‚

ç”¨æˆ·æŸ¥è¯¢ï¼š"{query}"

ğŸ”§ **å¯ç”¨å·¥å…·è¯´æ˜ï¼š**

1. **web_search** - ç½‘ç»œæœç´¢
   - é€‚ç”¨åœºæ™¯ï¼šæ—¶é—´æ—¥æœŸæŸ¥è¯¢ã€æœ€æ–°ä¿¡æ¯ã€å®æ—¶æ•°æ®ã€æ–°é—»
   - ç¤ºä¾‹ï¼š"ä»Šå¤©å‡ å·ï¼Ÿ"ã€"æœ€æ–°AIå‘å±•"ã€"ç°åœ¨å‡ ç‚¹ï¼Ÿ"
   
2. **vector_search** - å‘é‡çŸ¥è¯†åº“æ£€ç´¢
   - é€‚ç”¨åœºæ™¯ï¼šæŠ€æœ¯æ¦‚å¿µã€å®šä¹‰è§£é‡Šã€å†å²çŸ¥è¯†ã€æ•™ç¨‹
   - ç¤ºä¾‹ï¼š"ä»€ä¹ˆæ˜¯Pythonï¼Ÿ"ã€"å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ"
   
3. **calculation** - æ•°å­¦è®¡ç®—
   - é€‚ç”¨åœºæ™¯ï¼šæ•°å­¦è¿ç®—ã€æ•°å€¼è®¡ç®—ã€å…¬å¼æ±‚è§£
   - ç¤ºä¾‹ï¼š"100+200ç­‰äºå¤šå°‘ï¼Ÿ"ã€"è®¡ç®—å¹³æ–¹æ ¹"
   
4. **database_query** - æ•°æ®åº“æŸ¥è¯¢
   - é€‚ç”¨åœºæ™¯ï¼šç”¨æˆ·ç»Ÿè®¡ã€æ•°æ®åˆ†æã€è®°å½•æŸ¥è¯¢
   - ç¤ºä¾‹ï¼š"ç»Ÿè®¡ç”¨æˆ·æ•°é‡"ã€"æŸ¥è¯¢æ´»è·ƒç”¨æˆ·"

ğŸ§  **æ™ºèƒ½åˆ†æè¦æ±‚ï¼š**

è¯·ä»”ç»†åˆ†ææŸ¥è¯¢è¯­ä¹‰ï¼Œç†è§£ç”¨æˆ·çš„çœŸå®éœ€æ±‚ï¼Œç„¶åæ™ºèƒ½å¡«å……ä»¥ä¸‹JSONå‚æ•°ï¼š

ğŸ“ **è®¡ç®—æŸ¥è¯¢ç¤ºä¾‹åˆ†æï¼š**
- æŸ¥è¯¢ï¼š"100+100ç­‰äºå¤šå°‘ï¼Ÿ"
- è¯­ä¹‰åˆ†æï¼šç”¨æˆ·æ˜ç¡®è¦æ±‚è¿›è¡Œæ•°å­¦åŠ æ³•è¿ç®—
- åº”è®¾ç½®ï¼šneeds_calculation=true, calculation_args={{"operation":"add","x":100,"y":100}}
- æ¨ç†ï¼šè¿™æ˜¯æ˜ç¡®çš„æ•°å­¦è®¡ç®—é—®é¢˜ï¼Œç›´æ¥ä½¿ç”¨calculationå·¥å…·ï¼Œä¸éœ€è¦æœç´¢

â° **æ—¶é—´æŸ¥è¯¢ç¤ºä¾‹åˆ†æï¼š**
- æŸ¥è¯¢ï¼š"ä»Šå¤©å‡ å·ï¼Ÿ"
- è¯­ä¹‰åˆ†æï¼šç”¨æˆ·è¯¢é—®å½“å‰æ—¥æœŸä¿¡æ¯
- åº”è®¾ç½®ï¼šneeds_web_search=true, web_search_query="ä»Šå¤©æ—¥æœŸ", needs_vector_search=false
- æ¨ç†ï¼šæ—¶é—´ä¿¡æ¯éœ€è¦å®æ—¶è·å–ï¼Œä½¿ç”¨web_searchè·å–æœ€æ–°æ—¥æœŸ

ğŸ” **æŠ€æœ¯æ¦‚å¿µç¤ºä¾‹åˆ†æï¼š**
- æŸ¥è¯¢ï¼š"ä»€ä¹ˆæ˜¯Goè¯­è¨€ï¼Ÿ"
- è¯­ä¹‰åˆ†æï¼šç”¨æˆ·è¯¢é—®æŠ€æœ¯æ¦‚å¿µå®šä¹‰
- åº”è®¾ç½®ï¼šneeds_vector_search=true, enable_dynamic_search=true
- æ¨ç†ï¼šæŠ€æœ¯æ¦‚å¿µä¼˜å…ˆä»çŸ¥è¯†åº“æ£€ç´¢ï¼Œå¦‚æœè´¨é‡ä¸ä½³åˆ™å¯ç”¨ç½‘ç»œæœç´¢

ğŸ“Š **å‚æ•°è¯¦ç»†è¯´æ˜ï¼š**

calculation_argsæ”¯æŒçš„è¿ç®—ç±»å‹ï¼š
- åŠ æ³•ï¼š{{"operation":"add","x":æ•°å­—1,"y":æ•°å­—2}}
- å‡æ³•ï¼š{{"operation":"subtract","x":æ•°å­—1,"y":æ•°å­—2}}  
- ä¹˜æ³•ï¼š{{"operation":"multiply","x":æ•°å­—1,"y":æ•°å­—2}}
- é™¤æ³•ï¼š{{"operation":"divide","x":æ•°å­—1,"y":æ•°å­—2}}
- è·å–æ—¥æœŸï¼š{{"operation":"get_current_date"}}
- è¡¨è¾¾å¼ï¼š{{"operation":"expression","expression":"è¡¨è¾¾å¼å†…å®¹"}}

database_queryæ”¯æŒçš„æŸ¥è¯¢ç±»å‹ï¼š
- ç»Ÿè®¡æŸ¥è¯¢ï¼š{{"query_type":"count","table":"users","group_by":"status"}}
- æ¡ä»¶æŸ¥è¯¢ï¼š{{"query_type":"select","table":"users","where":{{"status":"active"}},"limit":10}}

ğŸ¯ **JSONæ ¼å¼è¦æ±‚ï¼š**

è¯·æ ¹æ®è¯­ä¹‰åˆ†æï¼Œæ™ºèƒ½å¡«å……æ‰€æœ‰å‚æ•°ï¼Œç¡®ä¿é€»è¾‘ä¸€è‡´ï¼š

{{
  "needs_web_search": å¸ƒå°”å€¼,
  "web_search_query": "å¦‚æœéœ€è¦ç½‘ç»œæœç´¢ï¼Œå¡«å…¥æœç´¢å…³é”®è¯",
  "needs_vector_search": å¸ƒå°”å€¼,
  "needs_database": å¸ƒå°”å€¼,
  "database_query": {{å…·ä½“çš„æ•°æ®åº“æŸ¥è¯¢å‚æ•°}},
  "needs_calculation": å¸ƒå°”å€¼,
  "calculation_args": {{å…·ä½“çš„è®¡ç®—å‚æ•°}},
  "query_type": "time/technical/calculation/database/general",
  "confidence": 0.0åˆ°1.0çš„ç½®ä¿¡åº¦,
  "reasoning": "è¯¦ç»†è¯´æ˜ä½ çš„è¯­ä¹‰åˆ†æè¿‡ç¨‹å’Œå‚æ•°è®¾ç½®ç†ç”±",
  "enable_dynamic_search": å¸ƒå°”å€¼,
  "min_similarity_threshold": 0.8
}}

âš ï¸ **é‡è¦è§„åˆ™ï¼š**
1. é€šè¿‡è¯­ä¹‰ç†è§£ï¼Œä¸æ˜¯å…³é”®è¯åŒ¹é…
2. reasoningå¿…é¡»è¯¦ç»†è§£é‡Šä¸ºä»€ä¹ˆè¿™æ ·è®¾ç½®å‚æ•°
3. å¦‚æœæ˜¯è®¡ç®—é—®é¢˜ï¼Œå¿…é¡»æ­£ç¡®è§£ææ•°å­—å’Œè¿ç®—ç¬¦
4. å¦‚æœæ˜¯æ—¶é—´é—®é¢˜ï¼Œweb_search_queryè¦å…·ä½“
5. ç½®ä¿¡åº¦è¦åæ˜ åˆ†æçš„ç¡®å®šç¨‹åº¦
6. ç¡®ä¿å‚æ•°ä¹‹é—´é€»è¾‘ä¸€è‡´

ç°åœ¨è¯·åˆ†æä¸Šè¿°æŸ¥è¯¢ï¼Œåªè¿”å›JSONç»“æœï¼š"""
    
    def _parse_llm_response(self, response: str, query: str) -> QueryAnalysisResult:
        """è§£æLLMå“åº” - å¤„ç†å„ç§å“åº”æ ¼å¼"""
        try:
            self.logger.info(f"ğŸ” è§£æLLMå“åº”ï¼Œé•¿åº¦: {len(response)} å­—ç¬¦")
            
            # ğŸ” æ£€æŸ¥æ˜¯å¦ä¸ºæ¨¡æ‹Ÿå“åº”
            if response.startswith("æ¨¡æ‹ŸLLMå“åº”:") or response.startswith("LLMåˆ†æ:") or response.startswith("LLMè°ƒç”¨å¤±è´¥"):
                self.logger.warning("âš ï¸ æ£€æµ‹åˆ°æ¨¡æ‹Ÿ/é”™è¯¯å“åº”ï¼Œä½¿ç”¨æ™ºèƒ½å›é€€ç­–ç•¥")
                return self._intelligent_fallback_sync(query)
            
            # ğŸ” æŸ¥æ‰¾JSONå†…å®¹
            json_patterns = [
                r'\{[^{}]*"needs_web_search"[^{}]*\}',  # åŒ¹é…åŒ…å«å…³é”®å­—æ®µçš„JSON
                r'\{.*?"reasoning".*?\}',                 # åŒ¹é…åŒ…å«reasoningçš„JSON
                r'\{.*\}',                               # é€šç”¨JSONåŒ¹é…
            ]
            
            json_data = None
            for pattern in json_patterns:
                json_match = re.search(pattern, response, re.DOTALL)
                if json_match:
                    try:
                        json_str = json_match.group()
                        json_data = json.loads(json_str)
                        self.logger.info(f"âœ… ä½¿ç”¨æ¨¡å¼åŒ¹é…æˆåŠŸè§£æJSON: {pattern[:30]}...")
                        break
                    except json.JSONDecodeError:
                        continue
            
            if not json_data:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆJSONï¼Œä½¿ç”¨æ™ºèƒ½å›é€€")
                return self._intelligent_fallback_sync(query)
            
            # éªŒè¯JSONç»“æ„
            if not isinstance(json_data, dict):
                raise ValueError("JSONè§£æç»“æœä¸æ˜¯å­—å…¸ç±»å‹")
            
            # ğŸ§  åˆ›å»ºåˆ†æç»“æœ
            result = QueryAnalysisResult(
                needs_web_search=json_data.get("needs_web_search", False),
                web_search_query=json_data.get("web_search_query", ""),
                needs_vector_search=json_data.get("needs_vector_search", True),
                needs_database=json_data.get("needs_database", False),
                database_query=json_data.get("database_query", {}),
                needs_calculation=json_data.get("needs_calculation", False),
                calculation_args=json_data.get("calculation_args", {}),
                query_type=json_data.get("query_type", "general"),
                confidence=float(json_data.get("confidence", 0.7)),
                reasoning=json_data.get("reasoning", "LLMè¯­ä¹‰åˆ†æç»“æœ"),
                enable_dynamic_search=json_data.get("enable_dynamic_search", True),
                min_similarity_threshold=float(json_data.get("min_similarity_threshold", 0.8))
            )
            
            # ğŸ“ è®°å½•åˆ†æç»“æœ
            self.logger.info(f"âœ… LLMè¯­ä¹‰åˆ†ææˆåŠŸ:")
            self.logger.info(f"   ğŸ¯ æŸ¥è¯¢ç±»å‹: {result.query_type}")
            self.logger.info(f"   ğŸŒ ç½‘ç»œæœç´¢: {result.needs_web_search}")
            self.logger.info(f"   ğŸ” å‘é‡æœç´¢: {result.needs_vector_search}")
            self.logger.info(f"   ğŸ§® æ•°å­¦è®¡ç®—: {result.needs_calculation}")
            self.logger.info(f"   ğŸ“Š æ•°æ®åº“æŸ¥è¯¢: {result.needs_database}")
            self.logger.info(f"   ğŸ“ˆ ç½®ä¿¡åº¦: {result.confidence:.2f}")
            self.logger.info(f"   ğŸ’­ æ¨ç†: {result.reasoning[:100]}...")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ è§£æLLMå“åº”å¤±è´¥: {e}")
            self.logger.error(f"åŸå§‹å“åº”: {response[:500]}...")
            # æ™ºèƒ½å›é€€
            return self._intelligent_fallback_sync(query)
    
    def _intelligent_fallback_sync(self, query: str) -> QueryAnalysisResult:
        """æ™ºèƒ½å›é€€åˆ†æ - å½“LLMæ— æ³•æ­£å¸¸å·¥ä½œæ—¶ä½¿ç”¨"""
        self.logger.info(f"ğŸ§  å¯åŠ¨æ™ºèƒ½å›é€€åˆ†æ: {query}")
        
        query_lower = query.lower()
        
        # ğŸ• æ—¶é—´ç›¸å…³æŸ¥è¯¢æ£€æµ‹
        time_indicators = ["ä»Šå¤©", "ç°åœ¨", "å‡ å·", "å‡ æœˆ", "å‡ ç‚¹", "å½“å‰", "æ—¥æœŸ", "æ—¶é—´", "æœ€æ–°"]
        if any(indicator in query_lower for indicator in time_indicators):
            return QueryAnalysisResult(
                needs_web_search=True,
                web_search_query=f"å½“å‰æ—¶é—´æ—¥æœŸ {query}",
                needs_vector_search=False,
                query_type="time",
                confidence=0.85,
                reasoning=f"æ™ºèƒ½å›é€€ï¼šæ£€æµ‹åˆ°æ—¶é—´ç›¸å…³æŸ¥è¯¢æŒ‡æ ‡: {[t for t in time_indicators if t in query_lower]}",
                enable_dynamic_search=False,
                min_similarity_threshold=0.8
            )
        
        # ğŸ§® è®¡ç®—ç›¸å…³æŸ¥è¯¢æ£€æµ‹
        calc_indicators = ["è®¡ç®—", "åŠ ", "å‡", "ä¹˜", "é™¤", "+", "-", "*", "/", "ç­‰äº", "å¤šå°‘", "æ±‚"]
        numbers = re.findall(r'\d+\.?\d*', query)
        if any(indicator in query_lower for indicator in calc_indicators) or len(numbers) >= 2:
            calc_args = self._parse_calculation(query)
            return QueryAnalysisResult(
                needs_calculation=True,
                calculation_args=calc_args,
                needs_vector_search=False,
                query_type="calculation",
                confidence=0.9,
                reasoning=f"æ™ºèƒ½å›é€€ï¼šæ£€æµ‹åˆ°è®¡ç®—æŸ¥è¯¢ï¼Œæå–åˆ°æ•°å­—: {numbers}ï¼Œè®¡ç®—å‚æ•°: {calc_args}",
                enable_dynamic_search=False,
                min_similarity_threshold=0.8
            )
        
        # ğŸ” æŠ€æœ¯æ¦‚å¿µæŸ¥è¯¢æ£€æµ‹
        tech_indicators = ["ä»€ä¹ˆæ˜¯", "å¦‚ä½•", "æ€ä¹ˆ", "åŸç†", "æ¦‚å¿µ", "å®šä¹‰", "æŠ€æœ¯", "è¯­è¨€", "æ¡†æ¶", "åº“"]
        if any(indicator in query_lower for indicator in tech_indicators):
            return QueryAnalysisResult(
                needs_vector_search=True,
                query_type="technical",
                confidence=0.8,
                reasoning=f"æ™ºèƒ½å›é€€ï¼šæ£€æµ‹åˆ°æŠ€æœ¯æ¦‚å¿µæŸ¥è¯¢æŒ‡æ ‡: {[t for t in tech_indicators if t in query_lower]}",
                enable_dynamic_search=True,
                min_similarity_threshold=0.8
            )
        
        # ğŸ“Š æ•°æ®åº“æŸ¥è¯¢æ£€æµ‹
        db_indicators = ["ç»Ÿè®¡", "æ•°é‡", "ç”¨æˆ·", "æ•°æ®", "æŸ¥è¯¢", "è®°å½•", "æ´»è·ƒ"]
        if any(indicator in query_lower for indicator in db_indicators):
            return QueryAnalysisResult(
                needs_database=True,
                database_query=self._build_database_query(query),
                needs_vector_search=True,
                query_type="database",
                confidence=0.75,
                reasoning=f"æ™ºèƒ½å›é€€ï¼šæ£€æµ‹åˆ°æ•°æ®åº“æŸ¥è¯¢æŒ‡æ ‡: {[d for d in db_indicators if d in query_lower]}",
                enable_dynamic_search=True,
                min_similarity_threshold=0.8
            )
        
        # ğŸŒ é»˜è®¤ç­–ç•¥ï¼šå‘é‡æœç´¢ + åŠ¨æ€æœç´¢
        return QueryAnalysisResult(
            needs_vector_search=True,
            query_type="general",
            confidence=0.6,
            reasoning="æ™ºèƒ½å›é€€ï¼šé€šç”¨æŸ¥è¯¢ç­–ç•¥ï¼Œä¼˜å…ˆå‘é‡æœç´¢+åŠ¨æ€æœç´¢",
            enable_dynamic_search=True,
            min_similarity_threshold=0.8
        )
    
    async def _fallback_analysis(self, query: str) -> QueryAnalysisResult:
        """ç®€åŒ–çš„å›é€€åˆ†æï¼ˆå½“LLMåˆ†æå¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
        self.logger.info("ğŸ“‹ ä½¿ç”¨ç®€åŒ–å›é€€åˆ†æ...")
        
        # é»˜è®¤ç­–ç•¥ï¼šä½¿ç”¨å‘é‡æœç´¢ + åŠ¨æ€æœç´¢ç­–ç•¥
        analysis = QueryAnalysisResult(
            needs_vector_search=True,
            query_type="general",
            confidence=0.6,
            reasoning="LLMåˆ†æå¤±è´¥ï¼Œä½¿ç”¨å›é€€ç­–ç•¥ï¼šå‘é‡æœç´¢+åŠ¨æ€æœç´¢",
            enable_dynamic_search=True,
            min_similarity_threshold=0.8
        )
        
        # ç®€å•çš„æ¨¡å¼è¯†åˆ«ï¼ˆä»…ä½œä¸ºåŸºæœ¬ä¿éšœï¼‰
        query_lower = query.lower()
        
        # æ˜æ˜¾çš„æ—¶é—´æŸ¥è¯¢
        if any(word in query_lower for word in ["ä»Šå¤©", "ç°åœ¨", "å‡ å·", "å‡ æœˆ", "å‡ ç‚¹", "å½“å‰æ—¶é—´", "æ—¥æœŸ"]):
            analysis.needs_web_search = True
            analysis.needs_vector_search = False
            analysis.web_search_query = "å½“å‰æ—¥æœŸæ—¶é—´"
            analysis.query_type = "time"
            analysis.enable_dynamic_search = False
            analysis.reasoning = "å›é€€åˆ†æï¼šæ£€æµ‹åˆ°æ—¶é—´ç›¸å…³æŸ¥è¯¢"
            analysis.confidence = 0.8
        
        # æ˜æ˜¾çš„è®¡ç®—æŸ¥è¯¢
        elif any(word in query_lower for word in ["è®¡ç®—", "åŠ ", "å‡", "ä¹˜", "é™¤", "+", "-", "*", "/"]):
            analysis.needs_calculation = True
            analysis.calculation_args = self._parse_calculation(query)
            analysis.query_type = "calculation"
            analysis.reasoning = "å›é€€åˆ†æï¼šæ£€æµ‹åˆ°è®¡ç®—æŸ¥è¯¢"
            analysis.confidence = 0.7
        
        # å¦‚æœæœ‰æ—¥æœŸæŸ¥è¯¢ï¼Œæ·»åŠ è·å–å½“å‰æ—¥æœŸçš„åŠŸèƒ½
        if "å‡ å·" in query_lower or "å‡ æœˆ" in query_lower or "æ—¥æœŸ" in query_lower:
            # æ·»åŠ ä¸€ä¸ªç‰¹æ®Šçš„è®¡ç®—æ“ä½œæ¥è·å–å½“å‰æ—¥æœŸ
            if not analysis.needs_calculation:
                analysis.needs_calculation = True
                analysis.calculation_args = {"operation": "get_current_date"}
            analysis.reasoning += " + è·å–å½“å‰æ—¥æœŸ"
        
        # å¡«å……å·¥å…·å‚æ•°
        self._fill_tool_parameters(analysis, query)
        
        return analysis
    
    def _fill_tool_parameters(self, analysis: QueryAnalysisResult, query: str):
        """æ™ºèƒ½å¡«å……å·¥å…·å‚æ•°"""
        # å¡«å……ç½‘ç»œæœç´¢å‚æ•°
        if analysis.needs_web_search and not analysis.web_search_query:
            analysis.web_search_query = query
        
        # å¡«å……æ•°æ®åº“æŸ¥è¯¢å‚æ•°
        if analysis.needs_database and not analysis.database_query:
            analysis.database_query = self._build_database_query(query)
        
        # å¡«å……è®¡ç®—å‚æ•°
        if analysis.needs_calculation and not analysis.calculation_args:
            analysis.calculation_args = self._parse_calculation(query)
        
        # æ„å»ºå·¥å…·è°ƒç”¨åˆ—è¡¨
        tool_calls = []
        
        if analysis.needs_web_search:
            tool_calls.append(ToolCall(
                name="web_search",
                args={"query": analysis.web_search_query, "limit": 5},
                reasoning="éœ€è¦è·å–æœ€æ–°ä¿¡æ¯"
            ))
        
        if analysis.needs_vector_search:
            tool_calls.append(ToolCall(
                name="vector_search",
                args={"query": query, "top_k": 5},
                reasoning="éœ€è¦æ£€ç´¢ç›¸å…³çŸ¥è¯†"
            ))
        
        if analysis.needs_database:
            tool_calls.append(ToolCall(
                name="database_query",
                args=analysis.database_query,
                reasoning="éœ€è¦æŸ¥è¯¢æ•°æ®åº“ä¿¡æ¯"
            ))
        
        if analysis.needs_calculation:
            tool_calls.append(ToolCall(
                name="calculator",
                args=analysis.calculation_args,
                reasoning="éœ€è¦è¿›è¡Œæ•°å­¦è®¡ç®—"
            ))
        
        analysis.tool_calls = tool_calls
    
    def _build_database_query(self, query: str) -> Dict[str, Any]:
        """æ„å»ºæ•°æ®åº“æŸ¥è¯¢å‚æ•°"""
        query_lower = query.lower()
        
        if "ç»Ÿè®¡" in query_lower or "æ•°é‡" in query_lower or "count" in query_lower:
            return {
                "query_type": "count",
                "table": "users",
                "group_by": "status"
            }
        
        if "æ´»è·ƒ" in query_lower:
            return {
                "query_type": "select",
                "table": "users",
                "where": {"status": "active"},
                "limit": 10
            }
        
        # é»˜è®¤æŸ¥è¯¢
        return {
            "query_type": "select",
            "table": "users",
            "limit": 5
        }
    
    def _parse_calculation(self, query: str) -> Dict[str, Any]:
        """è§£ææ•°å­¦è®¡ç®—è¡¨è¾¾å¼"""
        query_lower = query.lower()
        
        # ç®€å•çš„æ•°å­¦è¡¨è¾¾å¼è§£æ
        # åœ¨å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„è¡¨è¾¾å¼è§£æå™¨
        
        if "åŠ " in query_lower or "+" in query:
            # å°è¯•æå–æ•°å­—
            numbers = re.findall(r'\d+\.?\d*', query)
            if len(numbers) >= 2:
                return {
                    "operation": "add",
                    "x": float(numbers[0]),
                    "y": float(numbers[1])
                }
        
        if "å‡" in query_lower or "-" in query:
            numbers = re.findall(r'\d+\.?\d*', query)
            if len(numbers) >= 2:
                return {
                    "operation": "subtract",
                    "x": float(numbers[0]),
                    "y": float(numbers[1])
                }
        
        if "ä¹˜" in query_lower or "*" in query or "Ã—" in query:
            numbers = re.findall(r'\d+\.?\d*', query)
            if len(numbers) >= 2:
                return {
                    "operation": "multiply",
                    "x": float(numbers[0]),
                    "y": float(numbers[1])
                }
        
        if "é™¤" in query_lower or "/" in query or "Ã·" in query:
            numbers = re.findall(r'\d+\.?\d*', query)
            if len(numbers) >= 2:
                return {
                    "operation": "divide",
                    "x": float(numbers[0]),
                    "y": float(numbers[1])
                }
        
        # é»˜è®¤è®¡ç®—
        return {
            "operation": "expression",
            "expression": query
        }
    
    def should_use_search_engine(self, analysis: QueryAnalysisResult) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨æœç´¢å¼•æ“"""
        return analysis.needs_web_search or analysis.query_type in ["time", "news", "search"]
    
    def should_use_vector_store(self, analysis: QueryAnalysisResult) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨å‘é‡å­˜å‚¨"""
        return analysis.needs_vector_search or analysis.query_type in ["technical", "general"]
    
    def get_search_strategy(self, analysis: QueryAnalysisResult) -> str:
        """è·å–æœç´¢ç­–ç•¥"""
        if analysis.needs_web_search and analysis.needs_vector_search:
            return "hybrid"  # æ··åˆç­–ç•¥
        elif analysis.needs_web_search:
            return "search_only"  # ä»…æœç´¢
        elif analysis.needs_vector_search:
            return "vector_only"  # ä»…å‘é‡
        else:
            return "direct_llm"  # ç›´æ¥LLM


# ç®€å•çš„æ•°å­¦è®¡ç®—å™¨
class SimpleCalculator:
    """ç®€å•è®¡ç®—å™¨ - å¤„ç†åŸºæœ¬æ•°å­¦è¿ç®—"""
    
    @staticmethod
    def calculate(args: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œè®¡ç®—"""
        operation = args.get("operation", "")
        
        try:
            if operation == "get_current_date":
                # è·å–å½“å‰æ—¥æœŸ
                from datetime import datetime
                now = datetime.now()
                date_str = now.strftime("%Yå¹´%mæœˆ%dæ—¥")
                weekday = ["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"][now.weekday()]
                return {
                    "result": f"ä»Šå¤©æ˜¯{date_str}ï¼Œ{weekday}",
                    "expression": f"å½“å‰æ—¥æœŸ: {date_str} ({weekday})"
                }
                
            elif operation == "add":
                result = float(args["x"]) + float(args["y"])
                return {"result": result, "expression": f"{args['x']} + {args['y']} = {result}"}
            
            elif operation == "subtract":
                result = float(args["x"]) - float(args["y"])
                return {"result": result, "expression": f"{args['x']} - {args['y']} = {result}"}
            
            elif operation == "multiply":
                result = float(args["x"]) * float(args["y"])
                return {"result": result, "expression": f"{args['x']} Ã— {args['y']} = {result}"}
            
            elif operation == "divide":
                if float(args["y"]) == 0:
                    return {"error": "é™¤æ•°ä¸èƒ½ä¸ºé›¶"}
                result = float(args["x"]) / float(args["y"])
                return {"result": result, "expression": f"{args['x']} Ã· {args['y']} = {result}"}
            
            elif operation == "expression":
                # ç®€å•çš„è¡¨è¾¾å¼è®¡ç®—ï¼ˆå®‰å…¨èµ·è§ï¼Œåªæ”¯æŒåŸºæœ¬è¿ç®—ï¼‰
                expression = args.get("expression", "")
                # è¿™é‡Œå¯ä»¥é›†æˆæ›´å¤æ‚çš„è¡¨è¾¾å¼è§£æå™¨
                return {"result": "è¡¨è¾¾å¼è®¡ç®—åŠŸèƒ½å¾…å®ç°", "expression": expression}
            
            else:
                return {"error": f"ä¸æ”¯æŒçš„è¿ç®—ç±»å‹: {operation}"}
        
        except Exception as e:
            return {"error": f"è®¡ç®—é”™è¯¯: {str(e)}"}


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    async def test_analyzer():
        analyzer = SmartQueryAnalyzer()
        
        test_queries = [
            "ä»Šå¤©å‡ å·ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯Milvuså‘é‡æ•°æ®åº“ï¼Ÿ",
            "è®¡ç®—15.5åŠ ä¸Š24.3çš„ç»“æœ",
            "æŸ¥è¯¢æ´»è·ƒç”¨æˆ·æ•°é‡",
            "æœ€æ–°çš„AIæŠ€æœ¯å‘å±•å¦‚ä½•ï¼Ÿ"
        ]
        
        for query in test_queries:
            print(f"\nğŸ“ æµ‹è¯•æŸ¥è¯¢: {query}")
            print("-" * 40)
            
            result = await analyzer.analyze_query_intent(query)
            
            print(f"æŸ¥è¯¢ç±»å‹: {result.query_type}")
            print(f"éœ€è¦ç½‘ç»œæœç´¢: {result.needs_web_search}")
            print(f"éœ€è¦å‘é‡æœç´¢: {result.needs_vector_search}")
            print(f"éœ€è¦æ•°æ®åº“æŸ¥è¯¢: {result.needs_database}")
            print(f"éœ€è¦è®¡ç®—: {result.needs_calculation}")
            print(f"ç½®ä¿¡åº¦: {result.confidence:.2f}")
            print(f"æ¨ç†è¿‡ç¨‹: {result.reasoning}")
            print(f"å·¥å…·è°ƒç”¨æ•°: {len(result.tool_calls)}")
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_analyzer())
