#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åŠ¨æ€å‘é‡å­˜å‚¨ç®¡ç†å™¨

æä¾›å®æ—¶æœç´¢ç»“æœçš„å‘é‡åŒ–å­˜å‚¨å’Œæ£€ç´¢åŠŸèƒ½ã€‚
"""

import asyncio
import hashlib
import json
import logging
import os
import sys
import time
from dataclasses import asdict, dataclass
from typing import Any, Dict, List, Optional

from mcp_framework import SearchResult

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„ä»¥æ­£ç¡®å¯¼å…¥æ¨¡å—
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from encoder import emb_text
from milvus_utils import get_milvus_client

@dataclass
class VectorDocument:
    """å‘é‡æ–‡æ¡£æ•°æ®ç»“æ„"""
    id: int
    content: str
    title: str
    url: str
    source: str
    timestamp: float
    vector: List[float]
    metadata: Dict[str, Any]
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)


class DynamicVectorStore:
    """åŠ¨æ€å‘é‡å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(self, 
                 milvus_endpoint: str,
                 milvus_token: Optional[str] = None,
                 collection_name: str = "dynamic_rag_collection",
                 vector_dim: int = 384):
        """
        åˆå§‹åŒ–åŠ¨æ€å‘é‡å­˜å‚¨
        
        Args:
            milvus_endpoint: MilvusæœåŠ¡ç«¯ç‚¹
            milvus_token: Milvusè®¿é—®ä»¤ç‰Œ
            collection_name: é›†åˆåç§°
            vector_dim: å‘é‡ç»´åº¦
        """
        self.milvus_endpoint = milvus_endpoint
        self.milvus_token = milvus_token
        self.collection_name = collection_name
        self.vector_dim = vector_dim
        self.client = None
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self._init_client()
    
    def _init_client(self):
        """åˆå§‹åŒ–Milvuså®¢æˆ·ç«¯"""
        try:
            self.client = get_milvus_client(
                uri=self.milvus_endpoint,
                token=self.milvus_token
            )
            self._ensure_collection_exists()
            self.logger.info("Milvuså®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"Milvuså®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _ensure_collection_exists(self):
        """ç¡®ä¿é›†åˆå­˜åœ¨"""
        if not self.client.has_collection(self.collection_name):
            # åˆ›å»ºé›†åˆ
            self.client.create_collection(
                collection_name=self.collection_name,
                dimension=self.vector_dim,
                metric_type="IP",
                consistency_level="Strong",
                auto_id=False,
            )
            self.logger.info(f"åˆ›å»ºé›†åˆ: {self.collection_name}")
        else:
            self.logger.info(f"é›†åˆå·²å­˜åœ¨: {self.collection_name}")

    def _generate_doc_id(self, content: str, url: str,title:str) -> int:
        """
        ç”Ÿæˆæ–‡æ¡£ID - è½¬æ¢ä¸ºæ•°å­—ID
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            url: æ–‡æ¡£URL
            
        Returns:
            æ•°å­—ç±»å‹çš„æ–‡æ¡£ID
        """
        # å¢åŠ æ—¶é—´æˆ³æ»¡è¶³ç”Ÿæˆå”¯ä¸€IDï¼Œé¿å…å†²çª
        timestamp_str = str(int(time.time()*1000))
        unique_string = f"{url}_{title}_{content[:200]}_{timestamp_str}"
        hash_hex = hashlib.md5(unique_string.encode()).hexdigest()
        # å°†16è¿›åˆ¶å“ˆå¸Œè½¬æ¢ä¸ºæ•´æ•°ï¼Œå–å‰15ä½é¿å…æº¢å‡º
        doc_id = int(hash_hex[:15], 16)
        return doc_id
    
    def _is_content_similar(self,content1:str,content2:str,threshold:float=0.9) -> bool:
        """
        æ£€æŸ¥ä¸¤ä¸ªå†…å®¹æ˜¯å¦ç›¸ä¼¼
        
        Args:
            content1: ç¬¬ä¸€ä¸ªå†…å®¹
            content2: ç¬¬äºŒä¸ªå†…å®¹
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            æ˜¯å¦ç›¸ä¼¼
        """
        # ä½¿ç”¨ç®€å•çš„ç›¸ä¼¼åº¦æ£€æŸ¥ï¼ˆå¯ä»¥æ›¿æ¢ä¸ºæ›´å¤æ‚çš„ç®—æ³•ï¼‰
        words1 = set(content1.lower().split())
        words2 = set(content2.lower().split())
        if not words1 or not words2:
            return False
        
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        similarity = intersection / union if union > 0 else 0
        return similarity > threshold

    async def store_search_results(self, search_results: List[SearchResult]) -> int:
        """
        å­˜å‚¨æœç´¢ç»“æœåˆ°å‘é‡æ•°æ®åº“
        
        Args:
            search_results: æœç´¢ç»“æœåˆ—è¡¨
            
        Returns:
            æˆåŠŸå­˜å‚¨çš„æ–‡æ¡£æ•°é‡
        """
        if not search_results:
            return 0
        
        documents = []
        stored_count = 0
        processed_contents = [] # ç”¨äºæ£€æŸ¥å†…å®¹ç›¸ä¼¼æ€§    
        
        for result in search_results:
            try:
                # æ£€æŸ¥å†…å®¹ç›¸ä¼¼æ€§
                is_similar_to_existing=any(
                    self._is_content_similar(result.content, existing_content)
                    for existing_content in processed_contents
                )
                if is_similar_to_existing:
                    self.logger.debug(f"å†…å®¹è¿‡äºç›¸ä¼¼ï¼Œè·³è¿‡: {result.title[:50]}...")
                    continue

                # ç”Ÿæˆå‘é‡
                vector = emb_text(result.content)
                
                # ç”Ÿæˆæ–‡æ¡£IDï¼ˆæ•°å­—ç±»å‹ï¼‰
                doc_id = self._generate_doc_id(result.content, result.url,result.title)
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                if await self._document_exists(doc_id):
                    self.logger.debug(f"æ–‡æ¡£å·²å­˜åœ¨ï¼Œè·³è¿‡: {doc_id}")
                    continue
                
                # åˆ›å»ºå‘é‡æ–‡æ¡£
                doc = VectorDocument(
                    id=doc_id,
                    content=result.content,
                    title=result.title,
                    url=result.url,
                    source=result.source,
                    timestamp=result.timestamp,
                    vector=vector,
                    metadata=result.metadata or {}
                )
                
                documents.append(doc)
                
            except Exception as e:
                self.logger.error(f"å¤„ç†æœç´¢ç»“æœæ—¶å‡ºé”™: {e}")
                continue
        
        # æ‰¹é‡æ’å…¥
        if documents:
            stored_count = await self._batch_insert(documents)
        
        self.logger.info(f"æˆåŠŸå­˜å‚¨ {stored_count} ä¸ªæ–‡æ¡£")
        return stored_count
    
    async def _document_exists(self, doc_id: int) -> bool:
        """
        æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²å­˜åœ¨
        
        Args:
            doc_id: æ–‡æ¡£ID
            
        Returns:
            æ–‡æ¡£æ˜¯å¦å­˜åœ¨
        """
        try:
            result = self.client.query(
                collection_name=self.collection_name,
                filter=f'id == {doc_id}',
                output_fields=["id"],
                limit=1
            )
            return len(result) > 0
        except Exception as e:
            self.logger.warning(f"æ£€æŸ¥æ–‡æ¡£å­˜åœ¨æ€§æ—¶å‡ºé”™: {e}")
            return False
    
    async def _batch_insert(self, documents: List[VectorDocument]) -> int:
        """
        æ‰¹é‡æ’å…¥æ–‡æ¡£
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            æˆåŠŸæ’å…¥çš„æ–‡æ¡£æ•°é‡
        """
        try:
            # å‡†å¤‡æ’å…¥æ•°æ®
            data = []
            for doc in documents:
                data.append({
                    "id": doc.id,
                    "vector": doc.vector,
                    "content": doc.content,
                    "title": doc.title,
                    "url": doc.url,
                    "source": doc.source,
                    "timestamp": doc.timestamp,
                    "metadata": json.dumps(doc.metadata, ensure_ascii=False)
                })
            
            # æ‰§è¡Œæ’å…¥
            result = self.client.insert(
                collection_name=self.collection_name,
                data=data
            )
            
            return result["insert_count"]
            
        except Exception as e:
            self.logger.error(f"æ‰¹é‡æ’å…¥å¤±è´¥: {e}")
            return 0
    
    async def search_similar(self, 
                           query: str, 
                           limit: int = 10,
                           similarity_threshold: float = 0.3) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸ä¼¼æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨
        """
        try:
            # å‘é‡åŒ–æŸ¥è¯¢
            query_vector = emb_text(query)
            
            # æ‰§è¡Œæœç´¢
            search_results = self.client.search(
                collection_name=self.collection_name,
                data=[query_vector],
                limit=limit,
                search_params={"metric_type": "IP", "params": {}},
                output_fields=["content", "title", "url", "source", "timestamp", "metadata"]
            )
            
            # è¿‡æ»¤å’Œæ ¼å¼åŒ–ç»“æœ
            filtered_results = []
            for result in search_results[0]:
                if result["distance"] > similarity_threshold:
                    doc_data = {
                        "id": result["id"],
                        "content": result["entity"]["content"],
                        "title": result["entity"]["title"],
                        "url": result["entity"]["url"],
                        "source": result["entity"]["source"],
                        "timestamp": result["entity"]["timestamp"],
                        "similarity_score": result["distance"],
                        "metadata": (json.loads(result["entity"]["metadata"]) 
                                   if result["entity"]["metadata"] else {})
                    }
                    filtered_results.append(doc_data)
            
            return filtered_results
            
        except Exception as e:
            self.logger.error(f"ç›¸ä¼¼æ€§æœç´¢å¤±è´¥: {e}")
            return []
    
    async def cleanup_old_documents(self, max_age_hours: int = 24) -> int:
        """
        æ¸…ç†è¿‡æœŸæ–‡æ¡£
        
        Args:
            max_age_hours: æœ€å¤§ä¿ç•™æ—¶é—´ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            æ¸…ç†çš„æ–‡æ¡£æ•°é‡
        """
        try:
            cutoff_time = time.time() - (max_age_hours * 3600)
            
            # æŸ¥è¯¢è¿‡æœŸæ–‡æ¡£
            old_docs = self.client.query(
                collection_name=self.collection_name,
                filter=f"timestamp < {cutoff_time}",
                output_fields=["id"],
                limit=1000  # æ‰¹é‡å¤„ç†
            )
            
            if old_docs:
                # åˆ é™¤è¿‡æœŸæ–‡æ¡£
                doc_ids = [doc["id"] for doc in old_docs]
                id_list_str = "[" + ",".join(map(str, doc_ids)) + "]"
                self.client.delete(
                    collection_name=self.collection_name,
                    filter=f'id in {id_list_str}'
                )
                
                self.logger.info(f"æ¸…ç†äº† {len(doc_ids)} ä¸ªè¿‡æœŸæ–‡æ¡£")
                return len(doc_ids)
            
            return 0
            
        except Exception as e:
            self.logger.error(f"æ¸…ç†è¿‡æœŸæ–‡æ¡£å¤±è´¥: {e}")
            return 0
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """
        è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            é›†åˆç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            stats = self.client.get_collection_stats(self.collection_name)
            return {
                "total_documents": stats.get("row_count", 0),
                "collection_name": self.collection_name,
                "vector_dimension": self.vector_dim
            }
        except Exception as e:
            self.logger.error(f"è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {"error": str(e)}


class VectorStoreManager:
    """å‘é‡å­˜å‚¨ç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†å¤šä¸ªå‘é‡å­˜å‚¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å‘é‡å­˜å‚¨ç®¡ç†å™¨"""
        self.stores: Dict[str, DynamicVectorStore] = {}
        self.logger = logging.getLogger(self.__class__.__name__)
    
    def add_store(self, name: str, store: DynamicVectorStore):
        """
        æ·»åŠ å‘é‡å­˜å‚¨
        
        Args:
            name: å­˜å‚¨åç§°
            store: å‘é‡å­˜å‚¨å®ä¾‹
        """
        self.stores[name] = store
        self.logger.info(f"æ·»åŠ å‘é‡å­˜å‚¨: {name}")
    
    def get_store(self, name: str) -> Optional[DynamicVectorStore]:
        """
        è·å–å‘é‡å­˜å‚¨
        
        Args:
            name: å­˜å‚¨åç§°
            
        Returns:
            å‘é‡å­˜å‚¨å®ä¾‹æˆ–None
        """
        return self.stores.get(name)
    
    async def search_all_stores(self, 
                               query: str, 
                               limit_per_store: int = 5) -> Dict[str, List[Dict[str, Any]]]:
        """
        åœ¨æ‰€æœ‰å­˜å‚¨ä¸­æœç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            limit_per_store: æ¯ä¸ªå­˜å‚¨çš„ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            å„å­˜å‚¨çš„æœç´¢ç»“æœå­—å…¸
        """
        results = {}
        
        for name, store in self.stores.items():
            try:
                store_results = await store.search_similar(query, limit_per_store)
                results[name] = store_results
            except Exception as e:
                self.logger.error(f"åœ¨å­˜å‚¨ {name} ä¸­æœç´¢å¤±è´¥: {e}")
                results[name] = []
        
        return results
    
    async def cleanup_all_stores(self, max_age_hours: int = 24) -> int:
        """
        æ¸…ç†æ‰€æœ‰å­˜å‚¨ä¸­çš„è¿‡æœŸæ•°æ®
        
        Args:
            max_age_hours: æœ€å¤§ä¿ç•™æ—¶é—´ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            æ€»æ¸…ç†æ–‡æ¡£æ•°é‡
        """
        total_cleaned = 0
        
        for name, store in self.stores.items():
            try:
                cleaned = await store.cleanup_old_documents(max_age_hours)
                total_cleaned += cleaned
                self.logger.info(f"å­˜å‚¨ {name} æ¸…ç†äº† {cleaned} ä¸ªæ–‡æ¡£")
            except Exception as e:
                self.logger.error(f"æ¸…ç†å­˜å‚¨ {name} å¤±è´¥: {e}")
        
        return total_cleaned
    
    def get_all_stats(self) -> Dict[str, Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰å­˜å‚¨çš„ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            æ‰€æœ‰å­˜å‚¨çš„ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        stats = {}
        
        for name, store in self.stores.items():
            try:
                stats[name] = store.get_collection_stats()
            except Exception as e:
                stats[name] = {"error": str(e)}
        
        return stats


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
async def test_dynamic_vector_store():
    """æµ‹è¯•åŠ¨æ€å‘é‡å­˜å‚¨"""
    print("ğŸ§ª æµ‹è¯•åŠ¨æ€å‘é‡å­˜å‚¨...")
    
    # åˆ›å»ºå­˜å‚¨å®ä¾‹
    store = DynamicVectorStore(
        milvus_endpoint="./test_dynamic.db",
        collection_name="test_dynamic_collection"
    )
    
    # åˆ›å»ºæµ‹è¯•æœç´¢ç»“æœ
    from mcp_framework import ChannelType, SearchResult
    
    test_results = [
        SearchResult(
            title="æµ‹è¯•æ–‡æ¡£1",
            content="è¿™æ˜¯ä¸€ä¸ªå…³äºäººå·¥æ™ºèƒ½çš„æµ‹è¯•æ–‡æ¡£å†…å®¹",
            url="https://example.com/ai-doc1",
            source="test_source",
            timestamp=time.time(),
            relevance_score=0.9,
            channel_type=ChannelType.SEARCH_ENGINE,
            metadata={"category": "AI"}
        ),
        SearchResult(
            title="æµ‹è¯•æ–‡æ¡£2", 
            content="è¿™æ˜¯å¦ä¸€ä¸ªå…³äºæœºå™¨å­¦ä¹ çš„æµ‹è¯•æ–‡æ¡£å†…å®¹",
            url="https://example.com/ml-doc2",
            source="test_source",
            timestamp=time.time(),
            relevance_score=0.8,
            channel_type=ChannelType.SEARCH_ENGINE,
            metadata={"category": "ML"}
        )
    ]
    
    # å­˜å‚¨æµ‹è¯•ç»“æœ
    stored_count = await store.store_search_results(test_results)
    print(f"âœ… å­˜å‚¨äº† {stored_count} ä¸ªæ–‡æ¡£")
    
    # æœç´¢æµ‹è¯•
    search_results = await store.search_similar("äººå·¥æ™ºèƒ½", limit=5)
    print(f"âœ… æœç´¢åˆ° {len(search_results)} ä¸ªç›¸ä¼¼æ–‡æ¡£")
    
    for result in search_results:
        print(f"  - {result['title']}: {result['similarity_score']:.3f}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = store.get_collection_stats()
    print(f"âœ… é›†åˆç»Ÿè®¡: {stats}")
    
    print("ğŸ‰ æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_dynamic_vector_store())
