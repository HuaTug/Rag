#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¢å¼ºçš„RAGå¤„ç†å™¨

é›†æˆå¤šé€šé“æœç´¢ã€å‘é‡å­˜å‚¨å’ŒLLMç”Ÿæˆï¼Œæä¾›å®Œæ•´çš„RAGè§£å†³æ–¹æ¡ˆã€‚
"""

import asyncio
import logging
import time
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import json

from mcp_framework import MCPProcessor, QueryContext, QueryAnalyzer,QueryType,SearchResult
from search_channels import GoogleSearchChannel
from dynamic_vector_store import DynamicVectorStore, VectorStoreManager
from ask_llm import get_llm_answer_deepseek
from encoder import emb_text
from milvus_utils import get_milvus_client


@dataclass
class RAGResponse:
    """RAGå“åº”æ•°æ®ç»“æ„"""
    answer: str
    sources: List[Dict[str, Any]]
    search_results: List[SearchResult]
    processing_time: float
    confidence_score: float
    metadata: Dict[str, Any]


class EnhancedRAGProcessor:
    """å¢å¼ºçš„RAGå¤„ç†å™¨"""
    
    def __init__(self, vector_store=None, search_channels=None, llm_client=None, config: Dict[str, Any] = None):
        self.config = config or {}
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # å­˜å‚¨å¤–éƒ¨ä¼ å…¥çš„ç»„ä»¶
        self.vector_store = vector_store
        self.search_channels = search_channels or []
        self.llm_client = llm_client
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.mcp_processor = MCPProcessor()
        self.vector_store_manager = VectorStoreManager()
        self.query_analyzer = QueryAnalyzer()
        
        # æ™ºèƒ½æŸ¥è¯¢ç­–ç•¥é…ç½®
        self.similarity_threshold = self.config.get("similarity_threshold", 0.5)  # ç›¸ä¼¼åº¦é˜ˆå€¼
        self.min_vector_results = self.config.get("min_vector_results", 3)  # æœ€å°‘å‘é‡ç»“æœæ•°é‡
        self.enable_smart_search = self.config.get("enable_smart_search", True)  # å¯ç”¨æ™ºèƒ½æœç´¢
        
        # è¾“å‡ºé…ç½®ä¿¡æ¯ç”¨äºè°ƒè¯•
        self.logger.info(f"ğŸ“Š æ™ºèƒ½æœç´¢é…ç½®: similarity_threshold={self.similarity_threshold}, "
                        f"min_vector_results={self.min_vector_results}, "
                        f"enable_smart_search={self.enable_smart_search}")
        
        # åˆå§‹åŒ–é…ç½®
        self._init_components()
    
    def _init_components(self):
        """åˆå§‹åŒ–å„ä¸ªç»„ä»¶"""
        try:
            # 1. åˆå§‹åŒ–å‘é‡å­˜å‚¨
            self._init_vector_stores()
            
            # 2. åˆå§‹åŒ–æœç´¢é€šé“
            self._init_search_channels()
            
            self.logger.info("å¢å¼ºRAGå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _init_vector_stores(self):
        """åˆå§‹åŒ–å‘é‡å­˜å‚¨"""
        # è·å–Milvusé…ç½®ï¼Œä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„é…ç½®
        milvus_endpoint = (
            self.config.get("milvus_endpoint") or 
            self.config.get("endpoint") or 
            "./milvus_rag.db"
        )
        milvus_token = (
            self.config.get("milvus_token") or 
            self.config.get("token")
        )
        vector_dim = (
            self.config.get("vector_dim") or 
            self.config.get("dimension") or 
            384
        )
        
        # åŠ¨æ€å‘é‡å­˜å‚¨ï¼ˆç”¨äºå®æ—¶æœç´¢ç»“æœï¼‰
        dynamic_store = DynamicVectorStore(
            milvus_endpoint=milvus_endpoint,
            milvus_token=milvus_token,
            collection_name="dynamic_search_results",
            vector_dim=vector_dim
        )
        self.vector_store_manager.add_store("dynamic", dynamic_store)
        
        # æœ¬åœ°çŸ¥è¯†åº“å­˜å‚¨
        local_store = DynamicVectorStore(
            milvus_endpoint=milvus_endpoint,
            milvus_token=milvus_token,
            collection_name="local_knowledge",
            vector_dim=vector_dim
        )
        self.vector_store_manager.add_store("local", local_store)
    
    def _init_search_channels(self):
        """åˆå§‹åŒ–æœç´¢é€šé“"""
        # æ³¨å†Œå¤–éƒ¨ä¼ å…¥çš„æœç´¢é€šé“
        for channel in self.search_channels:
            self.mcp_processor.register_channel(channel)
        
        # Googleæœç´¢é€šé“ï¼ˆå¦‚æœé…ç½®ä¸­å¯ç”¨ä¸”æ²¡æœ‰å¤–éƒ¨ä¼ å…¥ï¼‰
        if (self.config.get("enable_search_engine", True) and 
            not any(hasattr(ch, 'channel_type') and 
                   str(ch.channel_type).endswith('SEARCH_ENGINE') 
                   for ch in self.search_channels)):
            
            search_config = {
                "api_key": self.config.get("google_api_key"),
                "search_engine_id": self.config.get("google_search_engine_id"),
                "timeout": self.config.get("search_timeout", 10),
                "priority": {"factual": 1, "analytical": 2, "creative": 5, "conversational": 3}
            }
            
            if search_config["api_key"] and search_config["search_engine_id"]:
                search_channel = GoogleSearchChannel(search_config)
                self.mcp_processor.register_channel(search_channel)
        
        # æœ¬åœ°çŸ¥è¯†åº“é€šé“ï¼ˆæš‚æ—¶ç¦ç”¨ï¼Œå› ä¸ºLocalKnowledgeChannelç±»ä¸å­˜åœ¨ï¼‰
        if False:  # self.config.get("enable_local_knowledge", True):
            local_store = self.vector_store_manager.get_store("local")
            if local_store:
                local_config = {
                    "milvus_client": local_store.client,
                    "collection_name": "local_knowledge",
                    "encoder": emb_text,
                    "priority": {"factual": 2, "analytical": 1, "creative": 4, "conversational": 2}
                }
                # local_channel = LocalKnowledgeChannel(local_config)
                # self.mcp_processor.register_channel(local_channel)
        
        # æ–°é—»é€šé“ï¼ˆæš‚æ—¶ç¦ç”¨ï¼Œå› ä¸ºNewsChannelç±»ä¸å­˜åœ¨ï¼‰
        if False:  # self.config.get("enable_news", False):
            news_config = {
                "news_api_key": self.config.get("news_api_key"),
                "priority": {"factual": 3, "analytical": 3, "creative": 6, "conversational": 4}
            }
            # news_channel = NewsChannel(news_config)
            # self.mcp_processor.register_channel(news_channel)
    
    async def process_query(self, context: QueryContext) -> RAGResponse:
        """
        å¤„ç†æŸ¥è¯¢è¯·æ±‚ - å®ç°æ™ºèƒ½æŸ¥è¯¢ç­–ç•¥
        
        ç­–ç•¥ï¼š
        1. é¦–å…ˆä»å‘é‡æ•°æ®åº“æŸ¥æ‰¾ç›¸ä¼¼å†…å®¹
        2. å¦‚æœæ‰¾åˆ°è¶³å¤Ÿç›¸ä¼¼ä¸”æ•°é‡å……è¶³çš„å†…å®¹ï¼Œç›´æ¥ä½¿ç”¨
        3. å¦åˆ™è°ƒç”¨æœç´¢å¼•æ“è·å–æ–°å†…å®¹
        4. å°†æ–°å†…å®¹å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        """
        start_time = time.time()
        query = context.query
        
        try:
            # 1. æŸ¥è¯¢åˆ†æ
            query_type = context.query_type
            self.logger.info(f"ğŸ” æŸ¥è¯¢ç±»å‹: {query_type.value}")
            
            # 2. æ™ºèƒ½æŸ¥è¯¢ç­–ç•¥ï¼šå…ˆæ£€æŸ¥å‘é‡æ•°æ®åº“
            vector_results = await self._perform_vector_search(query, context.max_results)
            
            # 3. åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨æœç´¢å¼•æ“
            need_search, reason = self._should_perform_search(vector_results, context)
            
            search_results = []
            if need_search:
                self.logger.info(f"ğŸŒ éœ€è¦æœç´¢å¼•æ“æŸ¥è¯¢: {reason}")
                search_results = await self._perform_search(context)
                
                # å­˜å‚¨æ–°çš„æœç´¢ç»“æœåˆ°å‘é‡æ•°æ®åº“
                if search_results:
                    await self._store_search_results(search_results)
                    self.logger.info(f"ğŸ’¾ å­˜å‚¨äº† {len(search_results)} ä¸ªæ–°çš„æœç´¢ç»“æœ")
            else:
                self.logger.info(f"âœ… ä½¿ç”¨å‘é‡æ•°æ®åº“ç»“æœ: {reason}")
            
            # 4. èåˆå’Œæ’åºç»“æœ
            all_results = self._merge_results(search_results, vector_results)
            
            # 5. ç”Ÿæˆç­”æ¡ˆ
            answer, confidence = await self._generate_answer(query, all_results)
            
            # 6. æ„å»ºå“åº”
            processing_time = time.time() - start_time
            response = RAGResponse(
                answer=answer,
                sources=self._extract_sources(all_results),
                search_results=search_results,
                processing_time=processing_time,
                confidence_score=confidence,
                metadata={
                    "query_type": query_type.value,
                    "total_results": len(all_results),
                    "search_results_count": len(search_results),
                    "vector_results_count": len(vector_results),
                    "used_search_engine": need_search,
                    "search_reason": reason,
                    "similarity_threshold": self.similarity_threshold
                }
            )
            
            self.logger.info(f"âœ… æŸ¥è¯¢å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}s")
            return response
            
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            return RAGResponse(
                answer=f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æŸ¥è¯¢æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                sources=[],
                search_results=[],
                processing_time=time.time() - start_time,
                confidence_score=0.0,
                metadata={"error": str(e)}
            )
    
    def _should_perform_search(self, vector_results: List[Dict[str, Any]], context: QueryContext) -> Tuple[bool, str]:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦æ‰§è¡Œæœç´¢å¼•æ“æŸ¥è¯¢
        
        Returns:
            Tuple[bool, str]: (æ˜¯å¦éœ€è¦æœç´¢, åŸå› è¯´æ˜)
        """
        if not self.enable_smart_search:
            return True, "æ™ºèƒ½æœç´¢å·²ç¦ç”¨"
        
        if not vector_results:
            return True, "å‘é‡æ•°æ®åº“ä¸­æ²¡æœ‰ç›¸å…³ç»“æœ"
        
        # æ£€æŸ¥ç»“æœæ•°é‡
        if len(vector_results) < self.min_vector_results:
            return True, f"å‘é‡ç»“æœæ•°é‡ä¸è¶³ ({len(vector_results)} < {self.min_vector_results})"
        
        # æ£€æŸ¥æœ€é«˜ç›¸ä¼¼åº¦
        max_similarity = max(result.get("similarity_score", 0) for result in vector_results)
        if max_similarity < self.similarity_threshold:
            return True, f"æœ€é«˜ç›¸ä¼¼åº¦ä¸è¶³ ({max_similarity:.3f} < {self.similarity_threshold})"
        
        # æ£€æŸ¥é«˜è´¨é‡ç»“æœæ•°é‡
        high_quality_results = [
            r for r in vector_results 
            if r.get("similarity_score", 0) >= self.similarity_threshold
        ]
        
        if len(high_quality_results) < 2:
            return True, f"é«˜è´¨é‡ç»“æœæ•°é‡ä¸è¶³ ({len(high_quality_results)} < 2)"
        
        # æ£€æŸ¥å†…å®¹æ–°é²œåº¦ï¼ˆå¯é€‰ï¼‰
        current_time = time.time()
        recent_results = [
            r for r in high_quality_results
            if current_time - r.get("timestamp", 0) < 7 * 24 * 3600  # 7å¤©å†…
        ]
        
        if len(recent_results) == 0:
            return True, "æ²¡æœ‰è¶³å¤Ÿæ–°é²œçš„é«˜è´¨é‡ç»“æœ"
        
        # ç‰¹æ®ŠæŸ¥è¯¢ç±»å‹å¤„ç†
        if context.query_type == QueryType.CREATIVE:
            return True, "åˆ›é€ æ€§æŸ¥è¯¢éœ€è¦å®æ—¶æœç´¢"
        
        # æ£€æŸ¥æŸ¥è¯¢ä¸­æ˜¯å¦åŒ…å«æ—¶é—´ç›¸å…³è¯æ±‡
        time_keywords = ["ä»Šå¤©", "æœ€æ–°", "ç°åœ¨", "å½“å‰", "æœ€è¿‘", "ä»Šå¹´", "2024", "2025"]
        if any(keyword in context.query for keyword in time_keywords):
            return True, "æŸ¥è¯¢åŒ…å«æ—¶é—´ç›¸å…³è¯æ±‡ï¼Œéœ€è¦æœ€æ–°ä¿¡æ¯"
        
        return False, f"å‘é‡æ•°æ®åº“æœ‰è¶³å¤Ÿçš„é«˜è´¨é‡ç»“æœ ({len(high_quality_results)} ä¸ªï¼Œæœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.3f})"
    
    async def _perform_search(self, context: QueryContext) -> List[SearchResult]:
        """æ‰§è¡Œå®æ—¶æœç´¢"""
        try:
            return await self.mcp_processor.process_query(context)
        except Exception as e:
            self.logger.error(f"å®æ—¶æœç´¢å¤±è´¥: {e}")
            return []
    
    async def _perform_vector_search(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """æ‰§è¡Œå‘é‡æœç´¢"""
        try:
            all_results = await self.vector_store_manager.search_all_stores(query, limit)
            
            # åˆå¹¶æ‰€æœ‰å­˜å‚¨çš„ç»“æœ
            merged_results = []
            for store_name, results in all_results.items():
                for result in results:
                    result["store_name"] = store_name
                    merged_results.append(result)
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            merged_results.sort(key=lambda x: x.get("similarity_score", 0), reverse=True)
            
            return merged_results[:limit]
            
        except Exception as e:
            self.logger.error(f"å‘é‡æœç´¢å¤±è´¥: {e}")
            return []
    
    async def _store_search_results(self, search_results: List[SearchResult]):
        """å­˜å‚¨æœç´¢ç»“æœåˆ°å‘é‡æ•°æ®åº“"""
        try:
            dynamic_store = self.vector_store_manager.get_store("dynamic")
            if dynamic_store:
                stored_count = await dynamic_store.store_search_results(search_results)
                self.logger.info(f"å­˜å‚¨äº† {stored_count} ä¸ªæœç´¢ç»“æœ")
        except Exception as e:
            self.logger.error(f"å­˜å‚¨æœç´¢ç»“æœå¤±è´¥: {e}")
    
    def _merge_results(self, 
                      search_results: List[SearchResult], 
                      vector_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """èåˆæœç´¢ç»“æœå’Œå‘é‡ç»“æœ"""
        merged = []
        
        # æ·»åŠ å®æ—¶æœç´¢ç»“æœï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰
        for result in search_results:
            merged.append({
                "content": result.content,
                "title": result.title,
                "url": result.url,
                "source": result.source,
                "score": result.relevance_score + 0.1,  # ç»™æ–°æœç´¢ç»“æœåŠ æƒ
                "type": "search",
                "timestamp": result.timestamp
            })
        
        # æ·»åŠ å‘é‡æœç´¢ç»“æœ
        for result in vector_results:
            merged.append({
                "content": result.get("content", ""),
                "title": result.get("title", ""),
                "url": result.get("url", ""),
                "source": result.get("source", ""),
                "score": result.get("similarity_score", 0),
                "type": "vector",
                "store_name": result.get("store_name", ""),
                "timestamp": result.get("timestamp", 0)
            })
        
        # å»é‡å’Œæ’åº
        seen_urls = set()
        deduplicated = []
        
        for result in merged:
            url = result.get("url", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                deduplicated.append(result)
            elif not url:  # æ²¡æœ‰URLçš„ç»“æœä¹Ÿä¿ç•™
                deduplicated.append(result)
        
        # æŒ‰åˆ†æ•°æ’åº
        deduplicated.sort(key=lambda x: x.get("score", 0), reverse=True)
        
        return deduplicated
    
    async def _generate_answer(self, 
                              query: str, 
                              results: List[Dict[str, Any]]) -> Tuple[str, float]:
        """ç”Ÿæˆç­”æ¡ˆ"""
        try:
            if not results:
                return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚", 0.0
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            for i, result in enumerate(results[:5]):  # ä½¿ç”¨å‰5ä¸ªæœ€ç›¸å…³çš„ç»“æœ
                content = result.get("content", "").strip()
                if content:
                    source_info = f"æ¥æº: {result.get('title', 'æœªçŸ¥')} ({result.get('source', 'æœªçŸ¥')})"
                    context_parts.append(f"å‚è€ƒèµ„æ–™ {i+1}:\n{content}\n{source_info}")
            
            context = "\n\n".join(context_parts)
            
            # è°ƒç”¨DeepSeek LLMç”Ÿæˆç­”æ¡ˆ
            try:
                # ä½¿ç”¨ä¼ å…¥çš„LLMå®¢æˆ·ç«¯
                if hasattr(self, 'llm_client') and self.llm_client:
                    # æ„å»ºæ¶ˆæ¯æ ¼å¼
                    messages = []
                    
                    if context:
                        system_prompt = """
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯å……åˆ†ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯å›ç­”ã€‚
å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸å¤Ÿå……åˆ†ï¼Œå¯ä»¥ç»“åˆä½ çš„çŸ¥è¯†ç»™å‡ºæœ‰å¸®åŠ©çš„å›ç­”ã€‚
è¯·ç¡®ä¿å›ç­”å‡†ç¡®ã€æœ‰æ¡ç†ï¼Œå¹¶å°½å¯èƒ½æä¾›å…·ä½“çš„ä¿¡æ¯ã€‚
è¯·æä¾›å®Œæ•´è¯¦ç»†çš„å›ç­”ï¼Œä¸è¦æˆªæ–­å†…å®¹ã€‚
"""
                        messages.append({"role": "system", "content": system_prompt})
                        
                        user_content = f"""
åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

{context}

é—®é¢˜ï¼š{query}
"""
                    else:
                        user_content = query
                    
                    messages.append({"role": "user", "content": user_content})
                    
                    # è°ƒç”¨DeepSeek API - æ·»åŠ é‡è¦å‚æ•°
                    response = self.llm_client.chat_completions_create(
                        model="deepseek-v3-0324",
                        messages=messages,
                        stream=False,
                        enable_search=True,  # å¯ç”¨DeepSeekçš„æœç´¢åŠŸèƒ½
                        temperature=0.7,     # è®¾ç½®åˆ›é€ æ€§å‚æ•°
                        top_p=0.9,          # è®¾ç½®æ ¸é‡‡æ ·å‚æ•°
                        frequency_penalty=0.0,  # é¢‘ç‡æƒ©ç½š
                        presence_penalty=0.0    # å­˜åœ¨æƒ©ç½š
                    )
                    
                    if response and "choices" in response:
                        answer = response["choices"][0]["message"]["content"]
                    else:
                        answer = "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆå›ç­”"
                        
                else:
                    # é™çº§åˆ°ç®€å•çš„ä¸Šä¸‹æ–‡æ‹¼æ¥
                    if context:
                        answer = f"åŸºäºæœç´¢ç»“æœï¼š\n\n{context}\n\nå›ç­”ï¼šè¯·å‚è€ƒä»¥ä¸Šä¿¡æ¯æ¥å›ç­”å…³äº'{query}'çš„é—®é¢˜ã€‚"
                    else:
                        answer = f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°å…³äº'{query}'çš„ç›¸å…³ä¿¡æ¯ã€‚"
                        
            except Exception as e:
                self.logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
                # é™çº§æ–¹æ¡ˆ
                if context:
                    answer = f"åŸºäºæœç´¢åˆ°çš„ä¿¡æ¯ï¼š\n\n{context}"
                else:
                    answer = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°äº†é”™è¯¯ï¼š{str(e)}"
            
            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºç»“æœæ•°é‡å’Œç›¸å…³æ€§ï¼‰
            confidence = min(1.0, len(results) * 0.1 + sum(r.get("score", 0) for r in results[:3]) / 3)
            
            return answer, confidence
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {e}")
            return f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯: {str(e)}", 0.0
    
    def _extract_sources(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æå–æ¥æºä¿¡æ¯"""
        sources = []
        for result in results[:10]:  # æœ€å¤šè¿”å›10ä¸ªæ¥æº
            source = {
                "title": result.get("title", "æœªçŸ¥æ ‡é¢˜"),
                "url": result.get("url", ""),
                "source": result.get("source", "æœªçŸ¥æ¥æº"),
                "score": result.get("score", 0),
                "type": result.get("type", "unknown")
            }
            sources.append(source)
        
        return sources
    
    async def cleanup_old_data(self, max_age_hours: int = 24):
        """æ¸…ç†è¿‡æœŸæ•°æ®"""
        try:
            dynamic_store = self.vector_store_manager.get_store("dynamic")
            if dynamic_store:
                cleaned_count = await dynamic_store.cleanup_old_documents(max_age_hours)
                self.logger.info(f"æ¸…ç†äº† {cleaned_count} ä¸ªè¿‡æœŸæ–‡æ¡£")
                return cleaned_count
        except Exception as e:
            self.logger.error(f"æ¸…ç†è¿‡æœŸæ•°æ®å¤±è´¥: {e}")
        return 0
    
    def update_smart_search_config(self, 
                                  similarity_threshold: float = None,
                                  min_vector_results: int = None,
                                  enable_smart_search: bool = None):
        """æ›´æ–°æ™ºèƒ½æœç´¢é…ç½®"""
        if similarity_threshold is not None:
            self.similarity_threshold = similarity_threshold
            self.logger.info(f"æ›´æ–°ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}")
        
        if min_vector_results is not None:
            self.min_vector_results = min_vector_results
            self.logger.info(f"æ›´æ–°æœ€å°‘å‘é‡ç»“æœæ•°é‡: {min_vector_results}")
        
        if enable_smart_search is not None:
            self.enable_smart_search = enable_smart_search
            self.logger.info(f"æ™ºèƒ½æœç´¢å¼€å…³: {enable_smart_search}")
    
    def get_smart_search_stats(self) -> Dict[str, Any]:
        """è·å–æ™ºèƒ½æœç´¢ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "similarity_threshold": self.similarity_threshold,
            "min_vector_results": self.min_vector_results,
            "enable_smart_search": self.enable_smart_search,
            "vector_stores": list(self.vector_store_manager.stores.keys())
        }


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
async def test_smart_rag():
    """æµ‹è¯•æ™ºèƒ½RAGç³»ç»Ÿ"""
    print("ğŸ§ª æµ‹è¯•æ™ºèƒ½RAGç³»ç»Ÿ...")
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ æµ‹è¯•ä»£ç 
    pass


if __name__ == "__main__":
    asyncio.run(test_smart_rag())
